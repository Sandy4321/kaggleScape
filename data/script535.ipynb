{"nbformat_minor": 1, "nbformat": 4, "cells": [{"cell_type": "code", "outputs": [], "source": ["import math\n", "import pandas as pd\n", "import numpy as np\n", "import seaborn as sns\n", "import json\n", "import itertools\n", "import re\n", "import random\n", "\n", "import matplotlib.pyplot as plt\n", "import matplotlib.gridspec as gridspec\n", "import plotly.plotly as py\n", "import plotly.graph_objs as go\n", "\n", "from datetime import date, timedelta\n", "from skimage.draw import ellipse\n", "from textwrap import wrap\n", "from plotly import __version__\n", "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n", "from IPython.display import HTML, Image\n", "from plotly import tools\n", "from bs4 import BeautifulSoup\n", "from nltk.corpus import stopwords\n", "from sklearn import preprocessing\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import matthews_corrcoef\n", "from sklearn.metrics import hamming_loss, f1_score\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "\n", "plt.style.use('seaborn-white')\n", "%matplotlib inline\n", "init_notebook_mode(connected=True)"], "execution_count": null, "metadata": {"_cell_guid": "4646b239-028d-4b2a-b370-4905c33f7e62", "_kg_hide-input": false, "_uuid": "c012bf0f8560eeab484e1abdcbb2f1c77662d624", "_kg_hide-output": false}}, {"cell_type": "markdown", "source": [" # 1. Get to know the data"], "metadata": {"_cell_guid": "1b0f0a44-086d-45f7-859e-d5c435c25b8d", "_uuid": "51843324020e7d8d2274f3d1e4becc2b85e7e91b"}}, {"cell_type": "markdown", "source": ["First step is to download database. Updates are made on a daily basis. Sometimes there could be an additional column that should not be in databases. It is not a big deal and can be corrected by the code below."], "metadata": {"_cell_guid": "702f2d22-0c0f-42b7-a209-16d8186c223c", "_uuid": "6a90c2c9a9f89ff9fa0913a7c984b6af9a09939d"}}, {"cell_type": "code", "outputs": [], "source": ["# read csv file\n", "# 12 columns that should be in database \n", "columns = ['video_id', 'title', 'channel_title', 'category_id',\n", "          'tags', 'views', 'likes', 'dislikes', 'comment_total',\n", "          'thumbnail_link', 'date']\n", "\n", "gb_vid_df =  pd.read_csv(\"../input/GBvideos.csv\", usecols = columns)\n", "us_vid_df =  pd.read_csv(\"../input/USvideos.csv\", usecols = columns)\n", "\n", "# find out errors in last columns\n", "print (gb_vid_df['date'].unique())\n", "print (us_vid_df['date'].unique())\n", "\n", "# replace error items\n", "gb_vid_df.loc[gb_vid_df['date'] == '24.09l7yxJDFvTRM', 'date'] = '24.09'\n", "us_vid_df.loc[us_vid_df['date'] == '24.09xcaeyJTx4Co', 'date'] = '24.09'\n", "gb_vid_df.loc[gb_vid_df['date'] == '26.09t2oVUxTV4WA', 'date'] = '26.09'\n", "us_vid_df.loc[us_vid_df['date'] == '26.0903jeumSTSzc', 'date'] = '26.09'\n", "us_vid_df.loc[us_vid_df['date'] == '100', 'date'] = '09.10'"], "execution_count": null, "metadata": {"_cell_guid": "d24b91f3-1e3e-49e7-aa0a-4d6c3ea90d19", "_uuid": "5d52d3ec511c26e15a8fe26908bb0d4b4c278d6d"}}, {"cell_type": "markdown", "source": ["Change of data format will make many things easier in the future. Also databases could contain doubles that are better to be removed."], "metadata": {"_cell_guid": "84048d41-bd9e-4e2b-befc-53f0b5386816", "_uuid": "d753e422cf1f00d40078983a80b57f125dcc77b0"}}, {"cell_type": "code", "outputs": [], "source": ["# change format of date table for further operations\n", "gb_vid_df['date'] = gb_vid_df['date'].apply(lambda x: pd.to_datetime(str(x).replace('.','')+\"2017\",\n", "                                            format='%d%m%Y'))\n", "gb_vid_df['date'] = gb_vid_df['date'].dt.date\n", "\n", "\n", "us_vid_df['date'] = us_vid_df['date'].apply(lambda x: pd.to_datetime(str(x).replace('.','')+\"2017\",\n", "                                                                     format='%d%m%Y'))\n", "us_vid_df['date'] = us_vid_df['date'].dt.date\n", "\n", "# remove duplicates for GB videos database\n", "gb_vid_df = gb_vid_df.drop_duplicates()\n", "us_vid_df = us_vid_df.drop_duplicates()\n", "\n", "gb_vid_df.head(5)"], "execution_count": null, "metadata": {"_cell_guid": "9b7f2397-0efe-4500-bf68-c5eb8ed8ab69", "_uuid": "3c8ada5ef2da9f2e680dc1a6aef8a88a5a094cc6"}}, {"cell_type": "markdown", "source": ["Now it is possible to make first overall analysis. What I am interested in is to support my understanding of database nature (top 200 trending videos per each day), check how many days a video can be in trend and get some details on longest trending videos."], "metadata": {"_cell_guid": "5bfbd040-61cd-4c83-90aa-957b39f138f3", "_uuid": "0198fba096eca57c9439877f70ec00006eafcdf0"}}, {"cell_type": "markdown", "source": ["First function is made to provide quick overall look on the data."], "metadata": {"_cell_guid": "f9382228-ed52-4b6f-88c8-bf73519545d2", "_uuid": "0b2ed12617e69b4826c5e59eeb0ffa07cfd3c1a6"}}, {"cell_type": "code", "outputs": [], "source": ["# dataframe to see videos per each date\n", "def quick_insight(df, country, color):\n", "    # dataframes for videos per each date\n", "    vid_check = df[['video_id', 'date']].copy().groupby('date', as_index = False).count()\n", "    vid_check.columns = ['Dates', 'Videos per date']\n", "    \n", "    # dataframes for \n", "    dates_per_id = df[['video_id', 'date']].groupby('video_id', as_index = False).count()\n", "    dates_per_vids = dates_per_id.groupby('date', as_index = False).count()\n", "    dates_per_vids.columns = ['Quantity of dates per video', 'Quantity of videos in a date group']\n", "    max_days = max(dates_per_vids['Quantity of dates per video'].values)\n", "    \n", "    # videos appeared in database as at 13 September 2017\n", "    sept_13_id = df.loc[df['date'] == date(2017,9,13), 'video_id'].tolist()\n", "    sept_13 = df.loc[df['video_id'].isin(sept_13_id), ['video_id', 'date']]\n", "    sept_13 = sept_13.groupby('date', as_index=False).count()   \n", "    \n", "    # combined plot\n", "    fig = plt.figure(figsize=(14, 10))\n", "    gs = gridspec.GridSpec(3, 2, width_ratios=[1,1], height_ratios = [1,0.1,1])\n", "    \n", "    # plotting videos per each date\n", "    ax1 = plt.subplot(gs[2,0:2])\n", "    ax1 = sns.barplot(x=\"Dates\", y=\"Videos per date\", data=vid_check, color='green', alpha=0.8)\n", "    ax1.set_ylabel('Videos per date', fontsize=12)\n", "    ax1.set_xticklabels(vid_check['Dates'].unique(), rotation=45)\n", "    ax1.set_xlabel('')\n", "    ax1.set_title('Videos per each date ({})'.format(country), fontsize=15)\n", "    \n", "    # plotting buckets of videos by quantity of trending dates\n", "    ax2 = plt.subplot(gs[0,0])\n", "    ax2 = sns.barplot(x=\"Quantity of dates per video\",\n", "                      y=\"Quantity of videos in a date group\",\n", "                      data=dates_per_vids, color=color)\n", "    ax2.set_ylabel('Quantity of videos in a date group', fontsize=12)\n", "    ax2.set_xlabel('Quantity of trending days in a bucket', fontsize=12)\n", "    t_str = u'Buckets of videos by quantity of trending dates ({})'\n", "    title = '\\n'.join(wrap(t_str.format(country),30))\n", "    ax2.set_title(title, fontsize=15)\n", "    \n", "    # plotting story of videos that appeared on September 13\n", "    ax3 = plt.subplot(gs[0,1])    \n", "    ax3 = sns.barplot(x='date', y=\"video_id\", data=sept_13, color=color, alpha = 0.7)\n", "    ax3.set_ylabel('Quantity of videos per date', fontsize=12)\n", "    ax3.set_xticklabels(sept_13['date'], rotation=45)\n", "    ax3.set_xlabel('')\n", "    t_str = u'Videos started at 13 September ({})'\n", "    title = '\\n'.join(wrap(t_str.format(country),20))\n", "    ax3.set_title(title, fontsize=15)\n", "    \n", "    plt.show()"], "execution_count": null, "metadata": {"_cell_guid": "a6366e4a-dd09-45b2-bb30-ac21d0f3bd0d", "collapsed": true, "_uuid": "fcd27925fc009ed201b576ab52ca386f4468e4b5"}}, {"cell_type": "markdown", "source": ["The second function is designed to give deeper information of what videos were in trend for longest period. It is interesting can I find there some familiar videos? How many and what dynamics of views, likes, dislikes and comments are required to become trending video? Are there some similarities in preferences of US and GB audience?"], "metadata": {"_cell_guid": "6f8d1571-616f-4710-9d8e-656a4970bd37", "_uuid": "719a5cb0c18996b93baddcd26ed57a8dc03defe2"}}, {"cell_type": "code", "outputs": [], "source": ["def best_survivors(df, days, country):\n", "    # videos with selected lifetime in top 200\n", "    dates_per_id = df[['video_id', 'date']].groupby('video_id', as_index = False).count()\n", "    long_surv_list = dates_per_id.loc[dates_per_id['date'] == days,'video_id'].tolist()\n", "    long_surv_vid = df.loc[df['video_id'].isin(long_surv_list),\n", "                           ['title','date','views','likes','dislikes','comment_total']]\n", "    long_surv_vid['views'] = long_surv_vid['views'].apply(lambda x: x/1000000)\n", "    titles_list = long_surv_vid['title'].unique().tolist()\n", "\n", "    # plotting videos views history\n", "    views = []\n", "    likes = []\n", "    dislikes = []\n", "    comments = []\n", "    plots_list = [views, likes, dislikes, comments]\n", "    column_list = ['views', 'likes', 'dislikes', 'comment_total']\n", "    boolean_list = [False, False, False, True]\n", "    colors_list = []\n", "    for i in range(0,len(titles_list)):\n", "        color = 'rgb('+str(np.random.randint(1,256))+\",\"+str(np.random.randint(1,256))+\",\"+str(np.random.randint(1,256))+\")\"\n", "        colors_list.append(color)\n", "    \n", "    for x in range (0,len(plots_list)):\n", "        for i in range (0, len(titles_list)):\n", "            vt = titles_list[i]\n", "            trace = go.Scatter(x = long_surv_vid.loc[long_surv_vid['title']== vt,'date'],\n", "                                y = long_surv_vid.loc[long_surv_vid['title']== vt, column_list[x]],\n", "                                name = vt,\n", "                                line = dict(width = 2, color = colors_list[i]),\n", "                                legendgroup = vt,\n", "                                showlegend = boolean_list[x])\n", "    \n", "            plots_list[x].extend([trace])\n", "    \n", "    fig = tools.make_subplots(rows=4,\n", "                              cols=1, \n", "                              subplot_titles=('Views', 'Comments', 'Likes', 'Dislikes'),\n", "                              vertical_spacing=0.07)\n", "    \n", "    for i in views:\n", "        fig.append_trace(i, 1, 1)\n", "    for i in comments:\n", "        fig.append_trace(i, 2, 1)\n", "    for i in likes:\n", "        fig.append_trace(i, 3, 1)\n", "    for i in dislikes:\n", "        fig.append_trace(i, 4, 1)\n", "    \n", "    fig['layout']['xaxis1'].update(title='')\n", "    fig['layout']['xaxis2'].update(title='')\n", "    fig['layout']['xaxis3'].update(title='')\n", "    fig['layout']['xaxis4'].update(title='')\n", "\n", "    fig['layout']['yaxis1'].update(title='mln. views')\n", "    fig['layout']['yaxis2'].update(title='comments')\n", "    fig['layout']['yaxis3'].update(title='likes')\n", "    fig['layout']['yaxis4'].update(title='dislikes')\n", "    \n", "    fig['layout'].update(width=800, height=(1000 + len(titles_list)*60))\n", "    fig['layout'].update(title='Different metrics for videos trended in {} for {} days'.format(country, days))\n", "    fig['layout'].update(legend = dict(x=0.0,\n", "                                       y = -(0.1+len(titles_list)*0.007),\n", "                                      tracegroupgap = 1))\n", "\n", "    iplot(fig, filename='customizing-subplot-axes')"], "execution_count": null, "metadata": {"_cell_guid": "5837d2be-502f-46b4-bc01-8940b75587f8", "collapsed": true, "_uuid": "d4a61d3af5221cc6ae7f9a346c8a93467369d0a4"}}, {"cell_type": "markdown", "source": ["* Now both functions can be applied to GB and US database."], "metadata": {"_cell_guid": "724062d0-646e-4f63-a14e-3da16c5bde4f", "_uuid": "e200a12dcc54d29bc6889931b4bad0ea575da50e"}}, {"cell_type": "code", "outputs": [], "source": ["quick_insight(gb_vid_df, \"GB\", 'red')"], "execution_count": null, "metadata": {"_cell_guid": "8b0689c0-e224-4079-9698-9d37afa90420", "scrolled": true, "_uuid": "1c315c517b847479e798112a72b091608b769220"}}, {"cell_type": "code", "outputs": [], "source": ["best_survivors(gb_vid_df, 10, \"GB\")"], "execution_count": null, "metadata": {"_cell_guid": "316ef052-cfd2-471b-89ba-222690d273da", "scrolled": true, "_uuid": "54e585b0124bf3154b83798632f0181deb37dcd0"}}, {"cell_type": "code", "outputs": [], "source": ["quick_insight(us_vid_df, \"US\", 'blue')"], "execution_count": null, "metadata": {"_cell_guid": "c7690f5a-3b17-4b13-87e5-f8967c54cad7", "_uuid": "b09275c2d3807452a7ab014f2456106a1bf50668"}}, {"cell_type": "code", "outputs": [], "source": ["best_survivors(us_vid_df, 7, \"US\")"], "execution_count": null, "metadata": {"_cell_guid": "07f58158-4c7c-4e38-aebb-87e6157299b6", "_uuid": "0742a11e7e9ca771ad5e5f09487c72386c6f35b0"}}, {"cell_type": "markdown", "source": ["Some initial conclusions:\n", "\n", "There are 200 trending videos per each date. Database does not look for some specific videos, only for top 200 trending. Video may be in this top list for a few days or only one day.\n", "\n", "Within covered period GB audience showed higher adherence to trending videos. Maximum amount of days being in trend for US audience was 7 days and for GB it was 10 days.\n", "\n", "Plotly charts above are very convenient, as it is possible to click on some videos in legend to exclude it from the charts. Thus if you interested in some specific video(s) it is possible to leave only them to look at.\n", "\n", "This method also reveal that in most cases likes and dislikes have similar shapes but different magnitudes.\n", "\n", "What is puzzling me now is why some of the top trending videos have so low amount of views/comments. I suppose that trending is more about speed of new views than they amont. Thus in following analysis I plan do check dynamic of these features for trending videos."], "metadata": {"_cell_guid": "b80a8b8d-a669-45b4-a05d-466ba4f3d706", "_uuid": "1afaeb9d180d37041de109754fd8923d641c518d"}}, {"cell_type": "markdown", "source": ["# 2. Trending channels"], "metadata": {"_cell_guid": "56136150-9032-4ec7-b260-cf2253a5a4b4", "_uuid": "587d42789955928a8cc8b9831efd104e04d39487"}}, {"cell_type": "markdown", "source": ["The questions I have are:\n", "\n", "* how US and GB trending longevity of channels compares?\n", "* what is the difference in lasting trends between US and GB?\n", "* how average trend of likes and comments for each longevity group looks like?\n", "* are there channels that reappear in top 200?\n", "* what are the features of stable-trending channels (category, tags, ratio of views to likes/comments)?"], "metadata": {"_cell_guid": "f1511474-1701-40b4-8948-dab42706a4af", "_uuid": "552915dea136bb7a9c43e70303a76b54ef3a0a3b"}}, {"cell_type": "code", "outputs": [], "source": ["# function designed to plot longevity of trending\n", "def trending_channels(gb, us, day_start, day_end, fig_height, hr_2ndRow):\n", "    # list of selected dates\n", "    datelist = [day_start + timedelta(days=x) for x in range((day_end-day_start).days + 1)]\n", "    # number of days is selected time span\n", "    tr_days = len(datelist)\n", "    \n", "    # leave only dates within selected range\n", "    gb = gb.loc[gb['date'].isin(datelist),]\n", "    us = us.loc[us['date'].isin(datelist),]\n", "    \n", "    # Dataframes for longeivety plot -----------------------------------------------------\n", "    # group by channel title to see for how many dates channel was in trend\n", "    gb_ch_days = gb.groupby('channel_title')['date'].nunique()\n", "    gb_ch_days = gb_ch_days.to_frame().reset_index()\n", "    # dataframe with amount of channels per each \"days in trend\" bucket\n", "    gb_days_trending = gb_ch_days.groupby('date', axis=0, as_index = False).count()\n", "    \n", "    # group by channel title to see for how many dates channel was in trend\n", "    us_ch_days = us.groupby('channel_title')['date'].nunique()\n", "    us_ch_days = us_ch_days.to_frame().reset_index()\n", "    # dataframe with amount of channels per each \"days in trend\" bucket\n", "    us_days_trending = us_ch_days.groupby('date', as_index = False).count()  \n", "    \n", "    # Difference in trending channels in US and GB -----------------------------------------\n", "    gb_day_tr_list = gb_ch_days.loc[gb_ch_days['date']==tr_days,\n", "                                    'channel_title'].values.tolist()\n", "    gb_day_tr = gb.loc[gb['channel_title'].isin(gb_day_tr_list),]\n", "    gb_day_tr = gb_day_tr.loc[gb_day_tr['date'].isin(datelist),]\n", "    gb_day_tr.sort_values('date', axis=0, ascending=True, inplace=True)\n", "    gb_day_tr.drop_duplicates(subset='video_id', keep='last', inplace=True)\n", "    gb_day_summary = gb_day_tr[['channel_title', 'views']].copy().groupby('channel_title',\n", "                                                                  as_index = False).sum()\n", "    \n", "    us_day_tr_list = us_ch_days.loc[us_ch_days['date']==tr_days,\n", "                                    'channel_title'].values.tolist()\n", "    us_day_tr = us.loc[us['channel_title'].isin(us_day_tr_list),]\n", "    us_day_tr = us_day_tr.loc[us_day_tr['date'].isin(datelist),]\n", "    us_day_tr.sort_values('date', axis=0, ascending=True, inplace=True)\n", "    us_day_tr.drop_duplicates(subset='video_id', keep='last', inplace=True)\n", "    us_day_summary = us_day_tr[['channel_title', 'views']].copy().groupby('channel_title',\n", "                                                                  as_index = False).sum()\n", "    \n", "    # make lists of channels overlap and channels for gb/us only --------------------------\n", "    gb_tr_list = gb_day_summary['channel_title'].values.tolist()\n", "    us_tr_list = us_day_summary['channel_title'].values.tolist()\n", "    overlaps = [x for x in gb_tr_list if x in us_tr_list]\n", "    gb_only = [x for x in gb_tr_list if x not in us_tr_list]\n", "    us_only = [x for x in us_tr_list if x not in gb_tr_list]\n", "    \n", "    # sort dataframes to make plots look nicer\n", "    gb_day_summary.sort_values('views', axis=0, ascending=False, inplace=True)\n", "    us_day_summary.sort_values('views', axis=0, ascending=False, inplace=True)\n", "    \n", "    # make overlap database and sort them by channel_title\n", "    gb_overlap = gb_day_summary.loc[gb_day_summary['channel_title'].isin(overlaps),]\n", "    gb_overlap = gb_overlap.sort_values('channel_title', axis=0, ascending=False)\n", "    \n", "    us_overlap = us_day_summary.loc[us_day_summary['channel_title'].isin(overlaps),]\n", "    gb_overlap = us_overlap.sort_values('channel_title', axis=0, ascending=False)\n", "    \n", "    # make us/gb only database\n", "    gb_only_df = gb_day_summary.loc[gb_day_summary['channel_title'].isin(gb_only),]\n", "    us_only_df = us_day_summary.loc[us_day_summary['channel_title'].isin(us_only),]  \n", "    \n", "    # combined plot ------------------------------------------------------------------\n", "    fig = plt.figure(figsize=(12, fig_height))\n", "    gs = gridspec.GridSpec(3, 6, height_ratios=[1,0.1,hr_2ndRow],\n", "                           width_ratios=[1,1,1,0.5,1,1])\n", "    \n", "    # plotting longevity of trending\n", "    ax1 = plt.subplot(gs[0,:3])\n", "    ax1.bar(gb_days_trending['date']+0.15,\n", "            gb_days_trending['channel_title'],\n", "            width = 0.35,\n", "            color='r', alpha = 1)\n", "    ax1.bar(us_days_trending['date']-0.15,\n", "            us_days_trending['channel_title'],\n", "            width = 0.35,\n", "            color='b', alpha = 1)\n", "    \n", "    ax1.set_title('Longevity of trending from {} to {}'.format(day_start,day_end),\n", "                fontstyle='italic', fontweight='bold')\n", "    ax1.set_ylabel('Channels within each bucket')\n", "    ax1.set_yticks(np.arange(0, max(us_days_trending['channel_title']), 10))\n", "    ax1.set_xlabel('Days channel was in list of top 200 trending videos')\n", "    ax1.set_xticks(us_days_trending['date'].unique())\n", "    ax1.set_xticklabels(gb_days_trending['date'].unique())\n", "    ax1.legend(('GB', 'US'))\n", "    ax1.grid(linestyle='-', linewidth=1)\n", "    \n", "    # plotting channels trending in both GB and US\n", "    ax2 = plt.subplot(gs[0,4:])\n", "    width = 0.4\n", "    ax2.barh(np.arange(gb_overlap['channel_title'].shape[0]),\n", "             gb_overlap['views'].apply(lambda x: x/1000000),\n", "             width, color='r', alpha = 1)\n", "    ax2.barh(np.arange(us_overlap['channel_title'].shape[0])+width,\n", "             us_overlap['views'].apply(lambda x: x/1000000),\n", "             width, color='b', alpha = 1)\n", "    overlaps = [ '\\n'.join(wrap(l, 15)) for l in overlaps]\n", "    ax2.set_yticks(np.arange(len(overlaps)))\n", "    yticklabels = ['\\n'.join(wrap(l, 15)) for l in us_overlap['channel_title'].values.tolist()]\n", "    ax2.set_yticklabels(yticklabels, stretch = 'ultra-condensed')\n", "    ax2.set_xlabel('mln viewes')\n", "    ax2.legend(('GB', 'US'))\n", "    t_str = u'Channels trended in GB and US for {} days from {} to {}'\n", "    title = '\\n'.join(wrap(t_str.format(tr_days,day_start, day_end),30))\n", "    ax2.set_title(title, fontstyle='italic', fontweight='bold')\n", "    \n", "    # plotting channels trending in GB only\n", "    ax3 = plt.subplot(gs[2,:2])\n", "    width = 0.3\n", "    ax3 = sns.barplot(x = gb_only_df['views'].apply(lambda x: x/1000000),\n", "                     y = gb_only_df['channel_title'],\n", "                     color = 'red')\n", "    gb_only = ['\\n'.join(wrap(l, 15)) for l in gb_only]\n", "    ax3.set_yticks(np.arange(len(gb_only)))\n", "    ax3.set_yticklabels(gb_only_df['channel_title'],stretch = 'ultra-condensed')\n", "    ax3.set_xlabel('mln viewes')\n", "    ax3.set_ylabel('')\n", "    t_str = u'Channels trended only in GB for {} days from {} to {}'\n", "    title = '\\n'.join(wrap(t_str.format(tr_days,day_start, day_end),30))\n", "    ax3.set_title(title, fontstyle='italic', fontweight='bold')\n", "    \n", "    # plotting channels trending in US only\n", "    ax4 = plt.subplot(gs[2,4:])\n", "    width = 0.3\n", "    ax4 = sns.barplot(x = us_only_df['views'].apply(lambda x: x/1000000),\n", "                     y = us_only_df['channel_title'],\n", "                     color = 'blue')\n", "    us_only = ['\\n'.join(wrap(l, 15)) for l in us_only]\n", "    ax4.set_yticks(np.arange(len(us_only)))\n", "    ax4.set_yticklabels(us_only_df['channel_title'], stretch = 'ultra-condensed')\n", "    ax4.set_xlabel('mln viewes')\n", "    ax4.set_ylabel('')\n", "    t_str = u'Channels trended only in US for {} days from {} to {}'\n", "    title = '\\n'.join(wrap(t_str.format(tr_days,day_start, day_end),30))\n", "    ax4.set_title(title, fontstyle='italic', fontweight='bold')\n", "    \n", "    plt.show()"], "execution_count": null, "metadata": {"_cell_guid": "fa412c48-8d68-498a-9b54-60838f245dc0", "collapsed": true, "_uuid": "2dfbe27179d2c5c4975a5699ccac3fd9942995dc"}}, {"cell_type": "code", "outputs": [], "source": ["trending_channels(gb_vid_df, us_vid_df, date(2017,9,13), date(2017,9,26), 15, 0.7)"], "execution_count": null, "metadata": {"_cell_guid": "acc243c2-7141-4d7c-9458-9fea79dddb07", "_uuid": "e708a20fa3a4cd4e6597abfe9d88a6bf0eed14c9"}}, {"cell_type": "markdown", "source": ["The charts above tell the following: \n", "* there were 19 channels with videos trending in GB for all 14 days selected\n", "* there were 10 channels with videos trending in US for all 14 days selected\n", "* the chart is not cumulative - the channels from bucket \"15 days\" don't appear in other buckets\n", "* 6 channels were in top trends in both countries\n", "* 13 channels were in top trends in GB only \n", "* 4 channels were in top trends in US only\n", "* if your target audience is US then much more effort should be made to keep the channel in top trends"], "metadata": {"_cell_guid": "23e5459c-6074-4833-8b17-85fe64cde88b", "_uuid": "36edbb8324dc2759026d42ae1fd618617f07dc7a"}}, {"cell_type": "markdown", "source": ["At first I expected that it is possible that one channel includes few trending videos at same date. But it appeared not to be so. Following chart supports this conclusion."], "metadata": {"_cell_guid": "339e5d23-f7ba-4ca6-807c-54594b1188b3", "_uuid": "7ddcb960f803771aea68b0db9532d5475e264956"}}, {"cell_type": "code", "outputs": [], "source": ["gb_chan_per_date = gb_vid_df[['channel_title', 'date']].groupby('date', \n", "                                                                as_index=False).count()\n", "\n", "us_chan_per_date = us_vid_df[['channel_title', 'date']].groupby('date', \n", "                                                                as_index=False).count()\n", "\n", "# plotting quantity of channels per each date\n", "fig = plt.figure(figsize=(14, 5))\n", "gs = gridspec.GridSpec(1, 2, width_ratios=[1,1])\n", "\n", "ax1 = plt.subplot(gs[0,0])\n", "ax1 = sns.barplot(x=\"date\", y=\"channel_title\", data=gb_chan_per_date,\n", "                  color='red', alpha=0.9)\n", "ax1.set_ylabel('Channels per date', fontsize=14)\n", "ax1.set_xticklabels(gb_chan_per_date['date'].unique(), rotation=45)\n", "ax1.set_xlabel('')\n", "ax1.set_title('Channels per each date (GB)', fontsize=20)\n", "\n", "ax2 = plt.subplot(gs[0,1])\n", "ax2 = sns.barplot(x=\"date\", y=\"channel_title\", data=us_chan_per_date,\n", "                  color='blue', alpha=0.9)\n", "ax2.set_ylabel('Channels per date', fontsize=14)\n", "ax2.set_xticklabels(us_chan_per_date['date'].unique(), rotation=45)\n", "ax2.set_xlabel('')\n", "ax2.set_title('Channels per each date (US)', fontsize=20)\n", "\n", "plt.show()"], "execution_count": null, "metadata": {"_cell_guid": "67f501c0-c3a4-40d2-9cc4-93d2c7af7f43", "_uuid": "fafce10e5399ab9d853ea0f8d1bfb3693a1831cb"}}, {"cell_type": "markdown", "source": ["These two looks very familiar to charts above with **videos** by each date. But this one shows **channels** by each date. Thus amount of videos per day equals to amount of channels per day which means only one videos from the channel can be in trend. "], "metadata": {"_cell_guid": "dde01590-501f-45d6-8921-39128a39c5e6", "_uuid": "6b68f4c44e078efcf415729aecbb4af68b90b678"}}, {"cell_type": "markdown", "source": ["Considering this conclusion it is interesting to see what happens with videos of trending channels. At what point one is replaced by another.\n", "\n", "Function below is designed to make Ploly chart with possibility to select specific channel and explore the story of its videos."], "metadata": {"_cell_guid": "71d956f0-f4e6-43a8-843a-49d1d249a6e4", "_uuid": "f0211bbb03d538b7b69c919b37e856c168711d48"}}, {"cell_type": "code", "outputs": [], "source": ["# function for channels analysis\n", "# days_a must be in range between day_start and day_end\n", "def channels_insight(df, country, day_start, day_end, days_a):\n", "    # list of selected dates\n", "    datelist = [day_start + timedelta(days=x) for x in range((day_end-day_start).days + 1)]\n", "    # take part of database that included in relevant period\n", "    df = df.copy().loc[df['date'].isin(datelist),]\n", "    # make list of channels survived for selected days_a\n", "    df_ch_days = df.groupby('channel_title')['date'].nunique().to_frame().reset_index()\n", "    df_ch_list = df_ch_days.loc[df_ch_days['date']==days_a, 'channel_title'].unique().tolist()\n", "    \n", "    # plotting videos per each channel (by views)\n", "    data = []\n", "    buttons = []\n", "    buttons_list = []\n", "    \n", "    for x in df_ch_list:\n", "        channel_videos = df.loc[df['channel_title'] == x,['title', 'date', 'views']]\n", "        videos_list = channel_videos['title'].unique().tolist()\n", "        boolean_list = [True] + [False]*(len(videos_list)-1)\n", "        color = 'rgb('+str(np.random.randint(1,256))+\",\"+str(np.random.randint(1,256))+\",\"+str(np.random.randint(1,256))+\")\"\n", "        for i in range(0,len(videos_list)):\n", "            vt = videos_list[i]\n", "            video = channel_videos.loc[channel_videos['title']==vt, ['views', 'date']]\n", "            trace = go.Scatter(x = video['date'].values,\n", "                               y = video['views'].values,\n", "                               name = x,\n", "                               line = dict(width = 2, color = color),\n", "                               legendgroup = x,\n", "                               showlegend = boolean_list[i])\n", "            buttons_list.append(x) # this list will help identify buttons visibility\n", "            data.extend([trace])\n", "    \n", "    for x in df_ch_list:       \n", "        buttons_upd = list([dict(label = x,\n", "                                method = 'update',\n", "                                args = [{'visible': [i==x for i in buttons_list]}])])\n", "        buttons.extend(buttons_upd)\n", "    \n", "    buttons_all = list([dict(label = 'All channels',\n", "                             method = 'update',\n", "                             args = [{'visible': [True for x in buttons_list]}])]) \n", "\n", "    buttons.extend(buttons_all)\n", "    \n", "    update_menus = list([dict(active=1,\n", "                              buttons = buttons,\n", "                              direction = 'down',\n", "                              pad = {'r': 10, 't': 10},\n", "                              showactive = True,\n", "                              x = 0.001,\n", "                              xanchor = 'left',\n", "                              y = 1.1,\n", "                              yanchor = 'top')])\n", "    \n", "    layout = dict(title = 'Videos of trending channels ({})'.format(country),\n", "                  xaxis = dict(title = ''),\n", "                  yaxis = dict(title = 'views'),\n", "                  updatemenus = update_menus,\n", "                  width=800, height=600, legend=dict(orientation=\"h\"))\n", "         \n", "    fig = dict(data = data, layout = layout)\n", "    \n", "    iplot(fig)"], "execution_count": null, "metadata": {"_cell_guid": "2e630b96-3cc7-49c7-9ca8-673d74ef84dd", "collapsed": true, "_uuid": "6381239ace716650174d4651a9e214a039751606"}}, {"cell_type": "code", "outputs": [], "source": ["channels_insight(gb_vid_df, \"GB\", date(2017,9,13), date(2017,9,24), 12)"], "execution_count": null, "metadata": {"_cell_guid": "88ad2bac-4ccd-45df-87fe-be35ce14f396", "scrolled": true, "_uuid": "f4df06f0c612ba35683df42a93fed58d050fb2f7"}}, {"cell_type": "code", "outputs": [], "source": ["channels_insight(us_vid_df, \"US\", date(2017,9,13), date(2017,9,24), 12)"], "execution_count": null, "metadata": {"_cell_guid": "3a3bee6e-6700-4b48-af52-60f6d318b372", "_uuid": "35e857d6ed528037ac200f68781f42b7aa5cf1f3"}}, {"cell_type": "markdown", "source": ["The best way to see the situation with 1 trending video per 1 trending channel is to select The Tonight Show Starring Jimmy Fallon for both countries.\n", "\n", "Each has 7 videos per selected period. Sometimes it is only one day for a video to be in trends. Then other video of the channel replaces it in top 200. This looks very irrational. Why can't two or more videos of the one channel be in trend? Well, it is something I will try to find out using this database or other sources.\n", "\n", "Also, what is clear now is that trending doesn't relay solely on views. Dynamic must be more important that row number of views."], "metadata": {"_cell_guid": "700904ad-9b72-4d18-97dc-3ea1d4b4a2d4", "_uuid": "f3fffac3b797cf437a6362142f7273b405515240"}}, {"cell_type": "markdown", "source": ["> # 3. Analysis by categories"], "metadata": {"_cell_guid": "cde0cfdb-663d-4470-92b8-0582fa2ee0f2", "_uuid": "99802e41a21d4b5c0f2e73d5841c9fd7113cd94b"}}, {"cell_type": "markdown", "source": ["Categories will help to better understand nature of channels. They can be retreived from JSON file."], "metadata": {"_cell_guid": "bcea4ae4-49f2-4ce8-88c4-8cb775ae26c7", "_uuid": "9a69deaa2bd66db287e40070cf4b645821518848"}}, {"cell_type": "code", "outputs": [], "source": ["# dictionary for GB database\n", "with open('../input/GB_category_id.json') as json_data:\n", "    gb_from_json = json.load(json_data)\n", "gb_prelim_dict = gb_from_json['items']\n", "json_data.close()\n", "\n", "gb_cat_dict = {}\n", "for i in range (0,len(gb_prelim_dict)):\n", "    cat_key = int(gb_prelim_dict[i]['id'])\n", "    cat_item = gb_prelim_dict[i]['snippet']['title']\n", "    gb_cat_dict[cat_key] = cat_item\n", "gb_cat_dict[29] = 'Schools & Hospitals'\n", "    \n", "# dictionary for US database\n", "with open('../input/GB_category_id.json') as json_data:\n", "    us_from_json = json.load(json_data)\n", "us_prelim_dict = us_from_json['items']\n", "json_data.close()\n", "\n", "us_cat_dict = {}\n", "for i in range (0,len(us_prelim_dict)):\n", "    cat_key = int(us_prelim_dict[i]['id'])\n", "    cat_item = us_prelim_dict[i]['snippet']['title']\n", "    us_cat_dict[cat_key] = cat_item\n", "gb_cat_dict[29] = 'Schools & Hospitals'\n", "\n", "# addition of categories to both databases\n", "gb_vid_df['category_name'] = gb_vid_df['category_id'].apply(lambda x: gb_cat_dict[x])\n", "gb_vid_df.head(5)"], "execution_count": null, "metadata": {"_cell_guid": "c11a3654-b9d5-4729-b923-9122c8941acf", "_uuid": "f8837a6f56d816632e4d533f538ffe5bd62256b9"}}, {"cell_type": "markdown", "source": ["The section is in progress..."], "metadata": {"_cell_guid": "5f192974-8584-47a7-aa84-bf4c6aad7272", "_uuid": "5c62b0c9ff61de6e45ba3187a250822a8cf952db"}}, {"cell_type": "markdown", "source": ["# 4. Views, likes, dislikes and comments"], "metadata": {"_cell_guid": "26c50219-02e2-41df-84cd-b08917543d91", "_uuid": "022624ed4346813aa78b0af32c92c20212a3d164"}}, {"cell_type": "code", "outputs": [], "source": ["gb_vid_df.rename(columns={'comment_total':'comments'}, inplace=True)\n", "us_vid_df.rename(columns={'comment_total':'comments'}, inplace=True)"], "execution_count": null, "metadata": {"_cell_guid": "6c2b7355-970a-4cfa-91a8-5b4e5aea7ee3", "collapsed": true, "_uuid": "38161218b87a3afeaf6a5425bbbdec8b2a7b0e49"}}, {"cell_type": "markdown", "source": ["There must be a strong positive correlation between views and all other feature (comments, likes, dislikes). The logic behind is simple: more views - more expression of opinion in any of its form."], "metadata": {"_cell_guid": "9e1a6705-a5b4-4229-b021-fcfd6e28686b", "_uuid": "a7dddfa43e2d89785e78abdce18161cb6a1f40a0"}}, {"cell_type": "code", "outputs": [], "source": ["corr_gb = gb_vid_df.loc[:,['views', 'likes', 'dislikes', 'comments']].corr()\n", "corr_us = us_vid_df.loc[:,['views', 'likes', 'dislikes', 'comments']].corr()\n", "\n", "mask = np.zeros_like(corr_gb, dtype=np.bool)\n", "mask[np.triu_indices_from(mask)] = True\n", "\n", "fig = plt.figure(figsize=(14, 5))\n", "gs = gridspec.GridSpec(1, 2, width_ratios=[1,1])\n", "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n", "\n", "ax1 = plt.subplot(gs[0,0])\n", "ax1 = sns.heatmap(corr_gb, cmap=cmap, vmax=1, center=0.5,\n", "            square=True, linewidths=.5, mask=mask)\n", "ax1.set_title('Correlation matrix (GB)', fontsize=20)\n", "plt.yticks(rotation=0)\n", "\n", "ax2 = plt.subplot(gs[0,1])\n", "ax1 = sns.heatmap(corr_us, cmap=cmap, vmax=1, center=0.5,\n", "                  square=True, linewidths=.5, mask=mask)\n", "ax2.set_title('Correlation matrix (US)', fontsize=20)\n", "plt.yticks(rotation=0)\n", "\n", "plt.show()"], "execution_count": null, "metadata": {"_cell_guid": "33b3553d-326e-49d5-a0c9-d9ad310453b8", "_uuid": "9731ac26dac8cad244abba594e40a8a828336359"}}, {"cell_type": "markdown", "source": ["For GB segment of YouTube everything is very straightforward - views bring other activities. The US segment is different - dislikes seems to have their own path despite what happens with views, comments and likes. "], "metadata": {"_cell_guid": "07c4ebb3-fde8-4086-a4af-8e87d6f948a9", "_uuid": "57b7891590859fd9f27b969b30d5eeb5f706be0c"}}, {"cell_type": "markdown", "source": ["Visualization of these features should help a lot. Next part of the code provides functions for plotting views, comments, likes and dislikes for whole period of database. The plot is cumulative. Decreases in trend-line occur when a video with big stats is excluded from the pool of 200."], "metadata": {"_cell_guid": "10aeac00-40e9-4f4a-b9fb-70e4ef6404eb", "_uuid": "74eafd1f6f59e7de7e15a87df0b6325b8bc837d2"}}, {"cell_type": "code", "outputs": [], "source": ["def vldc_plot(df_init, country):\n", "    df = df_init[['date','views', 'likes',\n", "             'dislikes','comments']].groupby('date', as_index = False).sum()\n", "    df[['views','likes', 'dislikes','comments']] = df[['views', 'likes',\n", "                                                       'dislikes', 'comments']].apply(lambda x: x/1000000)\n", "    fig = plt.figure(figsize=(12, 5))\n", "    gs = gridspec.GridSpec(2, 2, width_ratios=[1,1])\n", "\n", "    ax1 = plt.subplot(gs[0,0])\n", "    ax1.plot(df['date'], df['views'])\n", "    ax1.set_xticklabels(\"\")\n", "    ax1.set_title('Views, mln', fontsize=20)\n", "\n", "    ax2 = plt.subplot(gs[0,1])\n", "    ax2.plot(df['date'], df['comments'])\n", "    ax2.set_xticklabels(\"\")\n", "    ax2.set_title('Comments, mln', fontsize=20)\n", "\n", "    ax3 = plt.subplot(gs[1,0])\n", "    ax3.plot(df['date'], df['likes'])\n", "    ax3.set_xticks(df['date'].values)\n", "    ax3.set_xticklabels(df['date'], rotation=45)\n", "    ax3.set_title('Likes, mln', fontsize=20)\n", "\n", "    ax4 = plt.subplot(gs[1,1])\n", "    ax4.plot(df['date'], df['dislikes'])\n", "    ax4.set_xticks(df['date'].values)\n", "    ax4.set_xticklabels(df['date'], rotation=45)\n", "    ax4.set_title('Dislikes, mln', fontsize=20)\n", "\n", "    plt.suptitle('{} segment'.format(country), fontsize=22)\n", "\n", "    plt.show()"], "execution_count": null, "metadata": {"_cell_guid": "362e19bd-df5c-4131-93d2-36d9218c34d9", "collapsed": true, "_uuid": "bc6236e25bbba11fbd20e369d420a27f7fa7f17a"}}, {"cell_type": "code", "outputs": [], "source": ["vldc_plot(gb_vid_df, 'GB')"], "execution_count": null, "metadata": {"_cell_guid": "d22a2b5d-05e6-4075-96cd-c774a833c36e", "_uuid": "34622ff112a8c7581deaf4d9665cab66533ae7ee"}}, {"cell_type": "code", "outputs": [], "source": ["vldc_plot(us_vid_df, 'US')"], "execution_count": null, "metadata": {"_cell_guid": "54d52f2d-c878-4e30-8b9a-03286a28de9e", "_uuid": "c3037d191f6be4a3ee10ab23e19254bb60c24a92"}}, {"cell_type": "markdown", "source": ["Week correlation between dislikes and everything else in US segment is mostly guided by 13 September where high views and many likes had relatively small amount of comments and dislikes.  The database is updated regularly and is going to embrace longer period. After some time the correlation will relay less on daily data."], "metadata": {"_cell_guid": "bfa78b51-8e5f-4fc3-a130-f94f8548b4e5", "_uuid": "dba206589ce8bd2b24dbf3901a26ffe479ca8ee7"}}, {"cell_type": "code", "outputs": [], "source": ["us_vid_df_corr_check = gb_vid_df.loc[gb_vid_df['date']!=date(2017,9,13),]\n", "corr_us = us_vid_df_corr_check.loc[:,['views', 'likes', 'dislikes', 'comments']].corr()\n", "corr_us"], "execution_count": null, "metadata": {"_cell_guid": "74539faa-2982-4758-abee-e7e9d9264626", "_uuid": "7a4d74fbdf96a40dc9291391a8159e340cedaebf"}}, {"cell_type": "markdown", "source": ["Another interesting observation - cumulative amount of views for GB reached over 300 mln. and for US only 240 mln. The first thought is there must be some mistake in code or database - US has bigger YouTube audience than other countries. But first chapter of this analysis gave picture of how long a video is in top 200. In US segment this turnover is higher than in GB's. Thus for GB segment the videos with lots of views stay in top 200 and contribute to total amount of views. At the same time in US segment new videos with less views replace old ones."], "metadata": {"_cell_guid": "5aadcdc8-f640-40ea-a781-ccbd43824d72", "_uuid": "5f6b19e5f6ffebbdbfd00b5ed0ced42ac2fe1309"}}, {"cell_type": "markdown", "source": ["# 5. Predicting tags    "], "metadata": {"_cell_guid": "f008a628-ba99-4d0e-868e-1ac6d16aef29", "_uuid": "9d759a6a0e5274fab8230ad6671cd62138f81300"}}, {"cell_type": "markdown", "source": ["Each video has tags. They are made by videos' authors. But maybe we can try to predict them based on comments?\n", "These are the sources of inspiration:\n", "\n", "https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags/kernels\n", "\n", "https://web.stanford.edu/class/cs224n/reports/2759891.pdf\n", "\n", "https://www.kaggle.com/l3nnys/useful-text-preprocessing-on-the-datasets"], "metadata": {"_cell_guid": "0c65d6cb-2c49-4148-bade-7fd80ebb72a2", "_uuid": "4b0ae5b5e1401a031c702d52b970acca04ed6830"}}, {"cell_type": "markdown", "source": ["The first step is to convert tags from string to list of strings."], "metadata": {"_cell_guid": "56806c0a-c9c7-4920-8069-280f7eff42a0", "_uuid": "cb9e470fb5fa64a6a1027eeff8f22d93ae5eec9f"}}, {"cell_type": "code", "outputs": [], "source": ["gb_vid_df['tags'] = gb_vid_df['tags'].map(lambda x: x.split('|'))\n", "us_vid_df['tags'] = us_vid_df['tags'].map(lambda x: x.split('|'))"], "execution_count": null, "metadata": {"_cell_guid": "57ca8f8f-50cb-40ac-acd3-55c24c60a31d", "collapsed": true, "_uuid": "e368451cb03728b1c76cf83d0e32d07ce8db2822"}}, {"cell_type": "markdown", "source": ["There are few rows that have more collumns than they should. Due to insignificant amount it is safe to exclude them."], "metadata": {"_cell_guid": "2e8fa993-f7c7-402b-95e0-ac47fa8b17fe", "_uuid": "145f87f5f87de5fdcf8653dcb4f5a135b6843b01"}}, {"cell_type": "code", "outputs": [], "source": ["# downloading the data and skip bad rows\n", "gb_comm_df =  pd.read_csv(\"../input/GBcomments.csv\", error_bad_lines = False)\n", "us_comm_df =  pd.read_csv(\"../input/UScomments.csv\", error_bad_lines = False)"], "execution_count": null, "metadata": {"_cell_guid": "986508ad-1767-485a-b767-561ef28342fe", "_uuid": "331a2a3ca1506a51774c85fced33a26e9abb9cae"}}, {"cell_type": "markdown", "source": ["Comments database containes infromation on same videos. US and GB databases could be combined to provide full picture for any video. \n", "\n", "It is possible a video lasts longer in one database than in the other, but the goal of this part is to predict tags, so it is ok to combine all available information in one place."], "metadata": {"_cell_guid": "9b1d4acb-a026-40ae-b270-3455bfdcd876", "_uuid": "12ccc6b502c237f62ee7058e68a3444542e18246"}}, {"cell_type": "code", "outputs": [], "source": ["# combined database on videos features (will be used for vlookup only)\n", "all_vid_df = gb_vid_df.append(us_vid_df)\n", "all_vid_df.sort_values('date', ascending=False, inplace=True)\n", "# leave only most recent date for each video\n", "all_vid_df.drop_duplicates('video_id', keep = 'first', inplace = True)\n", "\n", "\n", "# combined database for comments\n", "all_comm_df = gb_comm_df.append(us_comm_df)\n", "# duplicates should be deleted\n", "all_comm_df.drop_duplicates(['video_id', 'comment_text'], keep = 'first', inplace = True)\n", "\n", "all_comm_df = all_comm_df.join(all_vid_df.set_index('video_id')[['tags',\n", "                                                               'channel_title',\n", "                                                               'title']], on = 'video_id')\n", "# make additional column for number of tags\n", "all_comm_df['num_tags'] = all_comm_df['tags'].apply(lambda x: len(x))\n", "all_comm_df.shape"], "execution_count": null, "metadata": {"_cell_guid": "550987a2-171b-48e1-b4a5-d36188c27c7b", "_uuid": "ee28f10888f036f8fc109a938041b5d61899bd18"}}, {"cell_type": "markdown", "source": ["To make predictions I need to get rid of different artifacts (html characters and emojis) and exclude non-ASCII characters. Following functions should help with this task: "], "metadata": {"_cell_guid": "273dbac7-5142-4e07-a994-e0921ae923f8", "_uuid": "51a18a68f8e9c6218b7d9dff6214ae43b92a6079"}}, {"cell_type": "code", "outputs": [], "source": ["# function for cleaning urls\n", "def remove_urls(x):\n", "    return re.sub(r\"http\\S+\", \"\", x)"], "execution_count": null, "metadata": {"_cell_guid": "e9345659-7a17-4671-9937-f1376676624e", "collapsed": true, "_uuid": "1d81f70e1d41f01e56aa133e8a76c04aa11f2499"}}, {"cell_type": "code", "outputs": [], "source": ["# function for cleaning text of html tags and uris\n", "\n", "# thanks to https://www.kaggle.com/l3nnys/useful-text-preprocessing-on-the-datasets\n", "uri_re = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))'\n", "\n", "def stripTagsAndUris(x):\n", "    # BeautifulSoup on content\n", "    soup = BeautifulSoup(x, \"html.parser\")\n", "    # Stripping all <code> tags with their content if any\n", "    if soup.code:\n", "        soup.code.decompose()\n", "    # Get all the text out of the html\n", "    text =  soup.get_text()\n", "    # Returning text stripping out all uris\n", "    return re.sub(uri_re, \"\", text)"], "execution_count": null, "metadata": {"_cell_guid": "7f928aeb-9e15-4d67-a3e6-4e3f3ce56e70", "collapsed": true, "_uuid": "e31c7ff61edf0d9bfbe4217cf89d259165a115c5"}}, {"cell_type": "code", "outputs": [], "source": ["# emoji pattern (https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python)\n", "emoji_pattern = re.compile(\"[\"\n", "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n", "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n", "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n", "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n", "                           \"]+\", flags=re.UNICODE)\n", "\n", "# function for removal of non-ASCII characters and emoji\n", "def exASCII(x):\n", "    # Lowercasing all words\n", "    x = x.lower()\n", "    # Removing non ASCII chars\n", "    ex_ascii = re.sub(r'[^\\x00-\\x7f]',r'',x)\n", "    # remove emoji\n", "    return emoji_pattern.sub(r'', ex_ascii)"], "execution_count": null, "metadata": {"_cell_guid": "e6143ab1-7f74-4904-adf0-6782e3653729", "collapsed": true, "_uuid": "6b7dfe2d2ec531fbefee0687fe2942680aa3945e"}}, {"cell_type": "code", "outputs": [], "source": ["# replace trippled+ characters\n", "# https://stackoverflow.com/questions/43110237/replace-consecutive-repeated-characters-with-one-column-wise-operation-pand\n", "def trim_trippled(x):\n", "    min_threshold_rep = 3\n", "    return re.sub(r'(.)\\1{%d,}'%(min_threshold_rep-1), r'\\1',x)"], "execution_count": null, "metadata": {"_cell_guid": "0fd88a13-46e2-4d88-8e42-b4d522f5de57", "collapsed": true, "_uuid": "d38eb6e80e05589619a6c6551945b77196ad9677"}}, {"cell_type": "code", "outputs": [], "source": ["# change shorcuts to full words, replace remains of unwanted characters\n", "# thanks to https://www.analyticsvidhya.com/blog/2014/11/text-data-cleaning-steps-python/\n", "apts = {\"'s\": \" is\",\n", "        \"'ll\": \" will\",\n", "        \"'re\": \" are\",\n", "        \"'d\": \" would\",\n", "        \"'t\": \" not\",\n", "        \"'m\": \" am\",\n", "        \"'ve\": \" have\",\n", "        \"&\": \" and \",\n", "        \"\\*\": \" \",\n", "        r'\\\\n': \" \",\n", "        '[%_=>#\"(--)\\)\\(:\\^~\\<( - )( / )([\\\\\\/\\])]': \" \",\n", "        \"[^(\\d]\\d+\\)\": \" \",\n", "        \"([\\d|\\d\\d]):([\\d|\\d\\d])(:([\\d|\\d\\d]))?\": \" \"}\n", "\n", "# exclude double spaces left after previous step\n", "rem = {'  +': ' '}\n", "\n", "def ex_apts_rem(x):\n", "    for i in apts:\n", "        x = re.sub(i, apts[i], x)\n", "    for i in rem:\n", "        x = re.sub(i, rem[i], x)\n", "    return x"], "execution_count": null, "metadata": {"_cell_guid": "b1621631-2637-40a5-90ef-852586d3cc78", "collapsed": true, "_uuid": "c17ba4f0dec92d89e382bb3ece0e6c3570557799"}}, {"cell_type": "code", "outputs": [], "source": ["# leave only those words that correspond to selected parts of speech\n", "def nlp_filter(x):\n", "    tokens = nlp(x)\n", "    tags = []\n", "    list_of_tags = ['CD','FW','JJ','JJR','JJS','NN','NNP','NNPS','NNS','RB','RBR','RBS']\n", "    for token in tokens:\n", "        tags.append((token.lemma_, token.tag_))\n", "    filtered_list = [(n,y) for (n,y) in tags if y in list_of_tags]\n", "    x = \" \".join([n for (n,y) in filtered_list])\n", "    return x"], "execution_count": null, "metadata": {"_cell_guid": "c0e6ccb0-485e-42c6-b690-72f22d60b1a8", "collapsed": true, "_uuid": "261fc7eb3754ad115a6864e48431b38b0f7859ec"}}, {"cell_type": "code", "outputs": [], "source": ["# function that embraces all previous text preparation process\n", "def preprocess_functions(x):\n", "    return (ex_apts_rem(trim_trippled(exASCII(stripTagsAndUris(remove_urls(x))))))"], "execution_count": null, "metadata": {"_cell_guid": "c83659ab-c985-4e78-8b84-16ef8af51ef1", "collapsed": true, "_uuid": "be2e3e16b8986a0d71cb640d9026f81ea817514b"}}, {"cell_type": "markdown", "source": ["Now functions above can be applied to comments column."], "metadata": {"_cell_guid": "9d160d54-b4d5-474b-b2e4-5b759dec549f", "_uuid": "469c290d55f2bd252c2a8349b8a3786252411be0"}}, {"cell_type": "code", "outputs": [], "source": ["# prepare comments section\n", "comments_df = all_comm_df.copy()\n", "comments_df['comment_text'] = comments_df['comment_text'].apply(str)\n", "comments_df['comment_text_proc'] = comments_df['comment_text'].apply(lambda x: preprocess_functions(str(x)))"], "execution_count": null, "metadata": {"_cell_guid": "bb3b265e-afa0-455c-b723-cd7bb868b4c2", "_uuid": "72ca919a38a75e5e7843305de8c5e4e0f534d1ed"}}, {"cell_type": "markdown", "source": ["There are handful of videos that are titled, tagged and commented in languages other than English. To simplify the prediction I exclude them using following procedure: \n", "1.  Create comments column without emojis but with non-ASCII characters.\n", "2. Put all videos that have more than 50% of comments made with non-ASCII characters into special list.\n", "3. Exclude all items these videos from the final set.\n", "\n", "Also videos with less than 10 comments were excluded as in most cases they provide insufficient amount of text for prediction. "], "metadata": {"_cell_guid": "cd62d3d0-57f9-42ff-a0ef-cadef1d6d2cf", "_uuid": "2d6f4c707844407e860c26618565ffe37a319c37"}}, {"cell_type": "code", "outputs": [], "source": ["# create comments column where emojis are excluded but other non-ASCII characters are preserved\n", "comments_df['no_emo_comments'] = comments_df['comment_text'].apply(lambda x: emoji_pattern.sub(r'', x))\n", "\n", "# videos where most of the comments made with non-ASCII characters\n", "vid_list = comments_df['video_id'].unique()\n", "vid_list_nonASCII = []\n", "for i in vid_list:\n", "    temp_df = comments_df.loc[comments_df['video_id'] == i, ]\n", "    nonASCII = temp_df['no_emo_comments'].str.contains(r'[^\\x00-\\x7f]').sum() # number of comment with non-ASCII characters\n", "    all_com = temp_df['no_emo_comments'].count() # total number of comments\n", "    if (nonASCII/all_com) > 0.5:\n", "        vid_list_nonASCII.append(i)\n", "\n", "vid_list_few_comments = []\n", "for i in vid_list:\n", "    temp_df = comments_df.loc[comments_df['video_id'] == i, ]\n", "    if (temp_df['video_id'].count()) < 10:\n", "        vid_list_few_comments.append(i)\n", "\n", "del temp_df"], "execution_count": null, "metadata": {"_cell_guid": "40ea02a4-23a4-4f45-8a7b-962783820627", "collapsed": true, "_uuid": "334c18462887088c09b4041e46840042918c6fe8"}}, {"cell_type": "code", "outputs": [], "source": ["# exclude videos selected in previous step\n", "vid_list_prunned = [x for x in vid_list if x not in vid_list_nonASCII]\n", "vid_list_prunned = [x for x in vid_list_prunned if x not in vid_list_few_comments]"], "execution_count": null, "metadata": {"_cell_guid": "e9412b11-0e43-4021-bc05-5a2aac0f6eca", "collapsed": true, "_uuid": "5bce92773f542bfd9bcc21a7082798a77db1e21b"}}, {"cell_type": "markdown", "source": ["Predictions are made with TfidfVectorizer per each video. Most of the parameters are identical but two of them are calculated for each video: number of tags to be predicted (n_tags) and maximum n_gram range (max_ngram).\n", "\n", "Thus number of predicted tags and number of words in predicted tags are in line with ground truth."], "metadata": {}}, {"cell_type": "code", "outputs": [], "source": ["# tags predictions\n", "tags_list = []\n", "for i in vid_list_prunned:\n", "    temp_df = comments_df.loc[comments_df['video_id'] == i, ]\n", "    n_tags = len(temp_df['tags'].values[0])\n", "    max_ngram = max([len(n.split()) for n in temp_df['tags'].values[0]])\n", "    tfidf = TfidfVectorizer(min_df = 0.03,\n", "                            max_features = 10000,\n", "                            analyzer = 'word', \n", "                            token_pattern = r'\\w+',\n", "                            ngram_range = (1, min(max_ngram,3)), # limit n-grams by 3 words max\n", "                            use_idf = True,\n", "                            smooth_idf = True,\n", "                            sublinear_tf = True,\n", "                            stop_words = 'english')\n", "    tfidf_matrix = tfidf.fit_transform(temp_df['comment_text_proc'].values)\n", "    tags_array = np.array(tfidf.get_feature_names())\n", "    tags_sorted = np.argsort(tfidf_matrix.toarray()).flatten()[::-1]\n", "    sel = tags_sorted[:n_tags]\n", "    selected_tags = tags_array[sel]\n", "    tags_list.append(selected_tags.tolist())\n", "print (\"Prediction is done.\")"], "execution_count": null, "metadata": {"_cell_guid": "45d10e74-94c9-4d0d-b1e1-60c93d74924b", "_uuid": "dedfec1ee5abda6b70f21fde8f80d695ac6d2ad5"}}, {"cell_type": "code", "outputs": [], "source": ["# add predicted tags to comments dataframe\n", "tags_df = pd.DataFrame({\"video_id\": vid_list_prunned, \"tags_pred\": tags_list})\n", "comments_df_selected = comments_df.copy().loc[comments_df['video_id'].isin(vid_list_prunned),]\n", "comments_df_selected = comments_df_selected.merge(tags_df, how = \"left\", left_on = \"video_id\", right_on = \"video_id\")\n", "comments_df_selected.head(3)"], "execution_count": null, "metadata": {"_cell_guid": "ed9e8ec2-a83d-4607-8507-7f6ed17a5d5f", "_uuid": "a75f2a5d2f43aa73db6c9dd8d5943b07573207c8"}}, {"cell_type": "markdown", "source": ["Now it is time to check prediction quality. To do this I took 5 randomly selected videos and printed title, tags and predicted tags per each video."], "metadata": {}}, {"cell_type": "code", "outputs": [], "source": ["check_list = random.sample(vid_list_prunned,5)\n", "print (check_list)\n", "for i in check_list:\n", "    temp_df = comments_df_selected.loc[comments_df_selected['video_id'] == i, ]\n", "    print (\"\\nVideo title:\")\n", "    print (temp_df['title'].values[0])\n", "    print (\"\\nGround truth tags:\")\n", "    print (temp_df['tags'].values[0])\n", "    print (\"\\nPredicted tags:\")\n", "    print (temp_df['tags_pred'].values[0])\n", "    print (\"--------------------\")"], "execution_count": null, "metadata": {}}, {"cell_type": "markdown", "source": ["Predicted tags are far from being precise and there are few reasons why it happens:\n", "1. Video creators make many tags to ensure that video will be found by YouTube search engine. It includes adding common misspells and words permutation.\n", "2. Discussion in comments can shift away from the video topic. Also comments may have little sense. The interesting option is to use only comments with big amount of likes but they are not always available per each video.\n", "3. The method is very susceptible to min_df parameters. Results were better with its higher value, but not all videos have enough comments to support higher values.\n", "\n", "Despite these drawbacks this method provides some grasp on video topic. Also It shows good insight into discussion tone. I must admit that I've made few runs of random search to get 5 examples without offensive language."], "metadata": {}}, {"cell_type": "markdown", "source": ["# 6. Emojis in comments"], "metadata": {}}, {"cell_type": "markdown", "source": ["How popular different emojis are? It can be visualized with plotly."], "metadata": {"_cell_guid": "fb94b156-fce9-42f7-9350-ac9f3a3d1bdf", "_uuid": "bcba0935e7e2fbe9eee7c932fabe2fe7b480892a"}}, {"cell_type": "code", "outputs": [], "source": ["# prepare emojis list\n", "all_comm_df['emoji'] = all_comm_df['comment_text'].apply(lambda x: re.findall(emoji_pattern, str(x)))\n", "all_comm_df['emoji'] = all_comm_df['emoji'].apply(lambda x: [i for sublist in x for i in sublist])\n", "emo_list = all_comm_df['emoji'].values.tolist()\n", "emo_list_flatten = [i for sublist in emo_list for i in sublist]\n", "\n", "# only unique values from the list\n", "emo_unique = list(set(emo_list_flatten))\n", "emo_all = \" \".join(emo_list_flatten)"], "execution_count": null, "metadata": {"_cell_guid": "b449682a-b980-4b2e-8343-0a7b8bb3a4d6", "_uuid": "d6b3f0aff0b139ac8c45c7d8aa1af2892999703d"}}, {"cell_type": "code", "outputs": [], "source": ["# dictionary for emoji frequency\n", "emo_dict = {}\n", "for i in emo_unique:\n", "    emo_dict[i] = len(re.findall(i, emo_all))"], "execution_count": null, "metadata": {"_cell_guid": "05b7aa43-74b2-442b-a93e-3df866d77129", "collapsed": true, "_uuid": "5e7ba8c3845c21d3b238ca35f93c0e81ecaf6a4c"}}, {"cell_type": "code", "outputs": [], "source": ["# top 15 emoji with Plotly\n", "emo_df = pd.DataFrame.from_dict(emo_dict, orient = 'index')\n", "emo_df = emo_df.reset_index()\n", "emo_df.columns = [\"emoji\", 'frequency']\n", "emo_df.sort_values('frequency', ascending = False, inplace = True)\n", "emo_df_top15 = emo_df.head(15).copy()\n", "emo_df_top15.sort_values('frequency', ascending = True, inplace = True)\n", "\n", "# plot\n", "data = [go.Bar(x = emo_df_top15['frequency'],\n", "               y = emo_df_top15['emoji'],\n", "               orientation = 'h',\n", "               marker = dict(color = 'rgba(200, 100, 1, 0.6)',\n", "                             line = dict(color = 'rgba(50, 171, 96, 1.0)',\n", "                                         width = 1)))]\n", "\n", "layout = dict(title = '15 most frequent emojis')\n", "fig = dict(data = data, layout = layout)\n", "iplot(fig, filename='basic-bar')"], "execution_count": null, "metadata": {"_cell_guid": "4ab45a84-ab7f-4769-b6c6-1c1bc53960ab", "_uuid": "324b3fa012815b455e6d05f072d09964d171f480"}}], "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "version": "3.6.3", "name": "python"}}}