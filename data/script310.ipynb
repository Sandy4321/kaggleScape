{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"# About the dataset\nThis dataset is about the show about \"nothing\". Yeah, you guessed it right. I am talking about Seinfeld, one of the greatest sitcoms of all time. This dataset provides episodic analysis of the series including the entire script and significant amount of information about each episode."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true},"cell_type":"markdown","source":"# What have I done here?\nI have tried to create a classifier to predict the name of the speaker of a given dialogue. I have used the scripts database for this purpose. So, if a line is given to the predictor, it returns the person who said or might say that line."},{"metadata":{"_uuid":"2ccbc52ffbcd2fd3133867b3480042e1cfeef3f1","_cell_guid":"28754255-4350-483b-b1d4-fe30646b8165","collapsed":true,"trusted":false},"cell_type":"code","source":"# Importing libraries and data\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# importing data\ndf = pd.read_csv(\"../input/scripts.csv\")\ndel df[\"Unnamed: 0\"]\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"166e3523-a279-4d90-b0e8-7075650be300","_uuid":"15b6c4273a825c57aed3be7d6006f7eee6e38e96"},"cell_type":"markdown","source":"Now, we don't really require the EpisodeNo, SEID and Season columns so we remove them."},{"metadata":{"_uuid":"4f0e0466b74d9b11af7f8662925d05eeecee9ee3","_cell_guid":"383b05c8-ba69-4f38-8497-71af0964efbb","collapsed":true,"trusted":false},"cell_type":"code","source":"dial_df = df.drop([\"EpisodeNo\",\"SEID\",\"Season\"],axis=1)\ndial_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f3c2a00c-f765-4cdb-896f-85cd5ee1d449","_uuid":"f6e7cc8a74ecb2920ca21a846d9b6c170496f828"},"cell_type":"markdown","source":"Time for some EDA"},{"metadata":{"_cell_guid":"6ae2e794-a833-42c9-8dfd-8e51d41868fc","_uuid":"3657df405d9acfc0bea43cb1bda8976d4d8d9bd6"},"cell_type":"markdown","source":"# Plot by number of dialogues spoken"},{"metadata":{"_uuid":"376bb4f62022e7f578ead32f41edba1d7f15d7a2","_cell_guid":"5189361f-5927-4353-a6f9-392cf1c0dbce","collapsed":true,"trusted":false},"cell_type":"code","source":"dial_df[\"Character\"].value_counts().head(12).plot(kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2880f8eb-bc40-4030-9699-9a2fe2f4b15b","_uuid":"1f4f2f10792ec3103f40bddd8fa70873fdad237f"},"cell_type":"markdown","source":"For creating a corpus out of the data, we will create a datframe concatenating all dialogues of a character. We are choosing 12 characters(by number of dialogues) "},{"metadata":{"_uuid":"4651d19e625d35d9c70695c4697d2e7a7a425475","_cell_guid":"0ed6e7e4-152b-4e90-afb5-42cdef5890ea","collapsed":true,"trusted":false},"cell_type":"code","source":"def corpus_creator(name):\n    st = \"\" \n    for i in dial_df[\"Dialogue\"][dial_df[\"Character\"]==name]:\n        st = st + i\n    return st\n\ncorpus_df = pd.DataFrame()\ncorpus_df[\"Character\"] = list(dial_df[\"Character\"].value_counts().head(12).index)\n\nli = []\nfor i in corpus_df[\"Character\"]:\n    li.append(corpus_creator(i))\n\ncorpus_df[\"Dialogues\"] = li\n\ncorpus_df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5fd47394-c8ae-40a6-a66d-8678295df557","_uuid":"7f64aef38d0ecb7a9cd08efd1af024057125dbd7"},"cell_type":"markdown","source":"# Preparing stopwords(words that are obsolete for NLP or words that hinder modelling). Very helpful in human speech but useless when trained for modelling"},{"metadata":{"_uuid":"91468de97026db0e33587ae22c7ca16924acc019","_cell_guid":"a4c77bbd-9b70-46f9-8f3a-72486c2b12f6","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.feature_extraction import text\npunc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\nstop_words = text.ENGLISH_STOP_WORDS.union(punc)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"782583c9-c021-4c65-993a-62d73d47adfb","_uuid":"44503b15dd63ff4aed59bfd3ad1a1ef04fa2d358"},"cell_type":"markdown","source":"Now, we create a text_processor function to tokenize concatenated dialogues and removing stop words"},{"metadata":{"_uuid":"2de975e77ab16410b7fc79010ee4b233dbd3048a","_cell_guid":"a8c13b22-875b-4f8a-b651-805d14e89f4f","collapsed":true,"trusted":false},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\ndef text_processor(dialogue):\n    dialogue = word_tokenize(dialogue)\n    nopunc=[word.lower() for word in dialogue if word not in stop_words]\n    nopunc=' '.join(nopunc)\n    return [word for word in nopunc.split()]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"617a350e-c6fc-4095-aa80-020ebb985b0b","_uuid":"d312026289fc6efe7b28e8cbd5cbdc248467e243"},"cell_type":"markdown","source":"Now, we apply this method"},{"metadata":{"_uuid":"19a8738546383225df356481e9e110f3216e7314","_cell_guid":"2f841939-3b43-4854-add9-f160d7c6c6d5","collapsed":true,"trusted":false},"cell_type":"code","source":"corpus_df[\"Dialogues\"] = corpus_df[\"Dialogues\"].apply(lambda x: text_processor(x))\ncorpus_df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d9c1ed60-0dc5-4580-8b25-cee52f7d5c23","_uuid":"ffdc641c2d04229e874f0060fccc6f4aa58fb9a9"},"cell_type":"markdown","source":"Adding a length column to the new dataframe which contains length of the concatenated dialogues"},{"metadata":{"_uuid":"573951a4b9dd589edf17161dfac666614addcddf","_cell_guid":"b786be4b-c8cf-4401-8c98-9da076a52bcf","collapsed":true,"trusted":false},"cell_type":"code","source":"corpus_df[\"Length\"] = corpus_df[\"Dialogues\"].apply(lambda x: len(x))\ncorpus_df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"df839bc8-fea2-46fb-9899-65cf27f3a805","_uuid":"428f1bcf017d9de10d3ca893ebd1560520b35ed0"},"cell_type":"markdown","source":"# Who has spoken the most in Seinfeld?"},{"metadata":{"_uuid":"f2db908857487f53d786018361df6199f509d04a","_cell_guid":"e1ddc419-73e7-40c8-ac9d-3637bab04613","collapsed":true,"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,10))\nsns.barplot(ax=ax,y=\"Length\",x=\"Character\",data=corpus_df)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a91c213e-c56b-4ca0-9fc5-c2c63080ab06","_uuid":"f4086af1cb3512f1e0645051d4e3296b7ac68857"},"cell_type":"markdown","source":"Obviously, Jerry speaks the most followed by George, Elaine and Kramer"},{"metadata":{"_cell_guid":"586297ec-3850-403f-9a06-353c03c4dfb5","_uuid":"7d05a68aa0c6c9ee4d37243fc14354b838a705e4"},"cell_type":"markdown","source":"Now, we do some **correlation analysis**  to find out how similar are the dialogues of different characters to each other.\n(For this, we will use a library called **gensim**. Next cell is the most important yet.)"},{"metadata":{"_uuid":"139e28190310bb6aef8100a9d9f1678ac7c80468","_cell_guid":"24b59f96-6fef-4e26-a405-f1193d65f536","collapsed":true,"trusted":false},"cell_type":"code","source":"import gensim\n# Creating a dictionary for mapping every word to a number\ndictionary = gensim.corpora.Dictionary(corpus_df[\"Dialogues\"])\nprint(dictionary[567])\nprint(dictionary.token2id['cereal'])\nprint(\"Number of words in dictionary: \",len(dictionary))\n\n# Now, we create a corpus which is a list of bags of words. A bag-of-words representation for a document just lists the number of times each word occurs in the document.\ncorpus = [dictionary.doc2bow(bw) for bw in corpus_df[\"Dialogues\"]]\n\n# Now, we use tf-idf model on our corpus\ntf_idf = gensim.models.TfidfModel(corpus)\n\n# Creating a Similarity objectr\nsims = gensim.similarities.Similarity('',tf_idf[corpus],num_features=len(dictionary))\n\n# Creating a dataframe out of similarities\nsim_list = []\nfor i in range(12):\n    query = dictionary.doc2bow(corpus_df[\"Dialogues\"][i])\n    query_tf_idf = tf_idf[query]\n    sim_list.append(sims[query_tf_idf])\n    \ncorr_df = pd.DataFrame()\nj=0\nfor i in corpus_df[\"Character\"]:\n    corr_df[i] = sim_list[j]\n    j = j + 1   ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4fe45e73-6bda-412a-af45-c1c2f731eaf0","_uuid":"d0f362db76ea587f8d81cc3df2c4eae5a63d63c6"},"cell_type":"markdown","source":"# Heatmap to detect similarity between characters' dialogues"},{"metadata":{"_uuid":"055d4ab85004629afddde377498e706badc2bea4","_cell_guid":"a8c38fc4-528a-4938-a4aa-d6286c02bcf2","collapsed":true,"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(corr_df,ax=ax,annot=True)\nax.set_yticklabels(corpus_df.Character)\nplt.savefig('similarity.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3ebb6797-68c6-42e0-b576-4fe23e811514","_uuid":"2ff33556d0b420053e0e75d36520ed186ec07ad6"},"cell_type":"markdown","source":"This plot can actually depict how different the cahracters are from each other. Like **Jerry, George, Elaine and Kramer** speak highly similar lines. Maybe, that's why they are friends(This might have happened because they are usually talking to each other and also because their dialogues are more than others). **[Setting] has lowest correlation scores** well because it is the odd one out because it is not a person. **Jerry and George have highly similar dialogues with 81%  correlation.**"},{"metadata":{"_cell_guid":"f790bf77-307e-4465-9c21-1d4be8bcbb2d","_uuid":"a27b44987df492c693a7dd180c91ba67096443d2"},"cell_type":"markdown","source":"# An awesome way to use classification\nI am predicting the dialogues said by Elaine, George and Kramer only, so we will choose only their dialogues(I wanted to include Jerry, I mean what is Seinfeld without Jerry Seinfeld but I will later tell you why I didn't do that. Look for clues in markdown)"},{"metadata":{"_uuid":"667ebe9e0f395d37e71ed68e1a8802669d229c72","_cell_guid":"f40e592e-20d3-4452-8a31-353df5a5c4af","collapsed":true,"trusted":false},"cell_type":"code","source":"dial_df = dial_df[(dial_df[\"Character\"]==\"ELAINE\") | (dial_df[\"Character\"]==\"GEORGE\") | (dial_df[\"Character\"]==\"KRAMER\")]\ndial_df.head(8)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b03c5487-ade5-43fc-8350-c6f76fdc99b6","_uuid":"c1044c2a9fd1a066be41cc98d6681246834e1704"},"cell_type":"markdown","source":"Way too many dialogues by george will certainly affect the classifier."},{"metadata":{"_cell_guid":"1f09b641-20c2-4bcf-a82d-5e8a38a84fb1","_uuid":"847e6ff85251ba7d4d710cdfdc4f8916b3d8507c"},"cell_type":"markdown","source":"# A text processor for processing dialogues"},{"metadata":{"_uuid":"36fc6aae8bd75781bb653658efd50d26a0104213","_cell_guid":"e61d92d2-59f1-42a5-841c-1d69a8860be1","collapsed":true,"trusted":false},"cell_type":"code","source":"def text_process(dialogue):\n    nopunc=[word.lower() for word in dialogue if word not in stop_words]\n    nopunc=''.join(nopunc)\n    return [word for word in nopunc.split()]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"648e1aa1-6dfb-4321-8639-3e997b408fac","_uuid":"3ffc5bab1aaabd4d3753fe6e295d8a5f966d8856"},"cell_type":"markdown","source":"# Preparation for classifier"},{"metadata":{"_uuid":"591c53a10469f8d7bf64de51fb25bc0ac57c3c46","_cell_guid":"8c3878ee-1bc3-497b-b938-cd123893c893","collapsed":true,"trusted":false},"cell_type":"code","source":"X = dial_df[\"Dialogue\"]\ny = dial_df[\"Character\"]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eb07c17a-a0f8-4840-aa8a-7fa50a87101f","_uuid":"e18c46b104169b963ebb45e13aed0ec212e59150"},"cell_type":"markdown","source":"# TF-IDF Vectorizer\nConvert a collection of raw documents to a matrix of TF-IDF features. Equivalent to CountVectorizer followed by TfidfTransformer.\n\nIn information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Nowadays, tf-idf is one of the most popular term-weighting schemes; 83% of text-based recommender systems in the domain of digital libraries use tf-idf.\n"},{"metadata":{"_uuid":"4229761199b1b72be2dbdae49fd4cffc37612572","_cell_guid":"3268ea05-90ba-469d-9303-ee480bd5f58d","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(analyzer=text_process).fit(X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb14b74c466d5e6d3b32a764625072c458fb15d7","_cell_guid":"184b27f4-791a-4d6e-a81e-5384891668a7","collapsed":true,"trusted":false},"cell_type":"code","source":"print(len(vectorizer.vocabulary_))\nX = vectorizer.transform(X)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0cdfb7bc09c08b089109377042f67e8a541aab03","_cell_guid":"1f3d2440-d42b-43e5-8987-8aa1a566ed06","collapsed":true,"trusted":false},"cell_type":"code","source":"# Splitting the data into train and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"81e21c93-5e1e-4328-baf8-ea963ce0f304","_uuid":"6b48f3cb33b0218ae2d9588025cad6f33667f859"},"cell_type":"markdown","source":"# Creating a voting classifier with Multinomial Naive Bayes, logistic regression and random forest classifier\nThe EnsembleVoteClassifier is a meta-classifier for combining similar or conceptually different machine learning classifiers for classification via majority or plurality voting.\n\nThe EnsembleVoteClassifier implements \"hard\" and \"soft\" voting. In hard voting, we predict the final class label as the class label that has been predicted most frequently by the classification models. In soft voting, we predict the class labels by averaging the class-probabilities (only recommended if the classifiers are well-calibrated)."},{"metadata":{"_uuid":"f1df6906adb501151eb8a683a38695b71b79ce3e","_cell_guid":"ea4ddc26-d031-43ad-812d-d4e090241c31","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB as MNB\nfrom sklearn.linear_model import LogisticRegression as LR\nfrom sklearn.ensemble import RandomForestClassifier as RFC\nfrom sklearn.ensemble import VotingClassifier as VC\nmnb = MNB(alpha=10)\nlr = LR(random_state=101)\nrfc = RFC(n_estimators=80, criterion=\"entropy\", random_state=42, n_jobs=-1)\nclf = VC(estimators=[('mnb', mnb), ('lr', lr), ('rfc', rfc)], voting='hard')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe7ac3fda0a4d4bdc5cae5e808ffdfd479a2a397","_cell_guid":"5d5bf29f-a772-482f-8a36-9c5a1d8a9549","collapsed":true,"trusted":false},"cell_type":"code","source":"# Fitting and predicting\nclf.fit(X_train,y_train)\n\npredict = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8349274897c5b1f3fd3481e45af8a31baf41892","_cell_guid":"65f75730-e838-4147-8c28-17c834e60c28","collapsed":true,"trusted":false},"cell_type":"code","source":"# Classification report\nfrom sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(y_test, predict))\nprint('\\n')\nprint(classification_report(y_test, predict))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5f9cf7a9-5bd9-4214-9e84-6b8a2a509222","_uuid":"209491d943f189541df90af6386ce6f1de4b385c"},"cell_type":"markdown","source":"So we get about 51% precision which is not bad considering the limited vocablury. George is dominant here due to high recall value of 0.80. So, unless a dialogue has words that don't exist at all in George's vocablury, there is a high chance George will the speaker of most lines. The situation was worse when Jerry was inclued. **This is why I decided to drop Jerry's dialogues from the dataset.** "},{"metadata":{"_cell_guid":"adf485ce-bb7a-48d5-a426-77056bcbca5c","_uuid":"60a91fd31b86e7a3343726d52271a4dc08f5fd8b"},"cell_type":"markdown","source":"# The Predictor"},{"metadata":{"_uuid":"4efe71a54a258eb1c9d9c9f33630fbc5db57c382","_cell_guid":"e13dcdf1-1fbb-4984-b18d-2fbb37b5785f","collapsed":true,"trusted":false},"cell_type":"code","source":"def predictor(s):\n    s = vectorizer.transform(s)\n    pre = clf.predict(s)\n    print(pre)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a0d808b3-e991-41cc-9229-271434209d0d","_uuid":"80d4b8fb28e14cf60a08a6fef2a0b374812bfaf1"},"cell_type":"markdown","source":"# Now, we predict..."},{"metadata":{"_uuid":"7a6331ec7c59df06f052d8e9c73c3c3719491431","_cell_guid":"6e6c662e-1153-4b9b-b4cd-56ef4860391c","collapsed":true,"trusted":false},"cell_type":"code","source":"# Answer should be Kramer\npredictor(['I\\'m on the Mexican, whoa oh oh, radio.'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9a85606236f54455da9917a22602bd8b8d0a64e","_cell_guid":"734c2bea-413f-4a39-acd5-2e450b137996","collapsed":true,"trusted":false},"cell_type":"code","source":"# Answer should be Elaine\npredictor(['Do you have any idea how much time I waste in this apartment?'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ece733e2a0843d4e8f8c3d8db97e046c1fdda24e","_cell_guid":"0bd85172-0040-4fbc-add3-8570f6ea3658","collapsed":true,"trusted":false},"cell_type":"code","source":"# Answer should be George \npredictor(['Yeah. I figured since I was lying about my income for a couple of years, I could afford a fake house in the Hamptons.'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ced7d5498b7997e5d9d873b3aaa21a0f1088f3be","_cell_guid":"94bc15e7-d53a-47f0-8c9d-f9a0e5274d59","collapsed":true,"trusted":false},"cell_type":"code","source":"# Now, a random sentence\npredictor(['Jerry, I\\'m gonna go join the circus.'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f91509a0a67746b39469fb77171cb9e49c1273c9","_cell_guid":"3fe969d5-9132-4f41-8eed-5ebfb9a172eb","collapsed":true,"trusted":false},"cell_type":"code","source":"# A random sentence\npredictor(['I wish we can find some way to move past this.'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c8f78b199ec49ce49d1033526195c7ea07ba2da","_cell_guid":"a7c1f7b3-64d7-45ed-95ec-55c5a46846e3","collapsed":true,"trusted":false},"cell_type":"code","source":"# Answer should be Kramer\npredictor(['You’re becoming one of the glitterati.'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"038682dc2832cab441f97887263c1bcdca6233e7","_cell_guid":"0207576a-7c9f-4340-bb38-e7eb94af4b31","collapsed":true,"trusted":false},"cell_type":"code","source":"# Answer should be Elaine\npredictor(['Jerry, we have to have sex to save the friendship.'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2ea795a5-32b0-44be-b06a-05fdc1a34018","_uuid":"431b4663b535e2a10b963225160803caf7b2a4eb"},"cell_type":"markdown","source":"See, this \"george effect\" can lead to awkward results. That's why I am trying to find a way to get rid of that high recall value and also improve the precision of classifier. I am open to suggestions."},{"metadata":{"_uuid":"6c5d5ad5cdbce3e6fe2d4f9f2665ec59db95d71f","_cell_guid":"2d1f4dd0-9c90-4883-8706-73d109d30d06","collapsed":true},"cell_type":"markdown","source":"This is not the end. More is yet to come.\nComing soon:\n1. Alternative deep learning approach using Keras\n2. Sentimental Analysis"},{"metadata":{"_uuid":"ac05daf5264310f3b1e10aebdaf2ec67f4d2c7ea","_cell_guid":"e40a2f28-6b8d-4638-a8da-167af416116e","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}