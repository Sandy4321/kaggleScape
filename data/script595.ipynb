{"nbformat_minor": 1, "metadata": {"_change_revision": 0, "_is_fork": false, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.1", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}}, "cells": [{"metadata": {"_uuid": "6326a50a8b137a873fc07f212935bdf4c404b726", "_cell_guid": "62f0bd8a-b3e5-0bad-0a88-9c0f8cf23c05"}, "cell_type": "markdown", "source": ["I will execute a full EDA and predictive analysis on this kernel. My plan is to heavily rely on three-dimensional data visualization throughout my EDA to help me carefully define my new features and add predictive power to  my model."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "cd563003153315eefd7650a40bec3b8921865c2d", "_cell_guid": "3059db3e-de53-13d5-a6e5-e6228dacfdf4"}, "cell_type": "code", "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt\n", "from mpl_toolkits.mplot3d import Axes3D\n", "import seaborn as sns\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "execution_count": null}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "98d5344d48e39181d69c6a0a53af3d6674d9c394", "_cell_guid": "a4c9b0d7-4ce5-6e3d-b7e2-80569bf06ad9"}, "cell_type": "code", "source": ["data = pd.read_csv('../input/HR_comma_sep.csv')\n", "data.head()"], "execution_count": null}, {"metadata": {"_uuid": "f6b9c4270f711026c4468680336ff76dc2226162", "_cell_guid": "cb0c5183-5990-90c3-9eda-45ff9f8e5f6e"}, "cell_type": "markdown", "source": ["Exploratory Data Analysis\n", "========================="]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "ea4ed4b2a6bcb76f63b376cc32edd3fb8f8ea3b7", "_cell_guid": "d294cf12-5994-9faa-4331-2fbfbb2820ce"}, "cell_type": "code", "source": ["fig = plt.figure()\n", "ax = fig.add_subplot(111, projection='3d')\n", "x = data['satisfaction_level']\n", "y = data['average_montly_hours']\n", "z = data['last_evaluation']\n", "c = data['number_project']\n", "_ = ax.scatter(xs=x, ys=y, zs=z, c=c)\n", "_ = ax.set_xlabel('Satisfaction Level')\n", "_ = ax.set_ylabel('Average Monthly Hours')\n", "_ = ax.set_zlabel('Last Evaluation')\n", "_ = plt.title('Plot 1: Multivariate Visualization with Number of Projects by Color')\n", "plt.show()"], "execution_count": null}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "7b96068c3a8b35adc6a5e08060147d64f608b0da", "_cell_guid": "88dcbe83-4a48-5af1-7d71-0add287960ff"}, "cell_type": "code", "source": ["pd.options.mode.chained_assignment = None\n", "data['workhorses'] = 0\n", "data['workhorses'][(data['number_project'] >= 6) & (data['average_montly_hours'] > 200) & (data['satisfaction_level'] < 0.130) & (data['last_evaluation'] > 0.8)] = 1\n", "print(\"We can identify {} employees who fit the 'workhorse' description according to this analysis.\".format(len(data[data['workhorses'] == 1])))\n", "\n", "workhorsedf = data[data['workhorses'] == 1]\n", "\n", "fig = plt.figure()\n", "ax = fig.add_subplot(111, projection='3d')\n", "x = workhorsedf['satisfaction_level']\n", "y = workhorsedf['average_montly_hours']\n", "z = workhorsedf['last_evaluation']\n", "_ = ax.scatter(xs=x, ys=y, zs=z, c='blue')\n", "_ = ax.set_xlabel('Satisfaction Level')\n", "_ = ax.set_ylabel('Average Monthly Hours')\n", "_ = ax.set_zlabel('Last Evaluation')\n", "_ = plt.title('Plot 1a: Multivariate Visualization of Company Workhorses')\n", "plt.show()\n", "\n", "data['disengaged'] = 0\n", "data['disengaged'][(data['number_project'] <= 2) & (data['average_montly_hours'] <= 170) & (data['average_montly_hours'] > 110) & (data['satisfaction_level'] < 0.50) & (data['satisfaction_level'] > 0.20) & (data['last_evaluation'] < 0.50) & (data['last_evaluation'] > 0.41)] = 1\n", "print(\"We can identify {} employees who fit the 'disengaged' description according to this analysis.\".format(len(data[data['disengaged'] == 1])))\n", "\n", "disengageddf = data[data['disengaged'] == 1]\n", "\n", "fig = plt.figure()\n", "ax = fig.add_subplot(111, projection='3d')\n", "x = disengageddf['satisfaction_level']\n", "y = disengageddf['average_montly_hours']\n", "z = disengageddf['last_evaluation']\n", "_ = ax.scatter(xs=x, ys=y, zs=z, c='green')\n", "_ = ax.set_xlabel('Satisfaction Level')\n", "_ = ax.set_ylabel('Average Monthly Hours')\n", "_ = ax.set_zlabel('Last Evaluation')\n", "_ = plt.title('Plot 1b: Multivariate Visualization of Disengaged Employees')\n", "plt.show()"], "execution_count": null}, {"metadata": {"_uuid": "68cbe9a262ecd3199a214599e213fa882ec09d1b", "_cell_guid": "cdf41cee-c84c-00a6-f632-c615954b16fd"}, "cell_type": "markdown", "source": ["The dark square of observations in the back top corner of this plot seems to represent a group of employees who a) work very long hours, b) do a huge number of projects, c) are evaluated very well for their additional efforts, and d) hate their jobs. I'll call these the 'workhorses' of this company. I've captured them in a separate feature and plotted them again. I like refining my features in this way because it allows me to make tiny adjustments to my various parameters (in this case the satisfaction level) to ensure that my feature captures exactly the values that I want it to represent.\n", "\n", "There is also an obvious group of employees (the big white blob) that a) do few projects, b) work a fairly average number of hours (around 160/month, what you would expect from a salaried full time employee), c) get very poor evaluations, and d) don't care for their jobs. I'll call these the 'disengaged' of this company and capture them in a separate feature as well.\n", "\n", "It is interesting to note that the two most obvious groupings here based on the number of projects each employee works are both at the low end of the satisfaction metric. This seems to indicate that either too many or too few projects lead to low satisfaction, even though the impact on average working hours and evaluations are radically different. "]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "b1a14661e50d4c8560b62c178d26a61c7631e5e5", "_cell_guid": "4a310da5-e2ae-e4c2-db4c-2ec346ee999d"}, "cell_type": "code", "source": ["fig = plt.figure()\n", "ax = fig.add_subplot(111, projection='3d')\n", "x = data['satisfaction_level']\n", "y = data['average_montly_hours']\n", "z = data['last_evaluation']\n", "c = data['time_spend_company']\n", "_ = ax.scatter(xs=x, ys=y, zs=z, c=c)\n", "_ = ax.set_xlabel('Satisfaction Level')\n", "_ = ax.set_ylabel('Average Monthly Hours')\n", "_ = ax.set_zlabel('Last Evaluation')\n", "_ = plt.title('Plot 2: Multivariate Visualization with Tenure at the Company by Color')\n", "plt.show()"], "execution_count": null}, {"metadata": {"_uuid": "0bc7697e80ea6936117f637b4a22f3bdbf9816dd", "_cell_guid": "1fd05c15-e95b-4c93-130b-49f333992b72"}, "cell_type": "markdown", "source": ["Nothing here that really leaps out at me. There is a significant group of employees with significant experience who work long hours, get great evaluations, and are highly satisfied. Looking at the previous plot we can see that they also take on a fair number of projects, although not enough to become part of the crowd of burnouts. Perhaps we can select these individuals out into a separate feature and call them our 'veterans'.\n", "\n", "It is interesting to note that our group of 'disengaged' employees are also quite junior and that our 'workhorses' are more senior but typically not as much as the 'veterans'. It is entirely possible that a key issue in this company is a failure to give adequate responsibility to junior employees (leading to disengagement) and overworking those who have proven their abilities (leading to burnout). \n", "\n", "Of course at this point this is only a hypothesis."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "fab960c77ac23acd33af66f64ee404e31f014c87", "_cell_guid": "0e8e1bf5-2452-6ad0-a19b-c7bb7801c079"}, "cell_type": "code", "source": ["fig = plt.figure()\n", "ax = fig.add_subplot(111, projection='3d')\n", "x = data['satisfaction_level']\n", "y = data['average_montly_hours']\n", "z = data['last_evaluation']\n", "c = data['left']\n", "_ = ax.scatter(xs=x, ys=y, zs=z, c=c)\n", "_ = ax.set_xlabel('Satisfaction Level')\n", "_ = ax.set_ylabel('Average Monthly Hours')\n", "_ = ax.set_zlabel('Last Evaluation')\n", "_ = plt.title('Plot 3: Multivariate Visualization with Attrition by Color (black if left)')\n", "plt.show()"], "execution_count": null}, {"metadata": {"_uuid": "edf760022b098f5b101ac2f11a804482ea7db084", "_cell_guid": "180d7600-8f2f-0a0a-c743-c43a2397d636"}, "cell_type": "markdown", "source": ["Interesting. This is not what I expected but it is exciting to see. It looks like ALL THREE of our already identified groups are the ones who are  leaving the company in the largest numbers. These three groups contain almost all the attrition experienced by the entire firm. \n", "\n", "An immediate hypothesis that springs to mind to explain this variance goes as follows:\n", "1. 'Workhorses' leave because they are burned out.\n", "2. 'Disengaged' employees leave because they don't like their job.\n", "3. 'Veterans' leave because they either retire or are offered attractive job opportunities elsewhere. Given their high evaluations, satisfaction, and tenure I like the retirement narrative a little better. \n", "\n", "Now that I can clearly see the importance of my 'veterans' group I'll create a feature to capture them and then use another 3D visualization to ensure I've got the right folks."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "0d4e1fdeec0fd28dc762d8559ef0b37cc89f6cf5", "_cell_guid": "b37ac03d-b6b1-f510-f01b-e5ea4a267a32"}, "cell_type": "code", "source": ["data['veterans'] = 0\n", "data['veterans'][(data['number_project'] >= 3) & (data['average_montly_hours'] > 200) & (data['satisfaction_level'] > 0.65) & (data['satisfaction_level'] < .93) & (data['last_evaluation'] > 0.78)] = 1\n", "print(\"We can identify {} employees who fit the 'veteran' description according to this analysis.\".format(len(data[data['veterans'] == 1])))\n", "\n", "veteransdf = data[data['veterans'] == 1]\n", "\n", "fig = plt.figure()\n", "ax = fig.add_subplot(111, projection='3d')\n", "x = veteransdf['satisfaction_level']\n", "y = veteransdf['average_montly_hours']\n", "z = veteransdf['last_evaluation']\n", "c = veteransdf['left']\n", "_ = ax.scatter(xs=x, ys=y, zs=z, c=c)\n", "_ = ax.set_xlabel('Satisfaction Level')\n", "_ = ax.set_ylabel('Average Monthly Hours')\n", "_ = ax.set_zlabel('Last Evaluation')\n", "_ = plt.title('Plot 1a: Multivariate Visualization of Company Workhorses')\n", "plt.show()"], "execution_count": null}, {"metadata": {"_uuid": "84cea038ddcbb0d4c74de78dce0f87d9b1226d6a", "_cell_guid": "5f893d0d-3c73-67ee-ac9c-35c36b770060"}, "cell_type": "markdown", "source": ["This isn't nearly as clean a subdivision as the previous two features I created, but I'm still pleased with it and quite confident that it will contribute significant improvements to the predictive power of my machine learning models."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "8291c229aad3a04a369260f3b62f887b230493cf", "_cell_guid": "bc5bf526-138c-c5d4-709c-09b28245d74a"}, "cell_type": "code", "source": ["fig = plt.figure()\n", "ax = fig.add_subplot(111, projection='3d')\n", "x = data['satisfaction_level']\n", "y = data['average_montly_hours']\n", "z = data['last_evaluation']\n", "c = data['promotion_last_5years']\n", "_ = ax.scatter(xs=x, ys=y, zs=z, c=c)\n", "_ = ax.set_xlabel('Satisfaction Level')\n", "_ = ax.set_ylabel('Average Monthly Hours')\n", "_ = ax.set_zlabel('Last Evaluation')\n", "_ = plt.title('Plot 4: Multivariate Visualization with Promotions within the Last 5 Years by Color')\n", "plt.show()"], "execution_count": null}, {"metadata": {"_uuid": "90988fe07f963feb44efc8104995bb0a28b20001", "_cell_guid": "f25d6cbd-7cda-9940-89e2-e7f738919ef3"}, "cell_type": "markdown", "source": ["No visually discernible pattern here. It is interesting (though probably irrelevant for this analysis) to note that folks do not get promoted very often in this company. This may indicate a tendency to hire from outside the company. We can explore that later with a histogram of employee tenure."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "d8e7004178824be505ec8bdb95e16c662121e943", "_cell_guid": "e4ba1d15-3191-17c4-c65a-3f5f9097a59c"}, "cell_type": "code", "source": ["fig = plt.figure()\n", "ax = fig.add_subplot(111, projection='3d')\n", "x = data['satisfaction_level']\n", "y = data['average_montly_hours']\n", "z = data['last_evaluation']\n", "c = data['Work_accident']\n", "_ = ax.scatter(xs=x, ys=y, zs=z, c=c)\n", "_ = ax.set_xlabel('Satisfaction Level')\n", "_ = ax.set_ylabel('Average Monthly Hours')\n", "_ = ax.set_zlabel('Last Evaluation')\n", "_ = plt.title('Plot 5: Multivariate Visualization with Work Accidents by Color')\n", "plt.show()"], "execution_count": null}, {"metadata": {"_uuid": "e3eb78424b8f695d3c00b7d0243db8ee60c002b2", "_cell_guid": "c40c7f02-8774-45c9-7628-5f5293073cb6"}, "cell_type": "markdown", "source": ["No visually discernible pattern here either."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "fc6cbf944a782155a426f32a1702c8bb65a7cd74", "_cell_guid": "5ad07686-0c3a-71f6-a5a1-47ae10827dd2"}, "cell_type": "code", "source": ["_ = sns.distplot(data['time_spend_company'])\n", "_ = plt.title('Plot 6: Histogram of Tenure at Company')\n", "_ = plt.xlabel('Number of Years at Company')\n", "_ = plt.ylabel('Employees')\n", "plt.show()"], "execution_count": null}, {"metadata": {"_uuid": "1b7bbc189cf81fd89a4830014fe455b2741c0c70", "_cell_guid": "1f29d133-675a-da64-e3de-b8a321514810"}, "cell_type": "markdown", "source": ["This seems to confirm my hunch that this company doesn't really promote from within. Even the old-timers at this company only have about 10 years of service, and the mode is only 3 years. It's interesting to note that the old-timers (largely captured in our 'veterans') are leaving with only ten years at the company. This may imply that it is not retirement that causes them to leave but something else. Perhaps better financial compensation elsewhere? Without a great deal of additional information (at a minimum salaries for company employees and average salaries for like employees in the same industry) it is not possible to really explore this idea.\n", "\n", "That said, the last thing I want to look at is the salary data (such as it is)."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "4314aae400c859c35ad0bfa3c7c360a627f74606", "_cell_guid": "d02a6240-4ad5-1872-e368-54163188e72c"}, "cell_type": "code", "source": ["data['salaries'] = 1\n", "data['salaries'][data['salary'] == 'medium'] = 2\n", "data['salaries'][data['salary'] == 'high'] = 3\n", "\n", "fig = plt.figure()\n", "ax = fig.add_subplot(111, projection='3d')\n", "x = data['satisfaction_level']\n", "y = data['average_montly_hours']\n", "z = data['last_evaluation']\n", "c = data['salaries']\n", "_ = ax.scatter(xs=x, ys=y, zs=z, c=c)\n", "_ = ax.set_xlabel('Satisfaction Level')\n", "_ = ax.set_ylabel('Average Monthly Hours')\n", "_ = ax.set_zlabel('Last Evaluation')\n", "_ = plt.title('Plot 7: Multivariate Visualization with Salaries by Color')\n", "plt.show()"], "execution_count": null}, {"metadata": {"_uuid": "77702176252e0e8e81fc576eb8703b614b45f36d", "_cell_guid": "5f77c972-e87e-860c-045c-49d31e05c2cc"}, "cell_type": "markdown", "source": ["It appears suspiciously as though salaries are not tied to either performance or tenure at the firm. I say this chiefly because the previous visualizations strongly brought those features out, but this one seems to show salaries scattered at random through the company. At a minimum one might expect a pile of low salaries among our 'disengaged' employees and high ones among our 'veterans'. Lets take a look and see."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "0d74e1bcc6550b0a89541ddda0c64f38329debcf", "_cell_guid": "9b3c5688-7c88-6ed6-0c3f-9ba04161f1d8"}, "cell_type": "code", "source": ["_ = sns.barplot(x='time_spend_company', y='salaries', data=data, hue='workhorses')\n", "_ = plt.title('Workhorse Salaries and Tenure')\n", "_ = plt.xlabel('Tenure')\n", "_ = plt.ylabel('Average Salary')\n", "plt.show()\n", "\n", "_ = sns.barplot(x='time_spend_company', y='salaries', data=data, hue='disengaged')\n", "_ = plt.title('Disengaged Employee Salaries and Tenure')\n", "_ = plt.xlabel('Tenure')\n", "_ = plt.ylabel('Average Salary') \n", "plt.show()\n", "\n", "_ = sns.barplot(x='time_spend_company', y='salaries', data=data, hue='veterans')\n", "_ = plt.title('Veteran Salaries and Tenure')\n", "_ = plt.xlabel('Tenure')\n", "_ = plt.ylabel('Average Salary')\n", "plt.show()"], "execution_count": null}, {"metadata": {"_uuid": "0d8406b2c62d830dbefd62d8a9762de10c2eb1c0", "_cell_guid": "219d97a5-1505-2b74-b616-3b14b7b29bb2"}, "cell_type": "markdown", "source": ["Looks like our workhorses are compensated a little below average for their above average levels of effort while our disengaged employees' compensation varies wildly. Veteran's seem to track fairly well with the company average.\n", "\n", "It seems that in general performance is not reflected in employee compensation in this firm. To explore this idea I will look at performance vs. compensation in the sales department. If any department is compensated for performance it is probably the sales team. If this is NOT the case then I will assume that no one is compensated relative to their contributions to this firm."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "7d2e990bc36f39928d6f07a08f035f9126a4fd50", "_cell_guid": "8a18a6e1-3957-6dce-9908-a4b46dd3ce60"}, "cell_type": "code", "source": ["_ = sns.violinplot(x='time_spend_company', y='salaries', data=data)\n", "plt.show()\n", "\n", "_ = sns.barplot(x='time_spend_company', y='last_evaluation', hue='salary', hue_order=['low', 'medium', 'high'], data=data)\n", "_ = plt.xlabel('Tenure (in years)')\n", "_ = plt.ylabel('Average of Last Evaluation Score')\n", "_ = plt.title('Average Compensation by Tenure and Performance')\n", "_ = plt.legend(loc=2)\n", "plt.show()\n", "\n", "sales = data[data['sales'] == 'sales']\n", "_ = sns.barplot(x='time_spend_company', y='last_evaluation', hue='salary', hue_order=['low', 'medium', 'high'], data=sales)\n", "_ = plt.xlabel('Tenure (in years)')\n", "_ = plt.ylabel('Average of Last Evaluation Score')\n", "_ = plt.title('Average Compensation Among Sales Employees by Tenure and Performance')\n", "_ = plt.legend(loc=2)\n", "plt.show()\n", "\n", "corr = np.corrcoef(x=data['last_evaluation'], y=data['salaries'])\n", "print('The correlation between evaluated performance and employee salaries is {}.'.format(corr))"], "execution_count": null}, {"metadata": {"_uuid": "7b773c7f58950bb5808debbbfb61e481d8195fd4", "_cell_guid": "633995a7-ee22-6aa8-9265-1ec3b6164bc2"}, "cell_type": "markdown", "source": ["So for both the average employee of the company and for the sales department in particular it appears that compensation is not tied to performance. In fact there is a tiny *negative* correlation between the two (although it is so small it is basically irrelevant). We can effectively say that there is no correlation at all between the two. This is quite troubling. If I was actually consulting with a real firm for this analysis I would attempt to dive into considerable detail at this point and try to identify the likely outcome this is having on company performance."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "1b7b81574085c2c2188be7ff1f562e7afebab98c", "_cell_guid": "c687631b-4809-6d11-3858-9c3b6e059186"}, "cell_type": "code", "source": ["_ = sns.barplot(x='time_spend_company', y='salaries', data=data, hue='left')\n", "_ = plt.title('Salaries and Tenure by Retention/Loss')\n", "_ = plt.xlabel('Tenure')\n", "_ = plt.ylabel('Average Salary')\n", "plt.show()"], "execution_count": null}, {"metadata": {"_uuid": "2afecf3774760e13bccad923f4a402e4227bdf0b", "_cell_guid": "85276f37-0e1d-4123-20d2-9a66cccbfa1d"}, "cell_type": "markdown", "source": ["Looks like employees with more than seven years don't leave the firm. I'll create another feature to capture this observation and label these folks my 'seniors'."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "8f521f5e25a6cda2f149857d84a33c9b755da83f", "_cell_guid": "8409f0e3-8a02-4d0f-f5d3-a0e58084f1ac"}, "cell_type": "code", "source": ["data['seniors'] = 0\n", "data['seniors'][data['time_spend_company'] > 6] = 1\n", "print(\"There are {} 'seniors' at this firm.\".format(len(data[data['seniors'] == 1])))"], "execution_count": null}, {"metadata": {"_uuid": "379320fbc017d70f5ac4a1256d84a58d513a3569", "_cell_guid": "87480ac7-16f4-1eba-3604-b191111ab491"}, "cell_type": "markdown", "source": ["Last thing I will do to finish up this EDA is take a look at my various departments. First I'll change the column name (because it is irritating), then I'll see if any obvious patterns emerge in a 3D scatter plot."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "0d121540bcbc3c1876e4cee6778d145eabbe47db", "_cell_guid": "67bbde05-4545-1e1f-bcd5-f0497456d11d"}, "cell_type": "code", "source": ["data = data.rename(columns={'sales':'department'})"], "execution_count": null}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "273208cef4cbaaddb9f3cf22e551bfff127c5598", "_cell_guid": "a835da17-ed04-bcb6-21f1-dfd714413bd8"}, "cell_type": "code", "source": ["data['dep'] = 1\n", "data['dep'][data['department'] == 'accounting'] = 2\n", "data['dep'][data['department'] == 'hr'] = 3\n", "data['dep'][data['department'] == 'technical'] = 4\n", "data['dep'][data['department'] == 'support'] = 5\n", "data['dep'][data['department'] == 'management'] = 6\n", "data['dep'][data['department'] == 'IT'] = 7\n", "data['dep'][data['department'] == 'product_mng'] = 8\n", "data['dep'][data['department'] == 'RandD'] = 9\n", "\n", "fig = plt.figure()\n", "ax = fig.add_subplot(111, projection='3d')\n", "x = data['satisfaction_level']\n", "y = data['average_montly_hours']\n", "z = data['last_evaluation']\n", "c = data['dep']\n", "_ = ax.scatter(xs=x, ys=y, zs=z, c=c)\n", "_ = ax.set_xlabel('Satisfaction Level')\n", "_ = ax.set_ylabel('Average Monthly Hours')\n", "_ = ax.set_zlabel('Last Evaluation')\n", "_ = plt.title('Plot 7: Multivariate Visualization with Salaries by Color')\n", "plt.show()"], "execution_count": null}, {"metadata": {"_uuid": "5f25c062182c4833c1f0f1b8ad9bee13d77935cb", "_cell_guid": "be466b26-5188-047b-ce07-043f6285d668"}, "cell_type": "markdown", "source": ["It looks suspiciously like the department to which an employee is assigned basically has nothing to do with the other issues we've identified so far. This is unexpected and a bit perplexing. I had assumed that these problems might be localized within a single function, but this appears to be untrue."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "3bc83b62652ea189daaab973cfa8d6e3ab8f698c", "_cell_guid": "9fdc5759-d69c-345b-75c4-c290134423a7"}, "cell_type": "code", "source": ["_ = sns.countplot(x='department', data=data)\n", "_ = plt.xlabel('Department')\n", "_ = plt.ylabel('Number of Employees')\n", "_ = plt.title('Employees Assigned per Department')\n", "plt.show()"], "execution_count": null}, {"metadata": {"_uuid": "b3ef0feedb169277b9b1cbe77a90d468ba6f5c0e", "_cell_guid": "389bda47-8810-6d04-fd1b-fdbbf5736c9c"}, "cell_type": "markdown", "source": ["Looks like most of our employees work in sales, the technical department, and support. Let's see how these departments have been affected by turnover using a seaborn factorplot."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "5fff943b489f1c81cff137df638c8063c69dfe80", "_cell_guid": "d39671c3-b39f-7192-da07-83c347cd6ca3"}, "cell_type": "code", "source": ["fp = sns.factorplot(x='number_project', y='satisfaction_level', col='left', row='department', kind='bar', data=data)\n", "_ = fp.set_axis_labels('Number of Projects', 'Satisfaction Level')\n", "_ = fp.set_xticklabels(['2','3','4','5','6','7'])\n", "plt.show()"], "execution_count": null}, {"metadata": {"_uuid": "20b28a4ea1a90ef6afa789062cd8436ec55e0bfd", "_cell_guid": "0a3ac73f-a674-f451-a106-ea2c2777d38c"}, "cell_type": "markdown", "source": ["OK, so what do we see here?\n", "\n", "Sales - Employees who left were both highly satisfied and had 3-5 projects. This places them in our \u2018veterans\u2019 category. There were also some with few projects and lower satisfaction (disengaged) and a few with a very high number of projects and very low satisfaction (workhorses). I don\u2019t see anything here I haven\u2019t already captured.\n", "\n", "Accounting, HR, Technical, Support, Management \u2013 Same general distribution as what we see in the Sales department.\n", "\n", "IT and Marketing \u2013 These are similar in some respects to the other departments, but there is wild variation in the satisfaction levels of people with 3 projects who left.\n", "\n", "Product Management \u2013 Similar to most of the rest of the departments. Satisfaction is extraordinarily high among those who left with 4-5 projects and extraordinarily low among those with more than 5. Still nothing here that doesn\u2019t fit into our previous observations, however.\n", "\n", "RandD \u2013 Strikingly similar to Product Management, although with more variation in satisfaction levels among those who left. \n", "\n", "Well, that does it for EDA for now. There is quite a bit more I can do in this vein, and if I was working on my home machine I certainly would. But in the interest of not crashing my kernel and still getting this analysis finished I will move on."]}, {"metadata": {"_uuid": "8b976b2eabe673eb4ff1c4ca4357908f50d13d51", "_cell_guid": "c1178354-96cb-66a3-8d2c-8ffaf04cafcc"}, "cell_type": "markdown", "source": ["Feature Engineering\n", "===================\n", "\n", "Now that we've had a chance to thoroughly  'roll around' in the data (as one of my favorite blog posts put it so eloquently) its time to do some additional feature engineering and then get on with the business of building our models.\n", "\n", "I've already created features to capture workhorses, disengaged employees, veterans, and seniors. I've also created a couple (salaries and dep) to provide a numeric equivalent to some categorical features. I could have used sklearn's preprocessing LabelEncoder to do this but I chose to do it manually. \n", "\n", "I have seen on other kernels on this dataset that some folks have used KMeans clustering by 'left', but this is cheating since I can't know in advance which employees will leave. I could never use that method to predict whether employees from a different dataset who *haven't* left will leave. So I won't use that technique."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "24b8148a8657db679453a13fa67442b166f87b53", "_cell_guid": "d4612446-af7e-c0b9-a834-701c22b0d6d6"}, "cell_type": "code", "source": ["data['tenure_vs_hours'] = data['time_spend_company'] / data['average_montly_hours']\n", "_ = sns.distplot(data['tenure_vs_hours'])\n", "plt.show()\n", "\n", "x = np.corrcoef(x=data['tenure_vs_hours'], y=data['left'])\n", "y = np.corrcoef(x=data['tenure_vs_hours'], y=data['satisfaction_level'])\n", "print(x, y)"], "execution_count": null}, {"metadata": {"_uuid": "9c051043ddcd42aa121f9aba348284da93e1150f", "_cell_guid": "67825110-9d95-6f2d-b7bd-ed704bc912de"}, "cell_type": "markdown", "source": ["This first feature captures the number of hours worked relative to an employees tenure at the company. It has a very small positive correlation to an employee leaving and a somewhat higher negative correlation to employee satisfaction. We'll see when it comes time to start looking at feature importances whether this is really important. Now I'll go ahead and compute a few more features capturing simple relationships between some of my existing features."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "fecfe78cec2768daf5febdb8438246a38b8e6a81", "_cell_guid": "e006cd8c-e416-dae2-3291-6468d4e3a382"}, "cell_type": "code", "source": ["data['tenure_vs_projects'] = data['time_spend_company'] / data['number_project']\n", "_ = sns.distplot(data['tenure_vs_projects'])\n", "plt.show()\n", "x = np.corrcoef(x=data['tenure_vs_projects'], y=data['left'])\n", "y = np.corrcoef(x=data['tenure_vs_projects'], y=data['satisfaction_level'])\n", "print(x, y)"], "execution_count": null}, {"metadata": {"_uuid": "554ae1587014f33cc512f29d2eaedd2e4d9db886", "_cell_guid": "5c9132d5-7750-b3b7-2275-07bb558914a3"}, "cell_type": "markdown", "source": ["Looks like we're capturing some interesting variance here around both propensity to leave and satisfaction level.\n", "\n", "Just as an aside, I'm choosing to informally evaluate the usefulness of these new features based on their correlation to whether and employee left and their satisfaction level. I think it is working out. We'll see when it comes time to start making predictions.\n", "\n", "Another aside - I really like using histograms to look at these features after I've created them to see if they make intuitive sense. I've discarded a couple of options as I've gone through this process (though I won't include those in this kernel for the sake of saving space)."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "5f9553f52fa49ad25d14870726f514f257fd08ac", "_cell_guid": "b874d1a1-effc-a806-e45e-6e94755fa4ca"}, "cell_type": "code", "source": ["def make_feat(df, new_feat_name, feat1, feat2, feat3, feat4):\n", "    df[new_feat_name] = df[feat1] / df[feat2]\n", "    _ = sns.distplot(df[new_feat_name])\n", "    plt.show()\n", "    x = np.corrcoef(x=df[new_feat_name], y=df[feat3])\n", "    y = np.corrcoef(x=df[new_feat_name], y=df[feat4])\n", "    print(x,y)\n", "    return df[new_feat_name]\n", "\n", "data['tenure_vs_evaluation'] = make_feat(data, 'tenure_vs_evaluation', 'time_spend_company', 'last_evaluation', 'left', 'satisfaction_level')\n", "data['tenure_vs_salaries'] = make_feat(data, 'tenure_vs_salaries', 'salaries', 'time_spend_company', 'left', 'satisfaction_level')\n", "data['projects_vs_eval'] = make_feat(data, 'projects_vs_eval', 'number_project', 'last_evaluation', 'left', 'satisfaction_level')\n", "data['projects_vs_salary'] = make_feat(data, 'projects_vs_salary', 'number_project', 'salaries', 'left', 'satisfaction_level')\n", "data['salaries_vs_evaluation'] = make_feat(data, 'salaries_vs_evaluation', 'salaries', 'last_evaluation', 'left', 'satisfaction_level')\n", "data['projects_v_eval_vs_salaries'] = make_feat(data, 'projects_v_eval_vs_salaries', 'projects_vs_eval', 'salaries', 'left', 'satisfaction_level')\n", "data['tvs_vs_tve'] = make_feat(data, 'tvs_vs_tve', 'tenure_vs_salaries', 'tenure_vs_evaluation', 'left', 'satisfaction_level')\n", "data['tvp_vs_pvs'] = make_feat(data, 'tvp_vs_pvs', 'tenure_vs_projects', 'projects_vs_salary', 'left', 'satisfaction_level')\n", "data['tvs_vs_tvp'] = make_feat(data, 'tvs_vs_tvp', 'tenure_vs_salaries', 'tenure_vs_projects', 'left', 'satisfaction_level')        "], "execution_count": null}, {"metadata": {"_uuid": "c05ab874ce20ffdd41c0e05c7296c9a37ac15585", "_cell_guid": "bd1ae725-7abf-7a62-42dc-b32aa2b7e5f8"}, "cell_type": "markdown", "source": ["I'm capturing some pretty strong negative correlations here, which I find striking. I've also created a few features that are probably worthless, but I'll figure that out later. For now I'll create as many as I can think of and then I'll let my machine tell me what is worth keeping and what needs to be thrown out.\n", "\n", "The last step before I move on to machine learning land is to discard all non-numeric features. I'll do that and then get on to the next step.\n", "\n", "Before I do that however, a brief word about handling continuous variables. Every feature I just made is continuous. I have encountered a few folks who assert that continuous variables cause inferior performance with machine learning algorithms. I'm not sure what this belief is based on. All the literature I've come across on the topic has strongly argued that binning continuous variables (basically as a means to make floats into integers, or nominal values) causes a tremendous amount of information loss. Furthermore, many machine learning algorithms essentially bin the data for you anyway, and do a better job at it than I can do just by guessing or experimentation. Basically unless you have a ***VERY*** good reason to do so (and I can't find even one that someone doesn't contest) you should not bin continuous variables. So for all these reasons I am leaving my new features just as they are.\n", "\n", "I would be very interested in hearing other perspectives on this issue. Please leave me a comment if you have any thoughts on the topic. I'll be sure to read it and respond."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "a9b96d8126525a026d4d3cdb23e167e928c1610a", "_cell_guid": "9c67a30a-7cd1-b5df-cf42-6b37b56df7d3"}, "cell_type": "code", "source": ["del data['department']\n", "del data['salary']"], "execution_count": null}, {"metadata": {"_uuid": "4f999f5c6e9df1d99683f073b937fbeaf3c1af04", "_cell_guid": "8219b597-9ace-7ba9-b04c-be3189c74828"}, "cell_type": "markdown", "source": ["Well, that's a wrap for creating new features. This dataset started out with 10 features and now has 25. On to the next step!\n", "\n", "Principle Component Analysis (PCA) and Scaling\n", "============================\n", "\n", "First I'll get everything scaled so my models can learn from it more easily, then I'll do some PCA. I'll split my train and test sets out prior to the unsupervised learning so that I can finish the business end of my analysis once my machine learning work is complete."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "674237928075e3dda7d26762baafb395e1d690de", "_cell_guid": "d0750df2-27be-5cdd-d4ee-d46b27e45f4d"}, "cell_type": "code", "source": ["from sklearn import preprocessing\n", "\n", "x = data\n", "scaler = preprocessing.scale(x)\n", "cols = x.columns\n", "data1 = pd.DataFrame(scaler, columns=cols, index=data.index)\n", "data1['left'] = data['left']"], "execution_count": null}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "f65b9cdcebcbf10e73c0c68bef6e3414c8db6dc2", "_cell_guid": "19206b49-db53-9f48-1fcb-d31a06f7d519"}, "cell_type": "code", "source": ["from sklearn.decomposition import PCA\n", "X = data\n", "Y = X['left']\n", "del X['left']"], "execution_count": null}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "a9a269ae690ad21e6b0f96ea83525c4af4b1eced", "_cell_guid": "dac2fe84-1262-cb9e-7b6e-ef56928e136d"}, "cell_type": "code", "source": ["pca = PCA(n_components=24)\n", "X = pca.fit_transform(X)\n", "var = pca.explained_variance_ratio_\n", "var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n", "_ = plt.plot(var1)\n", "_ = plt.xlabel('Number of Components')\n", "_ = plt.ylabel('Percent of Explained Variance')\n", "_ = plt.title('Primary Component Breakdown')\n", "plt.show()\n", "print(var1)"], "execution_count": null}, {"metadata": {"_uuid": "e8a0a8fa70d3ff3a6e0f9a5d2ab45282cb00409d", "_cell_guid": "fe44e273-a613-d17a-2a1d-a211a5ab3432"}, "cell_type": "markdown", "source": ["Well, how about that. All that feature engineering work and practically all of my variance is explained in the first 5 components. I suppose this isn't surprising. A lot of my new features were probably highly correlated with one another since they consisted of multiple recombinations of just a few key items. Doing the PCA helps me weed out the signal from the noise (thank you Nate Silver) and trim my data down to just what I need to make an accurate predictive model.\n", "\n", "I'll take the first eight components on this list and drive on. There is apparently no additional information to be gleaned from the remaining sixteen, as you can see from both the plot and the explained variance ratios above."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "66082114db095a7c2f388d3d9f5ed65b9d6776a4", "_cell_guid": "ee6f7ed0-06da-fd5e-19bb-481bf0c2674c"}, "cell_type": "code", "source": ["pca = PCA(n_components=8)\n", "X = pca.fit_transform(X)\n", "print(X.shape)"], "execution_count": null}, {"metadata": {"_uuid": "c3e6ed4ffd0b140053e96844d3d22c259239331d", "_cell_guid": "a15af5b5-f81b-ee85-e97c-b671f2516c59"}, "cell_type": "markdown", "source": ["Modeling and Prediction\n", "=======================\n", "\n", "It's finally time to do what I set out to do - make some predictions! I'll split out my train and test sets first, then run a bunch of models to get a feel for what works best.\n", "\n", "First I'll create a function to give me my total number of true/false positives and negatives. This is important because simply knowing the percentage I get correct is not enough. To understand the likely impact on the business I need to have an estimate of \n", "\n", "1. What we can do to retain folks and how much we will spend (on average) on those efforts for each employee identified as likely to leave.\n", "\n", "2. What it costs to replace an employee who leaves.\n", "\n", "3. How we can minimize the total cost of our retention efforts given that some money will likely be wasted on folks who don't need it and some folks not targeted who should be. Also we'll need to track the success of our efforts among those we target. If we know that among those we identify as a 'flight risk' some percentage are actually false positives we give ourselves a baseline we must rise above to start considering that we have achieved something in our retention efforts. At this point we can pull out some standard frequentist statistical tools and start calculating the effects of our retention program. But we'll get to that later.\n", "\n", "To do this analysis I'll make a few simplifying assumptions. This might not be necessary in a firm with a rich enough historical dataset or with a good feel for the effectiveness of its retention programs, but I have no such information available to me here. So I'll use the following numbers:\n", "\n", "1. Replacing an employee costs 16% of annual salary for low paying jobs, 20% for mid-paying jobs, and 213% for high paying jobs. These estimates come from a 2012 Center for American Progress report. This sounds rather spendy, but is actually a rather conservative estimate compared to some sources.\n", "\n", "2. Retention efforts cost 50% of the expected replacement cost for every employee identified as requiring such efforts by my predictive model. Importantly, this means that I will doubtless spend some money on people who don't actually need that incentive because my models are not perfect.\n", "\n", "4. Retention efforts will achieve 60% effectiveness.\n", "\n", "Time to define my scoring functions and see what my models give me."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "bc221338b13b99fd9a50ca7050de0e1cd5b092f8", "_cell_guid": "9b23e56a-e8d3-43c0-e66f-ae762c0e40ad"}, "cell_type": "code", "source": ["def outcomes_score(test_df, outcome_array):\n", "    \"\"\"Summarize the true/false positives/negatives identified by my model\"\"\"\n", "    \n", "    compare = pd.DataFrame(index=test_df.index)\n", "    compare['test'] = test_df\n", "    compare['prediction'] = outcome_array\n", "\n", "    # Compute total and percentage of true positives\n", "\n", "    compare['True Positive'] = 0\n", "    compare['True Positive'][(compare['test'] == 1) & (compare['prediction'] == 1)] = 1\n", "    truepospercent = np.round(np.sum(compare['True Positive']) / len(compare.index) * 100, decimals=2)\n", "    truepostotal = np.sum(compare['True Positive'])\n", "\n", "    # Compute total and percentage of true negatives\n", "\n", "    compare['True Negative'] = 0\n", "    compare['True Negative'][(compare['test'] == 0) & (compare['prediction'] == 0)] = 1\n", "    truenegpercent = np.round(np.sum(compare['True Negative']) / len(compare.index) * 100, decimals=2)\n", "    truenegtotal = np.sum(compare['True Negative'])\n", "\n", "    # Compute total and percentage of true negatives\n", "\n", "    compare['False Positive'] = 0\n", "    compare['False Positive'][(compare['test'] == 0) & (compare['prediction'] == 1)] = 1\n", "    falsepospercent = np.round(np.sum(compare['False Positive']) / len(compare.index) * 100, decimals=2)\n", "    falsepostotal = np.sum(compare['False Positive'])\n", "\n", "    # Compute total and percentage of false negatives\n", "\n", "    compare['False Negative'] = 0\n", "    compare['False Negative'][(compare['test'] == 1) & (compare['prediction'] == 0)] = 1\n", "    falsenegpercent = np.round(np.sum(compare['False Negative']) / len(compare.index) * 100, decimals=2)\n", "    falsenegtotal = np.sum(compare['False Negative'])\n", "\n", "    print('There are {}, or {}%, true positives.'.format(truepostotal, truepospercent))\n", "    print('There are {}, or {}%, true negatives.'.format(truenegtotal, truenegpercent))\n", "    print(\"Congratulations! You have correctly identified {}, or {}%, of the observed outcomes.\".format(truepostotal + truenegtotal, truepospercent + truenegpercent))\n", "    print('There are {}, or {}%, false positives.'.format(falsepostotal, falsepospercent))\n", "    print('There are {}, or {}%, false negatives.'.format(falsenegtotal, falsenegpercent))\n", "    print(\"Bummer! You incorrectly identified {}, or {}%, of the observed outcomes.\".format(falsenegtotal + falsepostotal, falsepospercent + falsenegpercent))\n", "    \n", "def bottomline_score(test_df, outcome_array):\n", "    \"\"\"Summarize the true/false positives/negatives identified by my model\"\"\"\n", "    \n", "    discard_train, verify_test, dscore_train, vscore_test = train_test_split(data, Y, test_size=0.33, random_state=42)\n", "    \n", "    compare = pd.DataFrame(verify_test, columns=data.columns)\n", "    compare['test'] = test_df\n", "    compare['prediction'] = outcome_array\n", "    compare['left'] = Y\n", "    \n", "    compare['estimated_salary'] = 0\n", "    compare['estimated_salary'][compare['salaries'] == 1] = 30000\n", "    compare['estimated_salary'][compare['salaries'] == 2] = 60000\n", "    compare['estimated_salary'][compare['salaries'] == 3] = 90000\n", "\n", "    # Compute total and percentage of true positives\n", "\n", "    compare['True Positive'] = 0\n", "    compare['True Positive'][(compare['test'] == 1) & (compare['prediction'] == 1)] = 1\n", "    truepospercent = np.sum(compare['True Positive']) / len(compare.index) * 100\n", "    truepostotal = np.sum(compare['True Positive'])\n", "\n", "    # Compute total and percentage of true negatives\n", "\n", "    compare['True Negative'] = 0\n", "    compare['True Negative'][(compare['test'] == 0) & (compare['prediction'] == 0)] = 1\n", "    truenegpercent = np.sum(compare['True Negative']) / len(compare.index) * 100\n", "    truenegtotal = np.sum(compare['True Negative'])\n", "\n", "    # Compute total and percentage of true negatives\n", "\n", "    compare['False Positive'] = 0\n", "    compare['False Positive'][(compare['test'] == 0) & (compare['prediction'] == 1)] = 1\n", "    falsepospercent = np.sum(compare['False Positive']) / len(compare.index) * 100\n", "    falsepostotal = np.sum(compare['False Positive'])\n", "\n", "    # Compute total and percentage of false negatives\n", "\n", "    compare['False Negative'] = 0\n", "    compare['False Negative'][(compare['test'] == 1) & (compare['prediction'] == 0)] = 1\n", "    falsenegpercent = np.sum(compare['False Negative']) / len(compare.index) * 100\n", "    falsenegtotal = np.sum(compare['False Negative'])\n", "\n", "    compare['projected_cost'] = 0\n", "    compare['projected_cost'][(compare['salaries'] == 1) & (compare['True Positive'] + compare['False Positive'] == 1)] = compare['estimated_salary'] * .16\n", "    compare['projected_cost'][(compare['salaries'] == 2) & (compare['True Positive'] + compare['False Positive'] == 1)] = compare['estimated_salary'] * .2\n", "    compare['projected_cost'][(compare['salaries'] == 3) & (compare['True Positive'] + compare['False Positive'] == 1)] = compare['estimated_salary'] * 2.13\n", "    \n", "    compare['retained'] = 0\n", "\n", "    np.random.seed(50)\n", "    \n", "    nums = []\n", "\n", "    for i in range(len(compare)):\n", "        num = np.random.randint(10)\n", "        nums.append(num)\n", "    \n", "    compare['randint'] = nums\n", "    compare['retained'][(compare['randint'] <= 5) & (compare['True Positive'] == 1)] = 1\n", "    \n", "    compare['actual_cost'] = compare['projected_cost'] * compare['retained']\n", "    \n", "    compare['retain_program_cost'] = 0\n", "    compare['retain_program_cost'][compare['True Positive'] + compare['False Positive'] == 1] = compare['projected_cost'] * .25\n", "\n", "    projected_cost = np.sum(compare.projected_cost)\n", "    model_cost = np.sum(compare.actual_cost) - np.sum(compare.retain_program_cost)\n", "    savings = projected_cost - model_cost - np.sum(compare.retain_program_cost)\n", "    benefit = projected_cost - model_cost\n", "    employees_retained = np.count_nonzero(compare.retained)\n", "    ROI = np.round(benefit / np.sum(compare.retain_program_cost), decimals=2)\n", "    cost_per_retention = np.round(np.sum(compare.retain_program_cost) / employees_retained, decimals=2)    \n", "    \n", "    print(\"Using this model will save the firm ${} at a cost of ${}, for an ROI of {}%.\".format(savings, np.sum(compare.retain_program_cost), ROI))\n", "    print(\"The firm will retain {} employees at an average cost of ${} each.\".format(employees_retained, cost_per_retention))"], "execution_count": null}, {"metadata": {"_uuid": "c2dc1592201256f7bfa44d8035039f5ee2e72044", "_cell_guid": "d299993c-6070-59b8-da60-f5a90aef1bad"}, "cell_type": "markdown", "source": ["Random Forest\n", "-------------"]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "43f8b2100a9fc5fd098b29172b82fc04df00b7d6", "_cell_guid": "43459b05-01ef-845c-2186-da406e32ed4e"}, "cell_type": "code", "source": ["from sklearn import preprocessing\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.model_selection import train_test_split\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n", "\n", "my_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n", "my_forest.fit(X_train, y_train)\n", "forest = my_forest.predict(X_test)\n", "print(outcomes_score(y_test, forest))\n", "print(bottomline_score(y_test, forest))"], "execution_count": null}, {"metadata": {"_uuid": "b5e7d3838854033664e306dac89fd2e59c38b057", "_cell_guid": "ae4f199d-ba89-1824-3d2b-b708eb8d64e8"}, "cell_type": "markdown", "source": ["Gradient Boosting Classifier\n", "-----------------"]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "543cd7367f1751d77149debf08f6994c7fd1b44a", "_cell_guid": "c34bd708-61dd-6956-3b7c-28753fcfda67"}, "cell_type": "code", "source": ["from sklearn.ensemble import GradientBoostingClassifier\n", "\n", "gradboost = GradientBoostingClassifier(n_estimators=100, random_state=42).fit(X_train, y_train)\n", "prediction = gradboost.predict(X_test)\n", "print(outcomes_score(y_test, prediction))\n", "print(bottomline_score(y_test, prediction))"], "execution_count": null}, {"metadata": {"_uuid": "74848bb14146fa09a4946be8668496e1b6a90304", "_cell_guid": "c1d6fec9-38fe-4079-c264-58b8bab3c752"}, "cell_type": "markdown", "source": ["Extremely Randomized Trees\n", "-----------"]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "3e6df3651a53a6dfa9eb99907192bf80da94195f", "_cell_guid": "0d2f9f84-eba3-18fa-4bf2-f51d8f7926de"}, "cell_type": "code", "source": ["from sklearn.ensemble import ExtraTreesClassifier\n", "\n", "extratrees = ExtraTreesClassifier(n_estimators=100, random_state=42).fit(X_train, y_train)\n", "prediction = extratrees.predict(X_test)\n", "print(outcomes_score(y_test, prediction))\n", "print(bottomline_score(y_test, prediction))"], "execution_count": null}, {"metadata": {"_uuid": "145a7c3e22dba130426752d5db32762beea70e59", "_cell_guid": "85fddfcc-e3e6-ee11-eb5b-3b6991501d5e"}, "cell_type": "markdown", "source": ["Support Vector Classification\n", "----------------"]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "52b77ec25445d6384b6638e980b918b9f43dbf9e", "_cell_guid": "3cf0887e-a466-b7fb-d350-1541d9df481a"}, "cell_type": "code", "source": ["from sklearn.svm import SVC\n", "\n", "gm = SVC(random_state=42).fit(X_train, y_train)\n", "prediction = gm.predict(X_test)\n", "print(outcomes_score(y_test, prediction))\n", "print(bottomline_score(y_test, prediction))"], "execution_count": null}, {"metadata": {"_uuid": "2f3539c468d2f38b0749dc472ca14f73a6cc82cf", "_cell_guid": "56ba1565-6fa9-8af8-42ff-07d0df49b346"}, "cell_type": "markdown", "source": ["So now comes the interesting part - deciding which model to actually use. Some of these models promise better ROI on the retention budget, while others offer to better aid the firm in retaining a higher number of employees. Our model with the best ROI (the Gradient Boosting Classifier at 2.84%) performs the best from a bean counter point of view, but the Random Forest model retains 45 more employees at only a fraction less. \n", "\n", "There are other questions we can easily answer now that we won't get into. For instance, it would almost certainly be useful to segregate our scores by salary, since our model assumes that highly paid executives are vastly more expensive to replace than low-paid workers. Getting those predictions consistently right is far more important than getting it right for the low man on the totem pole who is easily replaced. In fact it would be well worthwhile to build a model just for those individuals as an addition to the more general  ones we have already created.\n", "\n", "Overall, however, it is germane to make the point here that there are few investments that a) produce such a high ROI or b) promise to do so in such a short period of time. It is quite probable that an intelligently applied investment in retention in this firm would pay itself back to the company with dividends within less than a year. The key question now, of course, is what to spend the money on and how effective we think the chosen interventions will be. That is a subject we can't meaningfully address using this dataset.\n", "\n", "I hope this analysis has been interesting. For any fellow HR enthusiasts reading this please feel free to leave a comment with any insight into what other useful predictions you'd like to see in a model like this.\n", "\n", "And, as always...\n", "\n", "UPVOTE!!!\n", "=========\n", "\n", "It helps keep me motivated:-)"]}], "nbformat": 4}