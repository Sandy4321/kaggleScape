{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b9015933-c316-baf7-ae91-8bfafcb77380"
      },
      "source": [
        "### Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fe310775-1caa-06b8-10a4-e491708bd883"
      },
      "source": [
        "Inspired by the course of Apostolos N. Papadopoulos \u2013 Christos Giatsidis \n",
        "and Erwan Lepennec (Polytechnique)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "22946207-b60b-097d-9c02-fa704053fa26"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.linalg as linalg\n",
        "import sklearn.neighbors as nb\n",
        "import sklearn.utils.graph as ug\n",
        "from time import time\n",
        "from numpy import *\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.ticker import NullFormatter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn import manifold, datasets\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.spatial import distance as spd\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "afb1530d-9b49-7d1b-0b47-1c48516740ed"
      },
      "source": [
        "As soon as the dimension is larger than 3, it becomes hard to visualize the raw data. Dimension reduction technique can be used to alleviate this issue by *projecting* the data in a low dimensional space, typically a 2D space. Note that those dimension reduction ideas can also be used a preprocessing step for any learning task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "256e9519-e255-afb2-c9aa-3cf947a2ed41"
      },
      "source": [
        "### recall on SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d09eb5f8-f9b9-4200-25aa-857966e31c9b"
      },
      "source": [
        "SVD decomposition of $\\textbf{X}_{(n)}$ is $\\textbf{X}_{(n)} = U D(\\lambda) V^t$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cbf1ce87-d8a1-a6c5-91ba-97b367da8da1"
      },
      "source": [
        "### Own PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "79eca3ac-52d3-7f5f-d5ed-e6eac7568848"
      },
      "source": [
        "We will start by the most classical dimension reduction algorithm, the Principal Component Analysis one. The idea is to find a subspace spanned by $d'$ orthonormal columns $V^{(l)}$ such that the reconstruction error between the data and its projection on this subspace\n",
        "\n",
        "\\begin{equation}\n",
        "\\sum_{i=1}^n \\| \\textbf{X}_i - m + V^t (\\textbf{X}_i -m) V\\|^2\n",
        "\\end{equation}\n",
        "\n",
        " is as small as possible. An explicit solution to this problem is given by the space spanned by the $d'$ eigenvectors of the $d \\times d$ empirical covariance matrix of the observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "699cbc0a-d967-7ca3-0eb4-dfa8d1ec025d"
      },
      "source": [
        "#### Algorithm (main Idea)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "25518829-d4f9-76c7-0610-153ce067f094"
      },
      "source": [
        "- Standardize the data.\n",
        "- Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.\n",
        "- Sort eigenvalues in descending order and choose the k eigenvectors that correspond to the k largest eigenvalues where k is the number of dimensions of the new feature subspace (k\u2264d)/.\n",
        "- Construct the projection matrix W from the selected k eigenvectors.\n",
        "- Transform the original dataset X via W to obtain a k-dimensional feature subspace Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "86cbffbb-254f-370d-2203-0ec169325a76"
      },
      "outputs": [],
      "source": [
        "#data : the data matrix\n",
        "#k the number of component to return\n",
        "#return the new data and the variance that was maintained \n",
        "def pca1(data,k):\n",
        "\t# Performs principal components analysis (PCA) on the n-by-p data matrix A (data)\n",
        "\t# Rows of A correspond to observations (wines), columns to variables.\n",
        "\t## TODO: Implement PCA\n",
        "\n",
        "\t# compute the mean\n",
        "\n",
        "    m = np.mean(data,0)\n",
        "\t# subtract the mean (along columns)\n",
        "    C = data-m\n",
        "\t# compute covariance matrix\n",
        "    cov_mat = np.dot(transpose(C),C)\n",
        "\t# compute eigenvalues and eigenvectors of covariance matrix\n",
        "    eigval,eigvect = linalg.eig(cov_mat)\n",
        "\t# Sort eigenvalues (their indexes)\n",
        "    idx = eigval.argsort()[::-1]\n",
        "    eigval[idx]\n",
        "\t# Sort eigenvectors according to eigenvalues\n",
        "    vect = eigvect[:,idx]\n",
        "    eigk = vect[:,0:k]\n",
        "\t# Project the data to the new space (k-D) and measure how much variance we kept\n",
        "    data2 = np.dot(C,eigk)\n",
        "    perc = sum(eigval[0:k])/sum(eigval)\n",
        "    return  (data2, perc)#put here a tuple to return both "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "14b0fc23-5190-03e9-fa2b-496e7ff21b6e"
      },
      "source": [
        "### Own MDS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b3ec79bc-2d52-a586-ad06-37ff2c78162b"
      },
      "source": [
        "Maintain the distances / similarities / dissimilarities among\n",
        "\n",
        "all pairs of elements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bc9af35c-f1b5-ffcf-e7a4-e9a74151e86f"
      },
      "source": [
        "* Let us consider the matrix A of distances $D = A^2$\n",
        "* $J = I_{N} - (11^T)/N$\n",
        "* where $I_{N}$the NxN identity matrix (one\u2019s in diagonal and zeros elsewhere) and ($11^T$) is the NxN matrix with one\u2019s at each cell\n",
        "* $B = -1/2JDJ$\n",
        "* $UWV^T$ = svd($B$)\n",
        "* $A\u2032 = U_{k}W1/2_k$ (k largest eigen values and corresponding vectors from U)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7ca64720-b614-4c08-b27e-aeabe8a708a6"
      },
      "source": [
        "The classic MDS algorithm works the similarity of\n",
        "the data\n",
        "\n",
        "* PCA works on the similarity of the features\n",
        "* PCA and classic MDS have the same results\n",
        "\n",
        "if Euclidean distance is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9f5c46a7-ac53-a335-bf56-ec014efe44bf"
      },
      "outputs": [],
      "source": [
        "#D :distance matrix\n",
        "#k: number of vectors to use\n",
        "def mds(D,k):\n",
        "    nelem = D.shape[0]\n",
        "    J = eye(nelem) - (1.0/nelem) * ones(nelem)\n",
        "    # Compute matrix B\n",
        "    B = -(1.0/2) * dot(J,dot(pow(D,2),J))\n",
        "    # SVD decomposition of B  \n",
        "    U,L,V = linalg.svd(B)\n",
        "    return dot(U[:,:k],sqrt(diag(L)[:k,:k]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "21190241-484a-261e-4467-295de351d07b"
      },
      "source": [
        "### Own Isomap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "53a6d7c1-8289-07e2-739c-69cce46a3996"
      },
      "source": [
        "1. Find the k-nearest neighbors of each point\n",
        "2. Construct a graph where each node is connected only to the k-nearest\n",
        "neighbours. Graph Matrix \ud835\udc37\ud835\udc54\n",
        "The links are typically weighted with the Euclidian distance\n",
        "3. Find graph distances between all points in the graph. Shortest paths : \ud835\udc6b\ud835\udc88\n",
        "4. Apply MDS on \ud835\udc6b\ud835\udc88\n",
        "(or PCA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8c54a17f-38a9-dbbf-bc0b-067ca46bff12"
      },
      "outputs": [],
      "source": [
        "\n",
        "def isomap(D,k,n_neighbors):\n",
        "    #k nearest neighbour algorithm\n",
        "    knr = nb.NearestNeighbors(n_neighbors= n_neighbors)\n",
        "    knr.fit(D)\n",
        "    #neighbour graph where the edges are weighted with the euclidean distance\n",
        "    kng = nb.kneighbors_graph(knr,n_neighbors,mode='distance')\n",
        "    #graph distances \n",
        "    Dist_g = ug.graph_shortest_path(kng,directed=False,method='auto')\n",
        "    #the rest is just like mds or PCA (if we want)\n",
        "    nelem = D.shape[0]\n",
        "    J = np.eye(nelem) - (1.0/nelem) * np.ones(nelem)\n",
        "    # Compute matrix B\n",
        "    B = -1.0/2 * np.dot(J,np.dot(pow(Dist_g,2),J))\n",
        "    # SVD decomposition of B  \n",
        "    U,L,V = linalg.svd(B)\n",
        "    return np.dot(U[:,:k],np.sqrt(diag(L)[:k,:k]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "73d5d8a7-0751-da86-d013-7a3df07f6de6"
      },
      "source": [
        "### Own LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "10852add-9fff-381e-07ae-aeb352a172b0"
      },
      "source": [
        "The previous techniques focus on properties of the feature space\n",
        "- They don\u2019t take into account any labels/classes the data may have\n",
        "- What if we tried to take into account properties of the data per class?\n",
        "\n",
        "Linear Discriminant Analysis (LDA):\n",
        "- maximize the distance of the class centers\n",
        "- minimize the within-class variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a0611376-0dd0-eda5-f170-e5e1269142df"
      },
      "outputs": [],
      "source": [
        "def LDA(X, Y):\n",
        "    classLabels = np.unique(Y)\n",
        "    classNum = len(classLabels)\n",
        "    datanum, dim = X.shape\n",
        "    totalMean = np.mean(X,0)\n",
        "\n",
        "\t# partition class labels per label - list of arrays per label\n",
        "\n",
        "    partition = [np.where(Y==label) for label in classLabels]\n",
        "\n",
        "\t# find mean value per class (per attribute) - list of arrays per label\n",
        "    classMean = [(np.mean(X[idx],0),len(idx)) for idx in partition]\n",
        "\n",
        "\t# Compute the within-class scatter matrix\n",
        "    Sw = np.zeros((dim,dim))\n",
        "\t# covariance matrix of each class * fraction of instances for that class \n",
        "    for idx in partition:\n",
        "        Sw=Sw + np.cov(X[idx],rowvar = 0) * len(idx)\n",
        "\t# Compute the between-class scatter matrix\n",
        "    Sb = np.zeros((dim,dim))\n",
        "    for class_mean,class_size in classMean:\n",
        "        temp=(class_mean-totalMean)[:,np.newaxis]\n",
        "        Sb=Sb+class_size*np.dot(temp,np.transpose(temp))\n",
        "\n",
        "\n",
        "\t# Solve the eigenvalue problem for discriminant directions to maximize class separability while simultaneously minimizing\n",
        "\t# the variance within each class\n",
        "    T=np.dot(linalg.inv(Sw),Sb)\n",
        "    eigval, eigvec = linalg.eig(T) \n",
        "\n",
        "\n",
        "    idx = eigval.argsort()[::-1] # Sort eigenvalues\n",
        "    eigvec = eigvec[:,idx] # Sort eigenvectors according to eigenvalues\n",
        "    W = np.real(eigvec[:,:classNum-1]) # eigenvectors correspond to k-1 largest eigenvalues\n",
        "\n",
        "\n",
        "\t# Project data onto the new LDA space\n",
        "    X_lda = np.dot(X,W)\n",
        "\n",
        "\t# project the mean vectors of each class onto the LDA space\n",
        "    projected_centroid = [np.dot(m,W) for m,class_size in classMean]\n",
        "\n",
        "    return W, projected_centroid, X_lda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "696a8eea-4732-2955-dd09-5354229ae089"
      },
      "source": [
        "### Compare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2bfac971-8ba8-7b89-04b2-3c313e165a1a"
      },
      "source": [
        "We are going to compare PCA, MDS, ISomap from Sklearn and from our own implementations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5009b5ab-b403-67d4-f3c0-2babffda0a3a"
      },
      "outputs": [],
      "source": [
        "# The next line is required for plotting only\n",
        "Axes3D\n",
        "\n",
        "iris = pd.read_csv('../input/Iris.csv')\n",
        "X = np.array(iris[[c for c in iris.columns if c != \"Species\" and c!='Id']])\n",
        "Y_iris = iris[\"Species\"]\n",
        "color = LabelEncoder().fit_transform(Y_iris)\n",
        "\n",
        "\n",
        "n_components=2\n",
        "n_neighbors=5\n",
        "\n",
        "fig = plt.figure(figsize=(15, 8))\n",
        "ax = fig.add_subplot(251, projection='3d')\n",
        "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
        "ax.view_init(4, -72)\n",
        "\n",
        "#------PCA--------our implementation\n",
        "t0 = time()\n",
        "(Y,perc)=pca1(X,n_components)\n",
        "t1 = time()\n",
        "print(\"PCA(imp): %.2g sec\" % (t1 - t0))\n",
        "ax = fig.add_subplot(252)\n",
        "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
        "plt.title(\"PCA(imp) (%.2g sec)\" % (t1 - t0))\n",
        "ax.xaxis.set_major_formatter(NullFormatter())\n",
        "ax.yaxis.set_major_formatter(NullFormatter())\n",
        "plt.axis('tight')\n",
        "#-----------------\n",
        "\n",
        "#------MDS--------our implementation (classical MDS)\n",
        "t0 = time()\n",
        "D=spd.squareform(spd.pdist(X,'euclidean'))\n",
        "Y=mds(D,n_components)\n",
        "t1 = time()\n",
        "print(\"MDS(imp): %.2g sec\" % (t1 - t0))\n",
        "ax = fig.add_subplot(253)\n",
        "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
        "plt.title(\"MDS(imp) (%.2g sec)\" % (t1 - t0))\n",
        "ax.xaxis.set_major_formatter(NullFormatter())\n",
        "ax.yaxis.set_major_formatter(NullFormatter())\n",
        "plt.axis('tight')\n",
        "#-----------------\n",
        "#------isomap--------our implementation (with MDS)\n",
        "t0 = time()\n",
        "Y=isomap(X,n_components,n_neighbors)\n",
        "t1 = time()\n",
        "print(\"Isomap(imp): %.2g sec\" % (t1 - t0))\n",
        "ax = fig.add_subplot(254)\n",
        "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
        "plt.title(\"Isomap(imp) (%.2g sec)\" % (t1 - t0))\n",
        "ax.xaxis.set_major_formatter(NullFormatter())\n",
        "ax.yaxis.set_major_formatter(NullFormatter())\n",
        "plt.axis('tight')\n",
        "#-----------------\n",
        "\n",
        "#----PCA---------- sklearn implementation \n",
        "\n",
        "t0 = time()\n",
        "pca = PCA(n_components=n_components)\n",
        "Y = pca.fit_transform(X)\n",
        "t1 = time()\n",
        "print(\"PCA: %.2g sec\" % (t1 - t0))\n",
        "ax = fig.add_subplot(257)\n",
        "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
        "plt.title(\"PCA (%.2g sec)\" % (t1 - t0))\n",
        "ax.xaxis.set_major_formatter(NullFormatter())\n",
        "ax.yaxis.set_major_formatter(NullFormatter())\n",
        "plt.axis('tight')\n",
        "#--------------------\n",
        "#----MDS---------- sklearn implementation (Stress minimization-majorization algorithm SMACOF)\n",
        "t0 = time()\n",
        "mds = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
        "Y = mds.fit_transform(X)\n",
        "t1 = time()\n",
        "print(\"MDS: %.2g sec\" % (t1 - t0))\n",
        "ax = fig.add_subplot(258)\n",
        "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
        "plt.title(\"MDS (%.2g sec)\" % (t1 - t0))\n",
        "ax.xaxis.set_major_formatter(NullFormatter())\n",
        "ax.yaxis.set_major_formatter(NullFormatter())\n",
        "plt.axis('tight')\n",
        "#---------------\n",
        "#----Isomap---------- sklearn implementation (with kernel PCA)\n",
        "t0 = time()\n",
        "Y = manifold.Isomap(n_neighbors, n_components).fit_transform(X)\n",
        "t1 = time()\n",
        "print(\"Isomap: %.2g sec\" % (t1 - t0))\n",
        "ax = fig.add_subplot(259)\n",
        "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
        "plt.title(\"Isomap (%.2g sec)\" % (t1 - t0))\n",
        "ax.xaxis.set_major_formatter(NullFormatter())\n",
        "ax.yaxis.set_major_formatter(NullFormatter())\n",
        "plt.axis('tight')\n",
        "#--------------------\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bb8781ee-8264-8328-1358-9c98d2b6c078"
      },
      "source": [
        "### LDA vs PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a4296998-cb6b-026b-11bf-bd0865e48faf"
      },
      "outputs": [],
      "source": [
        "c=['r','g','b','k']\n",
        "colors=[c[int(i-1)] for i in color]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ece1f7eb-8d3f-7850-557f-69c9bc9290eb"
      },
      "outputs": [],
      "source": [
        "# get LDA projection\n",
        "W, projected_centroids, X_lda = LDA(X, color)\n",
        "\n",
        "#perform PCA to compare with LDA\n",
        "pca = PCA(n_components=2)\n",
        "Y = pca.fit_transform(X)\n",
        "\n",
        "#PLOT them side by side\n",
        "fig = plt.figure(figsize=(15, 8))\n",
        "ax = fig.add_subplot(121)\n",
        "ax.scatter(X_lda[:,0], X_lda[:,1],color=colors)\n",
        "for ar in projected_centroids:\n",
        "\tax.scatter(ar[0], ar[1],color='k',s=100)\n",
        "ax = fig.add_subplot(122)\n",
        "ax.scatter(Y[:,0], Y[:,1],color=colors)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "df76e53c-08d1-9269-a4db-70763c049fd1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "61644604-8291-adac-550f-f4706ba74829"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}