{"metadata": {"language_info": {"version": "3.6.3", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "name": "python"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat": 4, "cells": [{"cell_type": "markdown", "metadata": {"_cell_guid": "8dbd7982-c1d2-46ce-a797-53a4bf5e7cb0", "_uuid": "e4274826e1741fb50af2dc77c543c34f2fc8d581"}, "source": ["## Parameter tuning and feature engineering for highly imbalanced data\n", "\n", "Introduction -\n", "This includes a brief EDA with details about dealing with highly imbalanced data. This notebook is a living document -- that is, I will be updating it on a daily basis with additional experiments. \n", "\n", "Things that I tried differently (so-far) :\n", "1. Down-sampling of unbalanced data (True/False) to 1:10, 1:15, 1:20 ratio. Original is ~ 1:25. \n", "2. Tried a few iterations for RandomSearchCV (Random Forest sofar) with different sub-samples (10%) for parameter tuning.  \n", "3. Addition of new feature from ps_car_13 (ps_car_13^2*90,000) "]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "38aed070-a917-4e3d-9c99-d71fd311100b", "_uuid": "6d2588b6bfe0807702295cd37666d565219fec21"}, "source": ["### Importing libararies"]}, {"cell_type": "code", "metadata": {"_cell_guid": "9291511d-00e4-4140-887f-88d6607fe81d", "_uuid": "73a1256d5801b4ba9ee83f12203ce1f72c0a16f3", "collapsed": true}, "execution_count": null, "source": ["## Importing packages\n", "from sklearn.ensemble import RandomForestRegressor\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.preprocessing import scale\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.grid_search import GridSearchCV\n", "from sklearn.grid_search import RandomizedSearchCV\n", "from sklearn.metrics import make_scorer\n", "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n", "\n", "import random\n", "import pandas as pd\n", "import numpy as np\n", "import time\n", "\n", "import matplotlib.pyplot as plt\n", "import scipy.interpolate\n", "import scipy.integrate"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "7c092c1e-a4af-4520-a987-5e315c0b8b25", "_uuid": "2fd317fb17103f298a26360cf9df70b933315cfa"}, "source": ["### Functions that I will be using later in the code (gini code taken from some other Kernel) "]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "1381d94d-9306-45e3-acd2-2b25f4931187", "_uuid": "9019cfd6c4605568301543569a6ea0a2c4f9c78d"}, "source": ["#### Defining gini for prediction score"]}, {"cell_type": "code", "metadata": {"_cell_guid": "75b945b5-42f6-4a24-a26d-d6595a4c9624", "_uuid": "716efd59c7bfb3e6132b75345eddc110a5afeb81", "collapsed": true}, "execution_count": null, "source": ["\n", "def gini(truth, predictions):\n", "    assert (len(truth) == len(predictions))\n", "    all = np.asarray(np.c_[truth, predictions, np.arange(len(truth))], dtype=np.float)\n", "    all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]\n", "    totalLosses = all[:, 0].sum()\n", "    giniSum = all[:, 0].cumsum().sum() / totalLosses\n", "\n", "    giniSum -= (len(truth) + 1) / 2.\n", "    return giniSum / len(truth)\n", "\n", "def gini_xgb(predictions, truth):\n", "    truth = truth.get_label()\n", "    return 'gini', -1.0 * gini(truth, predictions) / gini(truth, truth)\n", "\n", "def gini_lgb(truth, predictions):\n", "    score = gini(truth, predictions) / gini(truth, truth)\n", "    return 'gini', score, True\n", "\n", "def gini_sklearn(truth, predictions):\n", "    return abs(gini(truth, predictions) / gini(truth, truth))\n", "\n", "gini_scorer = make_scorer(gini_sklearn, greater_is_better=True, needs_proba=True)\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "6ae656f9-3735-411d-a1a3-7e2d7af6d3e7", "_uuid": "94609ac0924c28cceadd2d3ffd7c1b6e43733f77"}, "source": ["### Preprocessing of data"]}, {"cell_type": "code", "metadata": {"_cell_guid": "a58c4c28-037e-456a-bdd2-84e9e72c74af", "_uuid": "d5ea8ff5c7b5a21f7dbd4064383f2fae45d9df0f", "collapsed": true}, "execution_count": null, "source": ["traincsv = pd.read_csv(\"../input/train.csv\") # reading csv "], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "a39180dd-3a5e-4dc0-95ba-d9dcad3ac748", "_uuid": "98e44bd86707cae150fc9d1dfb998420949a6fce", "collapsed": true}, "execution_count": null, "source": ["train = pd.DataFrame(traincsv) # pandas df"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "1a61bd69-9d8e-4451-bfe1-34ba67405eaa", "_uuid": "a6f4fd748795af68252aea55b549410f55035a03", "collapsed": true}, "execution_count": null, "source": ["train.shape # exploring shape"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "ffde5629-f603-4576-ad52-3371c44332f6", "_uuid": "64cac358d475268642820cdbbec2cfa21d98f927", "collapsed": true}, "execution_count": null, "source": ["train.columns # exploring the columns "], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "9ba58fb9-5b3c-40fd-9405-f4ae778b21f3", "_uuid": "cb53fde32e37def1ae473181c118020b8ee46d33", "collapsed": true}, "execution_count": null, "source": ["train.iloc[:5,]"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "7aa15252-86fa-4ec6-9cc6-1973c30324a1", "_uuid": "6b7a8d8e27d868d50680c737049ea70c493e723a", "collapsed": true}, "execution_count": null, "source": ["train.describe(include = 'all')"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "7b2c27d8-4f84-44e0-9995-33d358059d89", "_uuid": "df5a5df5c35b5386f6f35c82b844ff924d340827", "collapsed": true}, "execution_count": null, "source": ["## distribution of target variable (insurance claim filed or not)\n", "\n", "import matplotlib.pyplot as plt\n", "plt.hist(train['target'])\n", "plt.show()\n", "\n", "print('Percentage of claims filed :' , str(np.sum(train['target'])/train.shape[0]*100), '%')"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "8388824d-805b-40b4-bdec-80d469c971b3", "_uuid": "77e48ffb6df1b39bb220473e1e9e5d4a5919f739", "collapsed": true}, "execution_count": null, "source": ["## Distribution of columns based on groups\n", "\n", "cols = train.columns\n", "\n", "count_ind = 0; count_cat = 0; count_calc = 0; count_car = 0; count_remain = 0\n", "\n", "for col in cols:\n", "    col = str(col)\n", "    if 'ind' in col:\n", "        count_ind += 1\n", "    elif 'cat' in col:\n", "        count_cat += 1\n", "    elif 'calc' in col:\n", "        count_calc += 1\n", "    elif 'car' in col:\n", "        count_car += 1\n", "    else:\n", "        count_remain += 1\n", "\n", "print(\"Total columns are\", train.shape[1])\n", "print(\"Total ind columns: \", count_ind, \"| Percentage:\", round(count_ind*100/train.shape[1],1))\n", "print(\"Total cat columns: \", count_cat, \"| Percentage:\", round(count_cat*100/train.shape[1],1))\n", "print(\"Total calc columns: \", count_calc, \"| Percentage:\", round(count_calc*100/train.shape[1],1))\n", "print(\"Total car columns: \", count_car, \"| Percentage:\", round(count_car*100/train.shape[1],1))\n", "print(\"Total other columns: \", count_remain, \"| Percentage:\", round(count_remain*100/train.shape[1],1))"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "107f0096-8c12-47b0-83b7-988d2c99c66b", "_uuid": "a930bf4d7b281c712c4f219878f080d723b1b328", "collapsed": true}, "execution_count": null, "source": ["## Perentage of missing values in each column. (Only columns with % > 0)\n", "\n", "missing = np.sum(train == -1)/train.shape[0]*100\n", "print(\"Percentage of missing values (denoted by -1)\")\n", "print(missing[missing > 0].sort_values(ascending = False))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "f5c0674b-8860-4daf-9e13-09ee7601afcb", "_uuid": "18e1bda32d34277b07643daf3724345280238c29"}, "source": ["I will not change anything in **categorical** columns with null. But will add new columns of **isNull** (0,1) in those. For continous variables, -1 is better to be replaced by **median**. Let's do that below."]}, {"cell_type": "code", "metadata": {"_cell_guid": "a1f54c87-0e6f-42a0-a057-91119b939d82", "_uuid": "a43a03de53c801d69fa6390c1888c11ef0a67cf3", "collapsed": true}, "execution_count": null, "source": ["# making a copy\n", "train_natreat = train.copy()\n", "train_natreat.shape"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "e6b3f788-7b86-4f55-bc8a-73a5de1dccb0", "_uuid": "b692764f362cad2c0832f0b4257ad373ee9e7fb7", "collapsed": true}, "execution_count": null, "source": ["#train_natreat.columns"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "0da26dc2-b3d4-4c0e-ab81-577480ab8f76", "_uuid": "52fd6d8f6a6fd789e891d46acccabc8eff948e69"}, "source": ["### Transformations\n", "\n", "* Adding dummy columns \"IsNA?\" with 0/1 in rows for each variable with NAs  \n", "* Replacing -1 with median for *continuous* variables\n", "* Adding a column with sum of NAs for each row\n", "* By running random forest, **ps_car_13** was found to be one of the most important feature. Adding a transformation = round(ps_car_13^2*90000 , 2). [Thanks to \"raddar\" - https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/41489]"]}, {"cell_type": "code", "metadata": {"_cell_guid": "4c88f2eb-a4ed-4b6d-ad7d-03d01ab17d1d", "scrolled": false, "_uuid": "ff896605caa3ae7105c3632d4db469f3927704e9", "collapsed": true}, "execution_count": null, "source": ["## treating missing values. (denoted by -1 in data)\n", "from statistics import median\n", "\n", "# categorical NA variables\n", "cat_na_cols = ['ps_car_07_cat', 'ps_car_09_cat','ps_car_01_cat', 'ps_car_02_cat' ,\n", "              'ps_ind_05_cat', 'ps_ind_02_cat', 'ps_ind_04_cat']\n", "\n", "# adding dummy isNA variable for each feature with atleast 1 NA (-1)\n", "for l in cat_na_cols:\n", "    #train[l] = train[l].replace(-1, np.nan)\n", "    new_col_name = str(l) + \"isNull\"\n", "    train_natreat[new_col_name] = (train_natreat[l] == -1).astype('int')\n", "    print('done')\n", "    \n", "num_na_cols = ['ps_reg_03', 'ps_car_14', 'ps_car_11', 'ps_car_12' ]\n", "for l in num_na_cols:\n", "    new_col_name = str(l) + \"isNull\"\n", "    train_natreat[new_col_name] = (train_natreat[l] == -1).astype('int')\n", "    train_natreat[l] = train_natreat[l].replace(-1, train_natreat[l].median())\n", "\n", "# column with sum of NA in each row\n", "train_natreat[\"sumNA\"] = (train_natreat == -1).astype(int).sum(axis=1)\n", "\n", "## Another important feature with transformation of ps_car_13\n", "train_natreat['ps_car_13_new1'] = round(train_natreat['ps_car_13']*train_natreat['ps_car_13']*90000,2)\n", "train_natreat.columns\n", "\n", "print(\"new columns are - \", train_natreat.columns)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "a9d51d6c-4c90-4d3d-835a-aae36a8163f8", "_uuid": "ecd8604e1bafa361dc93ae69105eb034207b03dd"}, "source": ["##### Skipping logistic regression"]}, {"cell_type": "code", "metadata": {"_cell_guid": "1cfb8357-791d-43dd-9434-1abf76d37507", "_uuid": "2161c2ab81bf84be74d2279f01a1392dd226d507", "collapsed": true}, "execution_count": null, "source": ["# # split data into X and Y\n", "# X = train_nacleaned.iloc[:,2:train_nacleaned.shape[1]]\n", "# Y = train_nacleaned.iloc[:,1]\n", "\n", "# # scaling model to feed into logistic regression\n", "# X_scaled = scale(X)\n", "\n", "# # Split data into train (80%) /test (20%) set \n", "# from sklearn.model_selection import train_test_split\n", "# x_train, x_test, y_train, y_test = train_test_split(X_scaled, Y, test_size = 0.20, random_state = 99)"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "0ee17f23-7af6-48ba-ba88-46e6d7c5a2a8", "scrolled": true, "_uuid": "457917a851dd2d09baae710b97399d8c98e0750b", "collapsed": true}, "execution_count": null, "source": ["# # Fit logistic regression model\n", "# logit = LogisticRegression(penalty = 'l1', random_state = 99)\n", "# logit.fit(x_train, y_train)"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "2c997d1a-69a2-4662-8cd7-6a9b5fe5f241", "_uuid": "8436285b95bdf33295035b8b0ddd96eb2cf81110", "collapsed": true}, "execution_count": null, "source": ["# Predict logistic regression on test data and display sensitivity/specificity\n", "# logit_predict = logit.predict(X_test)\n", "# plt.hist(logit_predict)\n", "# plt.show()\n", "# nofaults, val = np.where(y_test==0)\n", "# faults, val = np.where(y_test==1)\n", "# sensitivity = sum(logit_predict[faults])/len(faults)\n", "# logit_predict_nofaults = logit_predict[nofaults]\n", "# specificity = float(len(logit_predict_nofaults[logit_predict_nofaults == 0]))/len(nofaults)\n", "# print \"LOGIT: sensitivity =\",sensitivity, \"| specificity =\",specificity"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "26ab0ba2-0ecc-43d2-8b4a-b20373d5fc95", "_uuid": "a52ffb2caa4584c9f78011173039df13cf5b793d"}, "source": ["## Random Forest Method - \n"]}, {"cell_type": "code", "metadata": {"_cell_guid": "54896384-7db2-460a-94ec-12953e0b1b81", "_uuid": "1bb3d2dc980af789abfa1edf495e00d90aa4ad03", "collapsed": true}, "execution_count": null, "source": ["# split data into df and Y\n", "df = train_natreat.drop('target', axis = 1)\n", "y = train_natreat.iloc[:,1]"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "c397f508-5643-477a-93b3-89f909f69d88", "_uuid": "ceab7243f31146a525e5d37e39a2c545bc6d531d", "collapsed": true}, "execution_count": null, "source": ["x_train, x_test, y_train, y_test = train_test_split(df, y, test_size = 0.20, random_state = 99)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "0a749bf3-e16e-4616-be7b-db3acc92045c", "_uuid": "dd79e19927ddcfc95b94183637846d1f40227acb"}, "source": ["Loading test data "]}, {"cell_type": "code", "metadata": {"_cell_guid": "c1f36d5e-9a81-43bc-859d-d55ed26819cf", "_uuid": "ae6b996d86733a29118934261c89b896a0866682", "collapsed": true}, "execution_count": null, "source": ["# test = pd.read_csv(\"test.csv\")\n", "# test = pd.DataFrame(test)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "4127bac7-71a9-40b6-8fb1-6c5a512e6f70", "_uuid": "22a9a34c3a9aee4da2a46a98078b0a016c3a9bac"}, "source": ["#### Transformations for test set\n", "\n", "Just replicating whatever transformation I did in `train`"]}, {"cell_type": "code", "metadata": {"_cell_guid": "88bd5037-f08e-4cd7-bc90-1e5a874b2f66", "_uuid": "5aa87e8b9d09b62748f2696aa55beda858e54695", "collapsed": true}, "execution_count": null, "source": ["## treating missing values. (denoted by -1 in data)\n", "\n", "# Commenting it out as not uploading test data here. Just 20-80 split\n", "\n", "\"\"\"\n", "from statistics import median\n", "\n", "# categorical NA variables\n", "cat_na_cols = ['ps_car_07_cat', 'ps_car_09_cat','ps_car_01_cat', 'ps_car_02_cat' ,\n", "              'ps_ind_05_cat', 'ps_ind_02_cat', 'ps_ind_04_cat']\n", "\n", "## Adding a dummy column IsNA? for all columns with NA. \n", "# Even if it's not important, it doesn't harm to input these for random forest\n", "\n", "for l in cat_na_cols:\n", "    #train[l] = train[l].replace(-1, np.nan)\n", "    new_col_name = str(l) + \"isNull\"\n", "    test[new_col_name] = (test[l] == -1).astype('int')\n", "    print('done')\n", "    \n", "num_na_cols = ['ps_reg_03', 'ps_car_14', 'ps_car_11', 'ps_car_12' ]\n", "for l in num_na_cols:\n", "    new_col_name = str(l) + \"isNull\"\n", "    test[new_col_name] = (test[l] == -1).astype('int')\n", "    test[l] = test[l].replace(-1, test[l].median())\n", "    print('done')\n", "    \n", "# column with sum of NA in each row\n", "test[\"sumNA\"] = (test == -1).astype(int).sum(axis=1)\n", "\n", "test['ps_car_13_new1'] = round(test['ps_car_13']*test['ps_car_13']*90000,2)\n", "\"\"\""], "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "4ad25c96-f961-4193-9134-6c7bc47f6526", "_uuid": "ce65cd93333d4357f3603ce98d0240fa86cad8cd"}, "source": ["### Parameter tuning for Random Forest"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "bb114bc1-bbff-4796-8215-29f18705dd81", "_uuid": "293e5c3457803253b95ddab80500c3621e9ce8c5"}, "source": ["#### Self tuning for parameters without using CV"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "2b7ae777-8ccd-49d3-8038-e90cf67e1aac", "_uuid": "35b8a258e7c7f128b80b3d6a8eced0b42df98938"}, "source": ["Downsampling as data is unbalanced. True ratio is like ~1/25. Let's pick 3 different ratios (but I will take total rows to be = 50,000 to increase CV speed). I hope I don't loose much information. Ratios -\n", "1. **1/20** (Trues are 2500/ 50,000)\n", "2. **1/15** (Trues are 3333/ 50,000)\n", "3. **1/10** (Trues are 5000/ 50,000)"]}, {"cell_type": "code", "metadata": {"_cell_guid": "7f0fe92a-532e-4fc1-9970-8ebadb031ce8", "_uuid": "b9ab1ddf04800da7afb3599431ad49872ca56eb3", "collapsed": true}, "execution_count": null, "source": ["# sub sampling for tuning (will work on only 1 lakhs rows of data)\n", "count0 = np.sum(train_natreat['target'] == 0)\n", "count1 = np.sum(train_natreat['target'] == 1)\n", "count0 + count1 == train_natreat.shape[0]"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "27252298-f317-434e-bf0e-ef7e7d866447", "_uuid": "802e4d7ba97f237ad1869b4b04658e1174f7808f"}, "source": ["#### Grid of parameters to tune"]}, {"cell_type": "code", "metadata": {"_cell_guid": "4ff53a5d-3c5e-485d-9ecb-dc2033d7f47d", "_uuid": "3781ae47f9ae03812706fd4cb4dc11dfb2aa107b", "collapsed": true}, "execution_count": null, "source": ["param_grid = {\"n_estimators\": np.arange(25, 500, 50,dtype=int),\n", "              \"max_depth\": np.arange(1, 20, 2),\n", "              #\"min_samples_split\": np.arange(1,150,1),\n", "              \"min_samples_leaf\": [1,5,10,50,100,200,500]}\n", "              #\"max_leaf_nodes\": np.arange(2,60,6),\n", "              #\"min_weight_fraction_leaf\": np.arange(0.1,0.4, 0.1)}\n", "\n", "# param_grid = {\"n_estimators\": np.arange(25, 500, 50,dtype=int),\n", "#               \"max_depth\": np.arange(1, 10, 1)}"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "469180b5-d0e3-4e1d-bdbf-bbdd75ade0fd", "_uuid": "a2afcda59ca81adf5aef899715c657f7322ce6ce"}, "source": ["#### Iterating for sub samle of 50000 and 3 different True/False ratios to find best parameters.\n", "\n", "I have commented the below code as it takes 15 minutes to run. But I have hardcoded the parameters/output from cross validation. "]}, {"cell_type": "code", "metadata": {"_cell_guid": "fe07e263-98ed-4fa6-8c77-49bdf01f5038", "scrolled": true, "_uuid": "280ea5a0d071aab3a0e0358e51c5311212c4a11a", "collapsed": true}, "execution_count": null, "source": ["features = train_natreat.columns.drop(['id', 'target'], 1)\n", "rf = RandomForestClassifier(random_state=50, n_jobs = -1, oob_score=True)\n", "\n", "# for i in range(3):\n", "#     for j in [2500, 3333, 5000]:\n", "#         index0 = train_natreat.index[train_natreat['target'] == 0]\n", "#         index1 = train_natreat.index[train_natreat['target'] == 1]\n", "\n", "#         subsample_index0 = random.sample(list(index0),50000 - j)\n", "#         subsample_index1 = random.sample(list(index1),j)\n", "#         subsample_index_both = subsample_index0 + subsample_index1\n", "    \n", "#         train_natreat_sample = train_natreat.ix[subsample_index_both]\n", "\n", "#         # splitting y and x\n", "#         #features = train_natreat_sample2.columns.drop(['id', 'target'], 1)\n", "#         df = train_natreat_sample[features]\n", "#         y = train_natreat_sample.iloc[:,1]\n", "              \n", "#         random_cv = RandomizedSearchCV(rf, param_distributions = param_grid, cv= 3, scoring=gini_scorer)\n", "#         random_cv.fit(df, y)\n", "#         print(random_cv.best_score_)\n", "#         print(random_cv.best_params_)\n", "#         print(random_cv.best_estimator_)\n", "    \n", "#score = cross_val_score(rf, df, y, scoring=gini_scorer, cv=StratifiedKFold()).mean()\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "c80a8c74-2961-43ee-bc5a-2379dd9947bd", "_uuid": "5a7a26013144b12604f55b738eb81be7053c516c"}, "source": ["### Hardcoding selected parameters from CV\n", "\n", "-0.21262327096741435\n", "{'n_estimators': 225, 'min_samples_leaf': 1, 'max_depth': 1}\n", "\n", "-0.2169821613491697\n", "{'n_estimators': 175, 'min_samples_leaf': 1, 'max_depth': 15}\n", "\n", "-0.21034002745394356\n", "{'n_estimators': 375, 'min_samples_leaf': 1, 'max_depth': 1}\n", "\n", "-0.18180637478659406\n", "{'n_estimators': 25, 'min_samples_leaf': 1, 'max_depth': 9}\n", "\n", "-0.18410624559552097\n", "{'n_estimators': 125, 'min_samples_leaf': 1, 'max_depth': 17}\n", "\n", "-0.21881615663247375\n", "{'n_estimators': 175, 'min_samples_leaf': 1, 'max_depth': 1}\n", "\n", "-0.20685089187304984\n", "{'n_estimators': 25, 'min_samples_leaf': 5, 'max_depth': 11}\n", "\n", "-0.19705679411050572\n", "{'n_estimators': 225, 'min_samples_leaf': 1, 'max_depth': 19}\n", "\n", "-0.22252957405534599\n", "{'n_estimators': 175, 'min_samples_leaf': 50, 'max_depth': 1}\n", "\n", "#### So, I am selecting {'n_estimators': 175, 'min_samples_leaf': 50, 'max_depth': 10}. \n", "But this is only for small sub-sample. Gini should increase for complete sample set."]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "416a325c-5a34-482c-b651-0befa7fa27cb", "_uuid": "0fcf531007b487f7e8b69920a3df9ccb26c9be6b"}, "source": ["Now let's fit the model using tuned parameters and submit predictions "]}, {"cell_type": "code", "metadata": {"_cell_guid": "7a1f8c3c-5f31-4068-854f-ad189d64e164", "scrolled": true, "_uuid": "4c2db36df1da19f117a5c4b2d4f0aa5554fd2864", "collapsed": true}, "execution_count": null, "source": ["#from sklearn.ensemble import RandomForestClassifier\n", "x_train, x_test, y_train, y_test = train_test_split(train_natreat[features], train_natreat.iloc[:,1], test_size = 0.20, random_state = 99)\n", "\n", "rf_final = RandomForestClassifier(criterion='gini',n_jobs=-1,min_samples_leaf=50,\n", "            max_depth=10, n_estimators=175, random_state=50, oob_score = True)\n", "rf_final.fit(x_train, y_train)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "b73a5054-67a7-4987-8fc3-c93741542cdf", "_uuid": "171d1d8600aeef23a32b9692e0297d10bfa740ba"}, "source": ["### Plotting top features"]}, {"cell_type": "code", "metadata": {"_cell_guid": "d31494fa-122f-4fa5-a46a-22e49b7087e4", "_uuid": "96da3dc6e3a9d5cd2b67983723d9cb3907c56a43", "collapsed": true}, "execution_count": null, "source": ["objects = x_train[features].columns\n", "performance = rf_final.feature_importances_\n", "features1 = pd.concat([pd.Series(objects),  pd.Series(performance)], axis = 1)\n", "features1.columns = ['feature', 'importance']\n", "features2 = features1.sort_values(by = 'importance', ascending=False)\n", "\n", "top15_feautures = features2[0:15]\n"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "0afcdf29-054f-4bee-a16b-6c2079000303", "_uuid": "90e7076309b1f95a458647e2bd3d7b12175fcc02", "collapsed": true}, "execution_count": null, "source": ["objects = top15_feautures.iloc[:,0]\n", "y_pos = np.arange(len(objects))\n", "performance = top15_feautures.iloc[:,1]\n", " \n", "plt.barh(y_pos, performance, align='center', alpha=0.5)\n", "plt.yticks(y_pos, objects)\n", "plt.xlabel('Weights')\n", "plt.title('Features')\n", " \n", "plt.show()"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "0b28f4f3-cfcb-41a7-aaaa-81675280259d", "_uuid": "53806051ac8181994af9730d663b7716283049fb"}, "source": ["* **ps_car_13_new1** is the feature that I transfored using ps_car_13. It is coming out to be most important feature now. \n", "* None of the feature related to \"NA\" came out to be important (atleast in top 15)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "03f10ec8-e656-4d04-89f8-6a7d45ba1541", "_uuid": "2e7a3776beb515f2c159bc7df2b765f25dd8a6c8"}, "source": ["#### Prediction on test (remaining 20%) set"]}, {"cell_type": "code", "metadata": {"_cell_guid": "035a6e74-11ad-4810-8bbe-b8f10cf797de", "_uuid": "5ec0f349aed75cc816057cc9dd75d9ec4490442e", "collapsed": true}, "execution_count": null, "source": ["prediction = rf_final.predict_proba(x_test[features])\n", "plt.hist(prediction[:,1])\n", "plt.show()\n"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "15e0e834-e2c0-460d-be4d-781014a849c3", "_uuid": "491cc7c48469a5cc0ee29f66b28c6fbd214541e4", "collapsed": true}, "execution_count": null, "source": ["from sklearn.metrics import roc_curve, auc\n", "fpr, tpr, thresholds = roc_curve(y_test, prediction[:,1])\n", "plt.plot(fpr, tpr,'r')\n", "plt.plot([0,1],[0,1],'b')\n", "plt.title('AUC: {}'.format(auc(fpr,tpr)))\n", "plt.show()"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "eff36abf-4116-44d2-8894-65e2662f45bb", "_uuid": "8eec4a376b2ba3cb0bf4d81973aff2183653158d", "collapsed": true}, "execution_count": null, "source": ["def gini_normalized(actual, pred):\n", "    return gini(actual, pred) / gini(actual, actual)\n", "\n", "gini_predictions = gini(y_test, prediction[:,1])\n", "gini_max = gini(y_test, y_test)\n", "ngini= gini_normalized(y_test, prediction[:,1])\n", "print('Gini: %.3f, Max. Gini: %.3f, Normalized Gini: %.3f' % (gini_predictions, gini_max, ngini))"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "0799a0be-edb3-4625-8782-78118f2d397d", "_uuid": "977d1be7772fdc8ccd920823edd58220b413436d", "collapsed": true}, "execution_count": null, "source": [], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "bc04a411-3222-46c3-95d2-0aaa116c0b4a", "_uuid": "37cfa2fb6d5deffaeba93a46c22dd6bdfcffb4a3", "collapsed": true}, "execution_count": null, "source": [], "outputs": []}], "nbformat_minor": 1}