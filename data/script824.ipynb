{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "40bcc798-0757-45c1-6bb3-6234af977c92"
      },
      "source": [
        "##Exploring several ways to understand the data##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c8a2ba42-05f6-39ed-b4e1-19209e75ae4d"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in\n",
        "\n",
        "import sys\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import random\n",
        "import matplotlib.pyplot as plt # plotting\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "%matplotlib inline\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "107ba22d-d52b-2f65-2b98-55decd0529d8"
      },
      "outputs": [],
      "source": [
        "import kagglegym\n",
        "\n",
        "env = kagglegym.make()\n",
        "\n",
        "# get the reference to the api\n",
        "observation = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "37daee34-3534-1734-3a4a-026540bc2d75"
      },
      "outputs": [],
      "source": [
        "with pd.HDFStore('../input/train.h5') as train:\n",
        "    df = train.get('train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "abc3ec03-f02a-4427-f436-94121a2c2009"
      },
      "outputs": [],
      "source": [
        "train = df.copy(deep = True)\n",
        "ID = \"id\"\n",
        "TSTAMP = \"timestamp\"\n",
        "TARGET = \"y\"\n",
        "print(\"There are {} rows and {} columns in the dataset\".format(*train.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ecbe93db-c24b-a6b6-2778-01782be29cd0"
      },
      "source": [
        " **Let us divide the columns according to the prefixes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "565f2b66-4a56-8eb5-2adb-1c29fcbbcc99"
      },
      "outputs": [],
      "source": [
        "def findMatchedColumnsUsingPrefix(prefix, df):\n",
        "    columns = df.columns[df.columns.str.startswith(prefix)]\n",
        "    return list(columns.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "49a6ca69-5d33-6aa8-df48-0bdd556fc5b4"
      },
      "outputs": [],
      "source": [
        "derived_columns = findMatchedColumnsUsingPrefix(\"derived\", train)\n",
        "fundamental_columns = findMatchedColumnsUsingPrefix(\"fundamental\", train)\n",
        "technical_columns = findMatchedColumnsUsingPrefix(\"technical\", train)\n",
        "\n",
        "print(\"There are {} derived columns\".format(len(derived_columns)))\n",
        "print(\"There are {} fundamental columns\".format(len(fundamental_columns)))\n",
        "print(\"There are {} technical columns\".format(len(technical_columns)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0ffd8421-92ad-263c-0141-3e243fb0126c"
      },
      "outputs": [],
      "source": [
        "ids = train[ID].unique()\n",
        "tstamps = train[TSTAMP].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c175eebf-3daa-4609-f832-1d00f7376ea9"
      },
      "source": [
        "**This function calculates the number of missing records within an asset for the group of columns passes as a reference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3a4e5a84-8ac9-1f7b-5959-26cf0765ab0d"
      },
      "outputs": [],
      "source": [
        "def findMissingEntriesForIds(columns, train, identifier):\n",
        "    indexes = []\n",
        "    \n",
        "    # prepare the header\n",
        "    rows = {}\n",
        "    rows['id'] = []\n",
        "    for column in columns:\n",
        "        rows[column] = []\n",
        "        \n",
        "    # count number of missing entries in a column for an id group\n",
        "    for id, group in train.groupby(identifier):\n",
        "        rows['id'].append(id)\n",
        "        for column in columns:\n",
        "            rows[column].append(pd.isnull(group[column]).sum())\n",
        "            \n",
        "    df = pd.DataFrame(rows)\n",
        "    #df.columns = pd.MultiIndex.from_tuples([tuple(c.split('_')) for c in df.columns])\n",
        "    #df = df.stack(0).reset_index(1)\n",
        "    #df.sort_index()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5da2cc61-b046-111b-1f1e-272f4197b119"
      },
      "source": [
        "**Calculates count of missing records for derived columns group by assets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6da04813-1571-2b8b-eabe-f242188cd3ee"
      },
      "outputs": [],
      "source": [
        "derived = findMissingEntriesForIds(derived_columns, train, ID)\n",
        "derived.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e48bf788-3ad7-7d46-2bf4-7d49862f9b27"
      },
      "source": [
        "**Calculates count of missing records for fundamental columns group by assets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aae55b86-fb83-30d1-5c1e-fcdb70a19589"
      },
      "outputs": [],
      "source": [
        "fundamental = findMissingEntriesForIds(fundamental_columns, train, ID)\n",
        "fundamental.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "34394f6c-4087-62f4-fe53-314e29fb896f"
      },
      "source": [
        "**Calculates count of missing records for technical columns group by assets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5bf97d5a-e583-fc57-c028-75f69341c43f"
      },
      "outputs": [],
      "source": [
        "technical = findMissingEntriesForIds(technical_columns, train, ID)\n",
        "technical.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d1f4a472-7c44-bd8f-176e-83cafc5fc373"
      },
      "source": [
        "**Find total count of missing entries for each column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4cf31474-7dba-b240-ded0-f09fe64f77dd"
      },
      "outputs": [],
      "source": [
        "def calculateColumnSum(res, columns):\n",
        "    names = []\n",
        "    values = []\n",
        "    for column in columns:\n",
        "        names.append(column)\n",
        "        values.append(res[column].sum())\n",
        "    data = pd.DataFrame({'columns' : names, 'counts' : values})\n",
        "    data = data.sort_values(by=['counts'], ascending = False)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "16b14e40-2bcd-a01f-df19-e6b501af1f92"
      },
      "outputs": [],
      "source": [
        "def createHorizontalBarPlot(labels, missing, plot_width, plot_height, width, title):\n",
        "    N = len(labels)\n",
        "    ind = np.arange(N)\n",
        "    fig, ax = plt.subplots(figsize = (plot_width, plot_height))\n",
        "    rects = ax.barh(ind, missing, width)\n",
        "    ax.set_yticks(ind + width / 2)\n",
        "    ax.set_yticklabels(labels)\n",
        "    ax.set_title(title)\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8bc12289-3eae-bb1d-0097-3640821e1036"
      },
      "outputs": [],
      "source": [
        "result = calculateColumnSum(derived, derived_columns)\n",
        "print(\"The columns with minimum null is {}\".format(result.iloc[-1]['columns']))\n",
        "print(\"The columns with maximum null is {}\".format(result.iloc[0]['columns']))\n",
        "\n",
        "createHorizontalBarPlot(result['columns'], result['counts'], 7, 6, 0.45, 'Missing counts by columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "813ecb97-6943-936b-11c2-71b4ed22be03"
      },
      "outputs": [],
      "source": [
        "result = calculateColumnSum(fundamental, fundamental_columns)\n",
        "print(\"The columns with minimum null is {}\".format(result.iloc[-1]['columns']))\n",
        "print(\"The columns with maximum null is {}\".format(result.iloc[0]['columns']))\n",
        "\n",
        "createHorizontalBarPlot(result['columns'], result['counts'], 10, 15, 0.65, 'Missing counts by columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fb365fc7-3db5-1e4a-0aa8-41182fb8d5d9"
      },
      "outputs": [],
      "source": [
        "result = calculateColumnSum(technical, technical_columns)\n",
        "print(\"The columns with minimum null is {}\".format(result.iloc[-1]['columns']))\n",
        "print(\"The columns with maximum null is {}\".format(result.iloc[0]['columns']))\n",
        "\n",
        "createHorizontalBarPlot(result['columns'], result['counts'], 10, 15, 0.45, 'Missing counts for technicals')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "577315a7-3c28-c79b-d7ea-462bc6b9c282"
      },
      "source": [
        "**Join the frames of missing records counts for all columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3c271358-6d41-5621-89ad-af31c1db7a13"
      },
      "outputs": [],
      "source": [
        "merged = derived.merge(fundamental, how = 'inner', on = 'id')\n",
        "merged = merged.merge(technical, how = 'inner', on = 'id')\n",
        "merged.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a487d513-3882-ccdf-2111-1d0d27ae15d5"
      },
      "source": [
        "**Add a new column sum which calculates count of missing records for an asset across all columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5b365ade-0dea-008a-3835-6335cc737ea8"
      },
      "outputs": [],
      "source": [
        "copy = merged.copy(deep = True)\n",
        "copy['sum'] = copy.apply(lambda x : x.sum(), axis = 1)\n",
        "copy = copy.sort_values(by='sum', ascending = True)\n",
        "result = copy[['id', 'sum']]\n",
        "result.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "73a5a21f-3607-a7fd-0575-e5d3a0faa893"
      },
      "outputs": [],
      "source": [
        "print(\"The asset with minimum missing records for all columns = {}\".format(result.iloc[0][ID]))\n",
        "print(\"The asset with maximum missing records for all columns = {}\".format(result.iloc[-1][ID]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f16a0cc3-c139-df34-4547-7a08f8ce75f0"
      },
      "source": [
        "**This chart prints the top x assets with minimum missing records**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "50136f02-48ee-94c3-0e4e-29d00bb28d5b"
      },
      "outputs": [],
      "source": [
        "plot_df = result[:5]\n",
        "createHorizontalBarPlot(plot_df['id'], plot_df['sum'], 5, 4, 0.75, 'Missing by assets')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6ca87bf4-5925-224b-740d-b6301d731f23"
      },
      "source": [
        "**This chart prints the top x assets with maximum missing records**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4d25d96d-488d-9e80-fa3e-e17c8bedc916"
      },
      "outputs": [],
      "source": [
        "plot_df = result.tail(5)\n",
        "createHorizontalBarPlot(plot_df['id'], plot_df['sum'], 5, 4, 0.75, 'Missing by assets')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bc917198-a693-5db9-96b2-eec8c9e141fd"
      },
      "source": [
        "**This function calculates the timestamps where data is missing for an asset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "74dce789-dfa7-af61-a3d3-b0a44f446d4b"
      },
      "outputs": [],
      "source": [
        "def getTstampForMissingRecordsInColumn(df, columns, identifier):\n",
        "    records = []\n",
        "    for id, group in df.groupby(identifier):\n",
        "        group_index_dict = {}\n",
        "        group_index_dict[identifier] = id\n",
        "        for col in columns:\n",
        "            group_index_dict[col] = list(group[pd.isnull(group[col])][TSTAMP])\n",
        "        records.append(group_index_dict)\n",
        "    return records"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1e86c4cd-f3b7-08d0-f6ec-0e3565a74378"
      },
      "source": [
        "**Execute the above function for derived columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7872b69d-72d8-e739-1b84-f50ed29d9e17"
      },
      "outputs": [],
      "source": [
        "missing_tstamps = getTstampForMissingRecordsInColumn(train, derived_columns, ID)\n",
        "missing_tstamps[:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3b5c0810-e647-ae62-1d18-6ff1535648e7"
      },
      "source": [
        "**From the result of the above function we calculate the difference between the maximum and the minimum timestamp to determine if they are consecutive in nature**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7d5aff6f-f77a-be37-b0c5-d647c03f4456"
      },
      "outputs": [],
      "source": [
        "def findTstampDiffForMissingRecordsByID(tstamp_indices):\n",
        "    rows = []\n",
        "    for item in tstamp_indices:\n",
        "        row_dict = {}\n",
        "        for key, value in item.items():\n",
        "            if key == 'id':\n",
        "                row_dict[key] = value\n",
        "            else:\n",
        "                row_dict[key] = int((value[-1] - value[0] + 1) / len(value)) if len(value) > 1 else len(value)\n",
        "        rows.append(row_dict)\n",
        "\n",
        "    return pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f8ae67dd-4150-fe5a-776c-c28a4f86c8b6"
      },
      "outputs": [],
      "source": [
        "tstamp_diff_df = findTstampDiffForMissingRecordsByID(missing_tstamps)\n",
        "tstamp_diff_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7009d6d6-17e2-9c60-0277-b1f00c745cb4"
      },
      "source": [
        "***1* - The timestamps for the missing entries are in succession.</br>**\n",
        "\n",
        "***0* - There are no missing records.</br>**\n",
        "\n",
        "***Anything besides 1 and 0 means that the timestamps are not in succession, some entries are there between the missing values.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a190e8b0-f0ea-faca-bc12-cf084d4f6b52"
      },
      "source": [
        "**This function brings out those columns with non-consecutive missing timestamps.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1672f80e-0f14-ac51-2ce0-786f2e5ab974"
      },
      "outputs": [],
      "source": [
        "def findColumnsWithNonConsecutiveMissingTstampDiff(columns, tstamp_diff_dataframe):\n",
        "    columns_with_discrete_missing_tstamps = {}\n",
        "    \n",
        "    for column in columns:\n",
        "        unique_arr = list(tstamp_diff_dataframe[column].unique())\n",
        "        temp = [i for i in unique_arr if i not in [1, 0]]\n",
        "        if len(temp) > 0:\n",
        "            columns_with_discrete_missing_tstamps[column] = temp\n",
        "        \n",
        "    return columns_with_discrete_missing_tstamps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a98063dd-31ab-6265-50a8-28431381f257"
      },
      "outputs": [],
      "source": [
        "tstamp_diff_dict = findColumnsWithNonConsecutiveMissingTstampDiff(derived_columns, tstamp_diff_df)\n",
        "tstamp_diff_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2fc22023-bf7d-07f9-cbea-b6f62330da14"
      },
      "source": [
        "**Determine the identifiers where the missing timestamps are not in succession**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "be7c6c7a-8d61-f04a-009b-cbcad3ac41c8"
      },
      "outputs": [],
      "source": [
        "def findAssetsWithNonConsecutiveMissingTstamp(discrete_missing_tstamp_diff_dict, tstamp_diff_data):\n",
        "    assets_with_discrete_missing_tstamps = []\n",
        "    \n",
        "    for key, values in discrete_missing_tstamp_diff_dict.items():\n",
        "        data = tstamp_diff_data.ix[tstamp_diff_data[key].isin(values)]\n",
        "        assets_with_discrete_missing_tstamps += list(data[ID].values)\n",
        "    \n",
        "    return list(set(assets_with_discrete_missing_tstamps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "71657a3a-c3fe-973b-eafc-5a41478bc356"
      },
      "outputs": [],
      "source": [
        "id_with_discrete_missing_tstamp = findAssetsWithNonConsecutiveMissingTstamp(tstamp_diff_dict, tstamp_diff_df)\n",
        "id_with_discrete_missing_tstamp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e9ae17b2-9209-a6e9-0dde-a83dde914b72"
      },
      "outputs": [],
      "source": [
        "missing_tstamps = getTstampForMissingRecordsInColumn(train, fundamental_columns, ID)\n",
        "tstamp_diff_df = findTstampDiffForMissingRecordsByID(missing_tstamps)\n",
        "\n",
        "tstamp_diff_dict = findColumnsWithNonConsecutiveMissingTstampDiff(fundamental_columns, tstamp_diff_df)\n",
        "id_with_discrete_missing_tstamp += findAssetsWithNonConsecutiveMissingTstamp(tstamp_diff_dict, tstamp_diff_df)\n",
        "id_with_discrete_missing_tstamp = list(set(id_with_discrete_missing_tstamp))\n",
        "id_with_discrete_missing_tstamp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c4f7acd8-d421-c379-f63a-c4a6b927d761"
      },
      "outputs": [],
      "source": [
        "missing_indices = getTstampForMissingRecordsInColumn(train, technical_columns, ID)\n",
        "tstamp_diff_df = findTstampDiffForMissingRecordsByID(missing_indices)\n",
        "\n",
        "tstamp_diff_dict = findColumnsWithNonConsecutiveMissingTstampDiff(technical_columns, tstamp_diff_df)\n",
        "id_with_discrete_missing_tstamp += findAssetsWithNonConsecutiveMissingTstamp(tstamp_diff_dict, tstamp_diff_df)\n",
        "id_with_discrete_missing_tstamp = list(set(id_with_discrete_missing_tstamp))\n",
        "id_with_discrete_missing_tstamp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fb207bd1-a2e3-49fa-c076-16f53ca1d4b6"
      },
      "outputs": [],
      "source": [
        "print(\"The assets with missing entries that are not consecutive is {}\".format(id_with_discrete_missing_tstamp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4ffda3d5-311b-0ab4-9304-d32d7510a01f"
      },
      "outputs": [],
      "source": [
        "def barplot(res, width, index, column):\n",
        "    N = int(res.shape[0])\n",
        "    fig, ax = plt.subplots(figsize = (2, 1))\n",
        "    ind = np.arange(N)\n",
        "    rects = plt.bar(res['diff'], res['size'])\n",
        "    ax.set_title(column)\n",
        "    ax.set_xticks(ind + width / 2)\n",
        "    ax.set_xticklabels(res['diff'])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "29fe153d-83f3-7543-df39-d92679725baa"
      },
      "source": [
        "## Calculate statistics ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a4670c12-7ce5-bef4-385e-8161dfc0815b"
      },
      "outputs": [],
      "source": [
        "def calculate_statistics(data, columns, identifier, excludes):\n",
        "    rows = {}\n",
        "    rows[identifier] = []\n",
        "\n",
        "    min = {}\n",
        "    max = {}\n",
        "    sum = {}\n",
        "    mean = {}\n",
        "    median = {}\n",
        "    std = {}\n",
        "    total = {}\n",
        "    null_counts = {}\n",
        "    non_null_counts = {}\n",
        "    \n",
        "    for column in columns:\n",
        "        if column in excludes:\n",
        "            continue\n",
        "        min[column + '-' + 'min'] = []\n",
        "        max[column + '-' + 'max'] = []\n",
        "        sum[column + '-' + 'sum'] = []\n",
        "        mean[column + '-' + 'mean'] = []\n",
        "        median[column + '-' + 'median'] = []\n",
        "        std[column + '-' + 'std'] = []\n",
        "        total[column + '-' + 'total'] = []\n",
        "        non_null_counts[column+ '-' + 'non_null'] = []\n",
        "\n",
        "    for id, group in train.groupby(identifier):\n",
        "        rows[identifier].append(id)\n",
        "        \n",
        "        for column in columns:\n",
        "            if column in excludes:\n",
        "                continue\n",
        "            min[column + '-' + 'min'].append(group[column].dropna().min())\n",
        "            max[column + '-' + 'max'].append(group[column].max())\n",
        "            sum[column + '-' + 'sum'].append(group[column].sum())\n",
        "            mean[column + '-' + 'mean'].append(group[column].mean())\n",
        "            median[column + '-' + 'median'].append(group[column].median())\n",
        "            std[column + '-' + 'std'].append(group[column].std())\n",
        "            total[column + '-' + 'total'].append(group[column].shape[0])\n",
        "            non_null_counts[column+ '-' + 'non_null'].append(pd.notnull(group[column]).sum())\n",
        "\n",
        "    records = {} \n",
        "    records['id'] = rows['id']\n",
        "    for feature in [min, max, mean, median, sum, std, total, non_null_counts]:\n",
        "        for key, values in feature.items():\n",
        "            records[key] = values\n",
        "            \n",
        "    stats_df = pd.DataFrame(records)        \n",
        "    stats_df.columns = pd.MultiIndex.from_tuples([tuple(c.split('-')) for c in stats_df.columns])\n",
        "    stats_df = stats_df.set_index('id')\n",
        "    stats_df = stats_df.stack(0).reset_index(1)\n",
        "    return stats_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9148e1eb-2ee3-61bb-0758-ce6df3d0c121"
      },
      "outputs": [],
      "source": [
        "stats_df_der = calculate_statistics(train, derived_columns , 'id', ['id, timestamp', 'y'])\n",
        "stats_df_der.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "555b8a6a-b374-6fb8-9dde-6bceee55f0b6"
      },
      "outputs": [],
      "source": [
        "stats_df_fund = calculate_statistics(train, fundamental_columns , 'id', ['id, timestamp', 'y'])\n",
        "stats_df_fund.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e1a9844a-57cf-423c-b062-da4096c3cad1"
      },
      "outputs": [],
      "source": [
        "stats_df_tech = calculate_statistics(train, technical_columns , 'id', ['id, timestamp', 'y'])\n",
        "stats_df_tech.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0f33ca64-f0b2-a88d-dc41-3d79fbf1dac4"
      },
      "source": [
        "**Find the total percentage of records present in each column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "886723d4-0ed4-298b-a65c-ce4122f042a1"
      },
      "outputs": [],
      "source": [
        "def get_total_count_of_occurring_records(column_name, column_values, data, order_dict):\n",
        "    \n",
        "    header_dict = {}\n",
        "    header_dict[column_name] = []\n",
        "    header_dict['total_present'] = []\n",
        "    header_dict['total'] = []\n",
        "    \n",
        "    for value in column_values:\n",
        "        df = data.loc[data[column_name] == value]\n",
        "        header_dict[column_name].append(value)\n",
        "        non_nulls = len(list(df.loc[df['non_null'] > 0].index))\n",
        "        header_dict['total_present'].append(non_nulls)\n",
        "        header_dict['total'].append(len(df.index))\n",
        "    \n",
        "    result = pd.DataFrame(header_dict)\n",
        "    ordered_column = next(iter(order_dict.keys()))\n",
        "    order = next(iter(order_dict.values()))\n",
        "    result = result.sort_values(by = ordered_column, ascending = order)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "04e3e919-61db-0fe5-d7f4-3df2b970dc57"
      },
      "source": [
        "**Execute the above function for derived columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e9876225-74c9-0b5e-1d5b-8c13ae154609"
      },
      "outputs": [],
      "source": [
        "percentage_df = get_total_count_of_occurring_records('level_1', derived_columns, stats_df_der, {'total_present' : False})\n",
        "percentage_df['percentage'] = (percentage_df['total_present'] / percentage_df['total']) * 100\n",
        "percentage_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f6a7bcd2-10b3-aeba-be8b-4f6f14b72173"
      },
      "outputs": [],
      "source": [
        "percentage_df = get_total_count_of_occurring_records('level_1', fundamental_columns, stats_df_fund, {'total_present' : False})\n",
        "percentage_df['percentage'] = (percentage_df['total_present'] / percentage_df['total']) * 100\n",
        "percentage_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "97fb1b90-f70f-6e96-2c9e-3d8298491df2"
      },
      "outputs": [],
      "source": [
        "percentage_df = get_total_count_of_occurring_records('level_1', technical_columns, stats_df_tech, {'total_present' : False})\n",
        "percentage_df['percentage'] = (percentage_df['total_present'] / percentage_df['total']) * 100\n",
        "percentage_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b4e8c8c6-7375-425c-6d34-5ad0bae93c6f"
      },
      "source": [
        "## Calculate the assets with missing values ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e698454b-c739-363f-4ad9-8a6bdb21e26e"
      },
      "outputs": [],
      "source": [
        "def get_assets_with_missing(columns, stats_df):\n",
        "    result = {}\n",
        "    for column in columns:\n",
        "        missing_ids_df = stats_df.loc[stats_df['level_1'] == column]\n",
        "        missing_ids_df = missing_ids_df.loc[missing_ids_df['non_null'] == 0]\n",
        "        result[column] = list(missing_ids_df.index)\n",
        "       \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79a8f241-155e-a0ac-8ca6-d4a02f3afa45"
      },
      "outputs": [],
      "source": [
        "der_missing_assets = get_assets_with_missing(derived_columns, stats_df_der)\n",
        "fun_missing_assets = get_assets_with_missing(fundamental_columns, stats_df_fund)\n",
        "tech_missing_assets = get_assets_with_missing(technical_columns, stats_df_tech)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5b0d6642-dfd8-c283-8923-db89d0b597ee"
      },
      "source": [
        "**Calculate maximum and minimum asset and its value from the statistics dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a33fe050-bcf5-a2a3-6f49-32e6778b0ca0"
      },
      "outputs": [],
      "source": [
        "def calculate_maxmin_id_and_value_from_stats(dataframe, field_name, field_values, feature_values):\n",
        "    \n",
        "    data_copy = dataframe.copy(deep = True)\n",
        "    \n",
        "    headers = list(itertools.product(field_values, feature_values))\n",
        "    headers = [\":\".join(item) for item in headers]\n",
        "    headers = list(itertools.product(headers, ['high', 'low', 'highid', 'lowid']))\n",
        "    headers = [\"-\".join(item) for item in headers]\n",
        "    \n",
        "    columns_dict = {}\n",
        "    for item in headers:\n",
        "        columns_dict[item] = []\n",
        "    \n",
        "    for key in columns_dict:\n",
        "        stats_column = key.split('-')[0]\n",
        "        feature_to_find = key.split('-')[1]\n",
        "        original_column = stats_column.split(':')[0]\n",
        "        stats_feature = stats_column.split(':')[1]\n",
        "        \n",
        "        temp = data_copy.loc[data_copy[field_name] == original_column]\n",
        "        temp = temp.sort_values(by=stats_feature, ascending = False)\n",
        "        temp = temp[[stats_feature]]\n",
        "        temp = temp.reset_index()\n",
        "        \n",
        "        if feature_to_find == 'high':\n",
        "            columns_dict[key].append(temp.head(1)[stats_feature][0])\n",
        "            \n",
        "        if feature_to_find == 'highid':\n",
        "            columns_dict[key].append(temp.head(1)['id'][0])\n",
        "            \n",
        "        if feature_to_find == 'low':\n",
        "            columns_dict[key].append(temp.iloc[-1][stats_feature])\n",
        "            \n",
        "        if feature_to_find == 'lowid':\n",
        "            columns_dict[key].append(temp.iloc[-1]['id'])\n",
        "            \n",
        "    result = pd.DataFrame(columns_dict)        \n",
        "    result.columns = pd.MultiIndex.from_tuples([tuple(c.split('-')) for c in result.columns])\n",
        "    result = result.stack(0).reset_index(1)\n",
        "    \n",
        "    #data_copy = data_copy.sort_values(by=max_column, ascending = False)\n",
        "    #data_copy = data_copy[[max_column]]\n",
        "    #data_copy = data_copy.reset_index()\n",
        "    #return data_copy\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c66df0b8-bc74-6057-062d-2977499e4022"
      },
      "outputs": [],
      "source": [
        "columns = list(stats_df_der.columns)\n",
        "columns.remove('level_1')\n",
        "result = calculate_maxmin_id_and_value_from_stats(stats_df_der, 'level_1', derived_columns, columns)\n",
        "result.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "63de5ed4-e35e-d023-2034-a3ba77bcb70e"
      },
      "outputs": [],
      "source": [
        "IDS_MAX = list(result['highid'].unique())\n",
        "IDS_MIN = list(result['lowid'].unique())\n",
        "print(IDS_MAX[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1e860f88-57e7-bb9d-d8f9-f3abb9d221c2"
      },
      "source": [
        "## Understand the distribution of the target variable  ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ec6ff7ed-4564-0615-b777-dc2d7366eb43"
      },
      "outputs": [],
      "source": [
        "grouped = train.groupby(ID)\n",
        "IDS = train[ID]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ea37b3f0-28a6-0605-7b14-79a96bb2085d"
      },
      "outputs": [],
      "source": [
        "def get_random_id():\n",
        "    return random.randint(0, len(IDS) - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "623ba5fa-f09c-d981-a4fd-d19112ac439b"
      },
      "outputs": [],
      "source": [
        "sns.distplot(train['y'], kde = False, bins = 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "92e27b8a-cda7-fc5d-ef51-7559c67b84ed"
      },
      "source": [
        "**We can infer from the above plot that the target is normally distributed with ruggedness.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "723ca361-5940-0961-962b-7dab1a8a9a96"
      },
      "outputs": [],
      "source": [
        "first_asset_df =  grouped.get_group(IDS[0])\n",
        "first_asset_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3ac2fa5c-734b-aad4-e832-9a496b32bf0e"
      },
      "outputs": [],
      "source": [
        "sns.distplot(first_asset_df['y'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a98189c1-5ceb-72cf-4f76-1d3d1771094b"
      },
      "outputs": [],
      "source": [
        "random_df = grouped.get_group(1749)\n",
        "random_df.head(3)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}