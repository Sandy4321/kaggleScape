{"cells":[{"metadata":{"_cell_guid":"e573be87-de4a-4d7d-aa55-69cfbdb18aba","_uuid":"9400636250dce53e1808ffd826155eb2c3119334"},"cell_type":"markdown","source":"# Montecarlo Model Selection \n\nIn this notebook we apply a montecarlo based feature selection method to identify a reduced number of features that can be used as a good predictor for this credit card fraud dataset. Using a reduced number of features is of crucial importance when overfitting needs to be prevented. This models has been tested in some other datasets with similar results.  \n\nWe expand the set of explanatory features by computing the products of all features against the rest. When the product of two features has better sorting capabilities than both features individually we include the product in our set of candidate features, a minimum threshold is applied. \n\nAdditionally the original set of features and the new set resulting from multiplying pairs of feature are transformed by means a logistic distribution. We have observed that this transformation increases predictive capabilities when compared to the ubiquitous normal transformation. \n\nIn order to measure if a model has a good predictive power we define a modified Jaccard distance. As follows:\n                                                  \n$$Modified\\,Jaccard\\,Distance = 1 − \\dfrac{\\sum\\limits_{i}{\\min{ (target_{i},\\,model\\, probability_{i})}}}{\\sum\\limits_{i}{\\max{(target_{i},\\,model\\, probability_{i})}}}$$                               \nThe lower the distance the best the model predicts the target.\n\nIn each Montecarlo iteration a reduced set of features, say 5 to 8, is randomly selected, then we compute the Logistic Regression model that best predicts fraud with this features and, finally we compute the modified Jaccard distance from the prediction to the target. The process is repeated for a large number of iterations. Resulting models are sorted by distance. \n\nWe have tested modified Jaccard distance metric against most common metrics such as ROC, AUC, recall… and found that models with the low values of this modified Jaccard distance have a better balanced results in the rest of metrics. \n\nFinal model selection is done by choosing the model with me minimum modified Jaccard distance, or any other among those with minimum distance that best fits the test subsample. "},{"metadata":{"_cell_guid":"8b5ac396-ae4c-44ce-a716-8ffd7a38fab4","_uuid":"a23e7253d44e901e6cc524ad8ed814793b2c5333"},"cell_type":"markdown","source":"### Libraries\n\nWe will use [scikit-learn](http://scikit-learn.org/stable/) for machine learning in general. All additional functions needed can be found as a dataset named [MonteCarloModelSelection_Functions.py](https://www.kaggle.com/forzzeeteam/monte-carlo-model-selection/data). "},{"metadata":{"_cell_guid":"0e3d0d90-2f12-48f3-bc49-b1ef08088fe9","_uuid":"dee7d276e135c69170fbe7eb3f7bed01d60db8e2","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport csv\nfrom sklearn.model_selection import train_test_split\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport sys \n\nsys.path.append ('../input/montecarlomodelselection-functions/')\nfrom MonteCarloModelSelection_Functions import *      \n\n\n%matplotlib inline\n%autosave 0","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1fa8dd19-e8d1-45b8-8fe1-72baa3b3840f","_uuid":"b7ace5a12524de1a495960074119cdb537a8a7f3"},"cell_type":"markdown","source":"### Loading Dataset "},{"metadata":{"_cell_guid":"bf058cea-a3e5-4892-ab1e-f484b5a36dc6","_uuid":"26950291e7034aded930f62441a1dad2cad1c493","collapsed":true,"trusted":true},"cell_type":"code","source":"# Loading dataset creditcard\nfilename = '../input/creditcardfraud/creditcard.csv'   \n\nwith open(filename, 'r') as f:\n    reader=csv.reader(f, delimiter=',') \n    labels=next(reader)\n\n    raw_data=[]\n    for row in reader:\n        raw_data.append(row)\n\ndata = np.array(raw_data)\ndata = data.astype(np.float)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6f97be9d-d3a9-4075-91f6-d86a0265972c","_uuid":"a401b5513e567ad2c87eca2d9d10927ecc9f9972"},"cell_type":"markdown","source":"The Amount column is normalized. "},{"metadata":{"_cell_guid":"e8d8deb2-d7ff-48c8-b649-7a7e7940cf66","_uuid":"cb3ec2f56bad69f305c04bbc61fa2ed6260128b5","collapsed":true,"trusted":true},"cell_type":"code","source":"# Setting target and data\ntarget = data[:,-1]\ndataAmount   = data[:,29]\ndata   = data[:,1:29]\n\n# Normalising Amount column \ndataAmountNormalize = np.array((dataAmount-np.mean(dataAmount))/np.std(dataAmount))\ndata = np.c_[ data,dataAmountNormalize]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"31d2ad8f-507e-43c4-aa6f-6709fc5a233a","_uuid":"d63e036931187d3e36ccd2fd88de2c43a4bb38ce","collapsed":true,"trusted":true},"cell_type":"code","source":"# Output Path\npath = './output/'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8c1b3ec3-91d2-47b0-9ba9-c009f69adaca","_uuid":"1bea71c768706c1954b88dd85752371e8651219c"},"cell_type":"markdown","source":"### Transformation \nAll features are tranformed using Univariate Logistic Regression. Normal transformation can be applied too, however we observed better results for the logit transformation. "},{"metadata":{"_cell_guid":"f9367bee-d568-4af0-a11c-07c1a5e2cc43","_uuid":"d648234e2db87af58789f8843fcb5e5a529a6a91","collapsed":true,"trusted":true},"cell_type":"code","source":"# Calculating transformed dataset by means of logit or normal method\ntransformation = 'logit' \ntransformed_dataset = Transformation(data, target, transformation)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"885b78e8-5a19-43bf-bf15-83b4336e75d2","_uuid":"4e779c10ab803a5b74c0298a8cbd739fac9b3165"},"cell_type":"markdown","source":"Calculate some metrics, initially we will pay special attention to the sorting capabilities of the different features by using different metrics.\n\n"},{"metadata":{"_cell_guid":"53414edb-b83e-43b5-99a4-094ba5a8f8aa","_uuid":"71ad023e0531c6858efaad46071182ee7736a556","collapsed":true,"trusted":true},"cell_type":"code","source":"# Calculating all metric\nmetric ='all'\nglobal_pi = Calculate_Metrics(transformed_dataset, target, metric, path, transformation)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9cca5f4b-5dfc-4e37-aff9-c4b984c0aaac","_uuid":"cd189bcc527894c2cfe952bac222ef4315323b52"},"cell_type":"markdown","source":"A new dataset resulting from combinations of products of features can be found. Following we look for products of features that improve the sorting capabilities of the features. First we select products that result in a “modified Jaccard distance” lower than that of the features independently and at the same time the metric is lower than 0.6."},{"metadata":{"_cell_guid":"19cd70bb-f06e-458d-89b3-bb1d0bda588e","_uuid":"e2f3b70df2d7cec1e1e0f5bdb3313f52357b1ad7","collapsed":true,"trusted":true},"cell_type":"code","source":"# Calculating new datasets with combinations of products of features using distance metric\nthreshold = 0.6\ntransformation = 'logit'\nmetric = 'all'\nmetric_prod = 'distance'\nnew_dataset, new_dataset_df = Products_Analysis(data, transformed_dataset, target, global_pi, metric, metric_prod, transformation, path, threshold)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33d5567ce7f8093c37fa47f1cf8a3e45db78778f"},"cell_type":"markdown","source":"Since “distance” did not produce predictive products of features the try “roc”."},{"metadata":{"_cell_guid":"72ba3a5c-b3e1-474b-807b-5670076cee8c","_uuid":"07f2b27170f071baece7b43dcee69a0e84b5b2c8","collapsed":true,"trusted":true},"cell_type":"code","source":"# Calculating new datasets with combinations of products of features using roc metric\nthreshold = 0.6\ntransformation = 'logit'\nmetric = 'all'\nmetric_prod = 'roc'\nnew_dataset, new_dataset_df = Products_Analysis(data, transformed_dataset, target, global_pi, metric, metric_prod, transformation, path, threshold)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9aae31b0-cb18-4470-84d4-f084ca290148","_uuid":"2cab40fad4a6609727bd262ab2be27ab362a65a8","collapsed":true,"trusted":true},"cell_type":"code","source":"new_dataset_df.tail(20)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5e972031-bb5a-4d87-9ec3-b58f003a34d3","_uuid":"0e79c93eef0d1a1258acb1a89b77390941cdce04"},"cell_type":"markdown","source":"18 new combinations have been created: 0 ratio and 16 ratio, 1 ratio and 6 ratio, etc. "},{"metadata":{"_cell_guid":"80f25bca-29a1-46e9-99bf-45114920c6b6","_uuid":"de2e602cabb69c843d56d0c9bddb58b54221612c"},"cell_type":"markdown","source":"### Resampling and Setting up the Training and Testing Sets\nThe original dataset has 492 fraud and 284.315 no fraud observations. We split the dataset into train and test as in the following table. \n\n|         | DATASET           | TRAIN  | TEST  | \n| ------------- |:-------------:| -----:|-----:|\n| No Fraud      | 284315 | 199019 |85296 | \n| Fraud      | 492      |    345| 147 |\n| Total | 284807      |   199364 | 85443 |\n- Dataset"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c0ad591a5d16694f3427d926811e11d9849c84d1"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(new_dataset, target, test_size = 0.3, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"064a30db-6620-4e3d-9140-3ef05288aef3","_uuid":"306ef8979696ed422e616e60d7ab235df0d297b1"},"cell_type":"markdown","source":"We split the dataset in order to work with a balanced dataset. Equal number of Fraud/No Fraud observations.\n \n\n|         | DATASET           | TRAIN UNDERSAMPLED  | TEST UNDERSAMPLED | \n| ------------- |:-------------:| -----:|-----:|\n| No Fraud      | 284315 | 688 | 296 | \n| Fraud      | 492      |   343| 149 |\n| Total | 284807      |   345 | 147 |\n\n- Resampled dataset"},{"metadata":{"_cell_guid":"7cccc860-d76e-4fb5-8b4d-c045d43e062b","_uuid":"49596b7dd6207a9325dc1fa597468d4a5d15dd75","collapsed":true,"trusted":true},"cell_type":"code","source":"# Resampling dataset  \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \nnp.random.seed(10)\nnumber_records_fraud = target.sum().astype(int)\nnormal_indices = (target==0).nonzero()[0]\nfraud_indices = (target==1).nonzero()[0]\nrandom_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\nrandom_normal_indices = np.array(random_normal_indices)\nunder_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\nunder_sample_data = new_dataset[under_sample_indices,:]\nX_undersample = under_sample_data\ny_undersample = target[under_sample_indices]\nX_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample, y_undersample, test_size = 0.3, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"83f40f83-86c5-4085-b0e0-aee81553947b","_uuid":"e09f3a67d73a91e66f3f55626885669e6507a824"},"cell_type":"markdown","source":"### Montecarlo selection of features \n\nWe performed **Montecarlo simulation** with 10.000 iterations to randomly select 5 features. In each iteration a new model is calibrated and its permormance metrics are calculated."},{"metadata":{"_cell_guid":"5c8165fe-54d4-4f5d-80a3-583eef3a3be9","_uuid":"6042d4c41a838a717ec925cd9ecc5b393867a8d4","collapsed":true,"trusted":true},"cell_type":"code","source":"metric = 'Distance'\nnumber_iterations = 10000\nnumber_ini_ratio = 5\nnumber_final_ratio = 5\nresults= Multivariate_Best_Model(number_iterations, X_train_undersample, y_train_undersample, X_test_undersample, y_test_undersample, metric, path, number_ini_ratio, number_final_ratio)       ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c3162202-4f1c-41fe-a6b9-a8cd35842121","_uuid":"6322f19b53f17c07dc173feeef63f0641105266c"},"cell_type":"markdown","source":"Models resulting from the montecarlo process are stored in a DataFrame with the following columns:\n- *Models*: set of ratios randomly selected.\n- *Metrics*: 'Roc', 'Accuracy', 'Precision', 'Recall', 'F1', 'Auc' metrics calculated for each set of models.\n- *P_def*: model probability.\n- *Prediction*: model prediction.\n- *Score*: score as the argument of the logit funtion.\n- *Betas*: multipliers of each variable in the logit function. \n- *Distance*: Modified Jaccard Distance. \n\nThen models are sorted by the Modified Jaccard Distance\n"},{"metadata":{"_cell_guid":"9362c1a4-1d8d-45fd-9bc0-0e567221d819","_uuid":"f802de7d940ccbc726ac8d8cd0e5fda4986785c4","collapsed":true,"trusted":true},"cell_type":"code","source":"results.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b0644cea-54f6-47d7-8fe5-0293bc8f86f0","_uuid":"87f3cf76926f338e35ff1614f48a423c2653bd36"},"cell_type":"markdown","source":"### PLOTS AND RESULTS\n\n- Resampled dataset"},{"metadata":{"_cell_guid":"b180e0ee-e3b6-4993-968c-0e84e3cecbeb","_uuid":"3d026535ca1aa453d23cadd65bc5c12add2aa157","collapsed":true,"trusted":true},"cell_type":"code","source":"models_list = [i-1 for i in results['Models'][0]]\nbt = results['Betas'][0]\nind_best = models_list \nX_test_b = X_test_undersample[:,ind_best]\nX_test_b_1 = np.array([1]*X_test_b.shape[0])\nX_test_b_ = np.c_[X_test_b_1, X_test_b]\nxtest_bt = np.ravel(np.dot(X_test_b_,np.transpose(bt)))\n\n[tn_u, fp_u, fn_u, tp_u] = Graph(y_test_undersample, xtest_bt)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4fd110f9-b3e1-4f4a-9d47-7712370e7104","_uuid":"47d0552769c836f57aa3cd80e301833c75cac8a1"},"cell_type":"markdown","source":"- Dataset"},{"metadata":{"_cell_guid":"ed0cbddd-8210-40cc-ac78-e9d8bebc77ec","_uuid":"7db1062e8332ca5477730d9aaf96c65b569e78de","collapsed":true,"trusted":true},"cell_type":"code","source":"models_list = [i-1 for i in results['Models'][0]]\nbt = results['Betas'][0]\nind_best = models_list \nX_test_b = X_test[:,ind_best]\nX_test_b_1 = np.array([1]*X_test_b.shape[0])\nX_test_b_ = np.c_[X_test_b_1, X_test_b]\nxtest_bt = np.ravel(np.dot(X_test_b_,np.transpose(bt)))\n\n[tn, fp, fn, tp]  = Graph(y_test, xtest_bt)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5e029b2c-766c-40a3-add1-6807f9c737af","_uuid":"81feea111caaa903c6e24d7edbd7d8ed271ec15c"},"cell_type":"markdown","source":"### Conclusion \nThe parsimony principle tells us to choose the simplest explanation that fits the evidence. In this work we used a Montecarlo method to find a model that can explain the target variable, proving that by selecting the appropriate features a model as simple as a Logistic Regression with 5 variables produces predictions that are as good as those coming from more complex models."},{"metadata":{"_cell_guid":"077a7e1f-5232-45ec-bbf9-90d259812d78","_uuid":"c181a9da1df01794accfdca9362e61d5e978ecaf","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}