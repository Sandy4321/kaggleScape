{"cells":[{"metadata":{"_cell_guid":"e2875ace-b565-4b60-9e69-79ad6ca3566b","_uuid":"d2c165a9adf3c495c9ce377e21445bd654d8de92","collapsed":true},"cell_type":"markdown","source":"# TED-Talks topic models","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c454963c-2df6-4722-8f52-d481f0a3fac8","_uuid":"27982888f6d448d0ff4ae3470cabf91088b1a8b9"},"cell_type":"markdown","source":"![](http://greatperformersacademy.com/images/images/Articles_images/10-best-ted-talks-ever.jpg)","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c4083d03-2b19-4e95-bf16-bb9486f12d4a","_uuid":"4581de0b5caee13f78a4a3881155e34481df47f6"},"cell_type":"markdown","source":"In this notebook we will study text processing using TED transcripts, passing through feature extraction to topic modeling in order to (1) have a first meet with text processing techniques and (2) analyze briefly some TED-Talks patterns.\n\nIn the amazing TED-Talks dataset, we have two files, one (ted_main.csv) with meta information about the talks, as # of comment, rating, related TEDs and so on; the other file has the transcripts which we'll care about in this tutorial. Even so, we'll use the ted_main.csv file to evaluate our topic modeling implementation, because it has a columns of talks' tags, useful as our \"ground truth topics\".","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"3388da79-2238-47ff-9d86-45438ad3ea8d","_uuid":"5d90eeb14490a2f69e98d1a8835e3018cded02dd","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nfrom time import time","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3654edf4-32c6-456e-a998-60d508433f8b","_uuid":"a3c8331a832f874409c31f3704f8e7c8515e22f1"},"cell_type":"markdown","source":"### 0.1. Transcripts loading","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"e3891acc-b56a-4f07-8666-8c62227c2ae9","_uuid":"a842242197826ec99193ca599dd1667a6a4ff1e1","trusted":true},"cell_type":"code","source":"ted_main_df = pd.read_csv('../input/ted_main.csv', encoding='utf-8')\ntranscripts_df = pd.read_csv('../input/transcripts.csv', encoding='utf-8')\ntranscripts_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fe118ab9-476b-42df-ac84-2dd5864fb657","_uuid":"7c225d935047b08e3817df7ad590dfa6ec0417d1"},"cell_type":"markdown","source":"## 1. Text feature extraction with TFIDF\n\nFirst,  consider the term-frequency (TF) matrix above, that can be extracted from a list of documents and the universe of terms in such documents.\n\n|        | Document 1 | Document 2 | ... | Document N |\n|--------|------------|------------|-----|------------|\n| Term 1 | 3          | 0          | ... | 1          |\n| Term 2 | 0          | 1          | ... | 2          |\n| Term 3 | 2          | 2          | ... | 1          |\n| ...    | ...        | ...        | ... | ...        |\n| Term N | 1          | 0          | ... | 0          |\n\n\nThis is a huge matrix with all elements' frequency in all documents. Now consider de idf (inverse document frequency) as an operation to transform this frequency into word importance, calculated by:\n\n$$ tfidf_{i,j} = tf_{i,j}  \\times log(\\frac{N}{df_{i}}) $$\n\nWhere $i$ refers to term index and $j$ document index. $N$ is the total number of documents and $df_{i}$ is the number of documents containing $i$.\n\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"0664d2fd-070d-4712-95cc-3ce34b5a0d3c","_uuid":"eb83b0a6d190cd99b6376071e578dd11660fe333","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words=\"english\",\n                        use_idf=True,\n                        ngram_range=(1,1), # considering only 1-grams\n                        min_df = 0.05,     # cut words present in less than 5% of documents\n                        max_df = 0.3)      # cut words present in more than 30% of documents \nt0 = time()\n\ntfidf = vectorizer.fit_transform(transcripts_df['transcript'])\nprint(\"done in %0.3fs.\" % (time() - t0))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7fa1c608-f090-4cca-93c4-e791b5dfd639","_uuid":"2ff07e6661353a7042fc768086c597ca5ad1ab9d"},"cell_type":"markdown","source":"Keeping that in mind, we'll want to see the 'most important' words in our matrix...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"61b9d9ad-3624-41c1-b492-9d9855ded2fe","_uuid":"9a1fd8b2ceeccdd44d8631ae8d33eb2e2eb48d43","trusted":true},"cell_type":"code","source":"# Let's make a function to call the top ranked words in a vectorizer\ndef rank_words(terms, feature_matrix):\n    sums = feature_matrix.sum(axis=0)\n    data = []\n    for col, term in enumerate(terms):\n        data.append( (term, sums[0,col]) )\n    ranked = pd.DataFrame(data, columns=['term','rank']).sort_values('rank', ascending=False)\n    return ranked\n\nranked = rank_words(terms=vectorizer.get_feature_names(), feature_matrix=tfidf)\nranked.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"918b522b-c239-4c5d-9a78-321b197885ed","_uuid":"d11a36b552bb49ec9fb01252b927ca28e6084e6c","trusted":true},"cell_type":"code","source":"# Let's visualize a word cloud with the frequencies obtained by idf transformation\ndic = {ranked.loc[i,'term'].upper(): ranked.loc[i,'rank'] for i in range(0,len(ranked))}\n\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(background_color='white',\n                      max_words=100,\n                      colormap='Reds').generate_from_frequencies(dic)\nfig = plt.figure(1,figsize=(12,10))\nplt.imshow(wordcloud,interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"76d35ca7-2814-483f-bb00-986641b13477","_uuid":"9e419ac0904aa54c5a3a92c060af3ac79c4f5f16"},"cell_type":"markdown","source":"## 2. Topic modeling\n\nNot recently decomposition techniques have been used to extract topics from text data. A topic is a mixture of words and a document should be pertinent to a topic if these words are present in there. Look at the diagram to understand better.\n\n![alt text](https://image.ibb.co/kH9t87/d.png)\n\nHere we will extract topics from NMF and LDA to check for better results. First, let's try LDA.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"79ecdca2-0be0-4eb2-98b9-d6fcb3d12966","_uuid":"5daa58197360e432d7c1258749f3b0e0d3012047","trusted":true},"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation\n\nn_topics = 10\nlda = LatentDirichletAllocation(n_components=n_topics,random_state=0)\n\ntopics = lda.fit_transform(tfidf)\ntop_n_words = 5\nt_words, word_strengths = {}, {}\nfor t_id, t in enumerate(lda.components_):\n    t_words[t_id] = [vectorizer.get_feature_names()[i] for i in t.argsort()[:-top_n_words - 1:-1]]\n    word_strengths[t_id] = t[t.argsort()[:-top_n_words - 1:-1]]\nt_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da678522c5e6e82c68a2de33a4f933c47a47b916"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(7,15), ncols=2, nrows=5)\nplt.subplots_adjust(\n    wspace  =  0.5,\n    hspace  =  0.5\n)\nc=0\nfor row in range(0,5):\n    for col in range(0,2):\n        sns.barplot(x=word_strengths[c], y=t_words[c], color=\"red\", ax=ax[row][col])\n        c+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"319bbb3f-67aa-422b-be51-f628ab53f0c6","_uuid":"bc189d0fad17572daaabe9aa3c0c234b78985eb2"},"cell_type":"markdown","source":"Not so bad, but there are topics that has words fairly inappropriate. We could say that these topics are uncohesive for humans. That is common difficult when applying topic modeling to real problems. \n\nAnother problem is the optimal number of topics. There are several ways to validate it, as with perplexity or log-likelyhood. However, let's keep with 10 topics! :)","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"5a10e9a6-bd17-4afd-9994-9bdf4421c3ca","_uuid":"4f2a089fe5e83b29fe38f7c3085f97f9c22218e0","trusted":true},"cell_type":"code","source":"from sklearn.decomposition import NMF\n\nn_topics = 10\nnmf = NMF(n_components=n_topics,random_state=0)\n\ntopics = nmf.fit_transform(tfidf)\ntop_n_words = 5\nt_words, word_strengths = {}, {}\nfor t_id, t in enumerate(nmf.components_):\n    t_words[t_id] = [vectorizer.get_feature_names()[i] for i in t.argsort()[:-top_n_words - 1:-1]]\n    word_strengths[t_id] = t[t.argsort()[:-top_n_words - 1:-1]]\nt_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75ebf4318271a475f93df8d24c3e4d75d2c1a4c6"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(7,15), ncols=2, nrows=5)\nplt.subplots_adjust(\n    wspace  =  0.5,\n    hspace  =  0.5\n)\nc=0\nfor row in range(0,5):\n    for col in range(0,2):\n        sns.barplot(x=word_strengths[c], y=t_words[c], color=\"red\", ax=ax[row][col])\n        c+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b91f760e-a0d7-472f-b38f-e1651f0744b4","_uuid":"ad0f0c320dba07eb268e5560e339ac7075ab4972"},"cell_type":"markdown","source":"Hmm. Now you see that with NMF things get better. So, we'll use it testing for a document and see what topics are extracted.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"6ffba558-1f16-4316-b321-2419e34c08f5","_uuid":"eb2e12c8952bf9af31590cdae055e441f8a5f358","trusted":true},"cell_type":"code","source":"# Formulating a pipeline to insert a document and extract the topics pertinency\nfrom sklearn.pipeline import Pipeline\npipe = Pipeline([\n    ('tfidf', vectorizer),\n    ('nmf', nmf)\n])\n\ndocument_id = 4\nt = pipe.transform([transcripts_df['transcript'].iloc[document_id]]) \nprint('Topic distribution for document #{}: \\n'.format(document_id),t)\nprint('Relevant topics for document #{}: \\n'.format(document_id),np.where(t>0.01)[1])\nprint('\\nTranscript:\\n',transcripts_df['transcript'].iloc[document_id][:500],'...')\n\ntalk = ted_main_df[ted_main_df['url']==transcripts_df['url'].iloc[document_id]]\nprint('\\nTrue tags from ted_main.csv: \\n',talk['tags'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e3658c0e-8cc3-4c87-8b40-b16147b86f8a","_uuid":"80c9f76fe370f06eccbef7de30979d17fb7ffd3c"},"cell_type":"markdown","source":"Seems nice! The transcript #4 really talk about topic #5.\n\nNow we would do a exploratory analysis about our topics and extract descriptitve statistics and visualizations for the transcripts.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"00442e9f-7ab5-4996-b6fc-81c428f877ee","_uuid":"018ffa19dbb77e35434f731b804ced87a21cf941","collapsed":true,"trusted":true},"cell_type":"code","source":"t = pipe.transform(transcripts_df['transcript']) \nt = pd.DataFrame(t, columns=['#{}'.format(i) for i in range(0,10)])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2dab3ac0-6ec0-4191-af03-5084622d44b5","_uuid":"65ee970e7b3d67404b81564921a208792612f730","collapsed":true,"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nnew_t = pd.DataFrame({'value':t['#0'].values,'topic':['#0']*len(t)})\nfor tid in t.columns[1:]:\n    new_t = pd.concat([new_t, pd.DataFrame({'value':t[tid].values,'topic':[tid]*len(t)})])\n\nfig = plt.figure(1,figsize=(12,6))\nsns.violinplot(x=\"topic\", y=\"value\", data=new_t, palette='Reds')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b08e213f-8506-4de7-89fb-41a98d531d40","_uuid":"ecf0e6befb9b4e94ed97673fc613d276517f10cd"},"cell_type":"markdown","source":"Through analyzing this plot, I would say that in general the topic distribution behaves evenly, excepts for #0. Things to be concluded:\n\n1. Topic #0 (['god', 'book', 'stories', 'oh', 'art']) is very general per si in terms of words meaning and perhaps explain this result. \n2. Topic #2 is only about music and it has high incidences because of its specificity, in contrast to #0.\n3. Excluding #0 (open meaning issues), topics **#4 (about earth), #5 (about government), #7 (about data&information) and #9 (about education)** have higher quartiles, meaning that they are the most frequent topics that TED Talks carry on.\n\nI believe that it effectively summarizes what TEDs are about: **ideas that really matter and worth spreading**.\n\nHope that it could help NLP beginners!","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}