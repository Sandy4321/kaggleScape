{"nbformat_minor": 1, "cells": [{"source": ["import numpy as np\n", "np.random.seed(666)\n", "import pandas as pd\n", "from sklearn.cross_validation import train_test_split\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "execution_count": null, "metadata": {"_uuid": "e6909cc1da233cb8d1bbbd1ffb6657711295fd93", "_cell_guid": "1c28b23f-c164-48fa-8f18-439ad139f85d"}, "cell_type": "code", "outputs": []}, {"source": ["#LOAD DATA\n", "\n", "train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "sample = pd.read_csv(\"../input/sample_submission.csv\")\n", "sample.head(2)"], "execution_count": null, "metadata": {"_uuid": "a3ee22afefa5610745210a5954ecaf9581b5b3ef", "_cell_guid": "dfaeea53-a796-4d1f-8dc1-167794e1b5b1"}, "cell_type": "code", "outputs": []}, {"source": ["#CREATE TARGET VARIABLE\n", "train[\"EAP\"] = (train.author==\"EAP\")*1\n", "train[\"HPL\"] = (train.author==\"HPL\")*1\n", "train[\"MWS\"] = (train.author==\"MWS\")*1\n", "train.drop(\"author\", 1, inplace=True)\n", "target_vars = [\"EAP\", \"HPL\", \"MWS\"]\n", "train.head(2)"], "execution_count": null, "metadata": {"_uuid": "8acfe9382d6d9f03b5b39d63aebc33f23e524106", "_cell_guid": "4ab16324-31c2-4840-b3aa-03226e3f5ae9"}, "cell_type": "code", "outputs": []}, {"source": ["#STEMMING WORDS\n", "import nltk.stem as stm\n", "import re\n", "stemmer = stm.SnowballStemmer(\"english\")\n", "train[\"stem_text\"] = train.text.apply(lambda x: (\" \").join([stemmer.stem(z) for z in re.sub(\"[^a-zA-Z0-9]\",\" \", x).split(\" \")]))\n", "test[\"stem_text\"] = test.text.apply(lambda x: (\" \").join([stemmer.stem(z) for z in re.sub(\"[^a-zA-Z0-9]\",\" \", x).split(\" \")]))\n", "train.head(3)\n"], "execution_count": null, "metadata": {"_uuid": "70425ebf88489c8616ec264b127696a4aed86e6f", "_cell_guid": "60034c26-8f87-4d20-9876-0319a3209588"}, "cell_type": "code", "outputs": []}, {"source": ["#PROCESS TEXT: RAW\n", "from keras.preprocessing.text import Tokenizer\n", "tok_raw = Tokenizer()\n", "tok_raw.fit_on_texts(train.text.str.lower())\n", "tok_stem = Tokenizer()\n", "tok_stem.fit_on_texts(train.stem_text)\n", "train[\"seq_text_stem\"] = tok_stem.texts_to_sequences(train.stem_text)\n", "test[\"seq_text_stem\"] = tok_stem.texts_to_sequences(test.stem_text)\n", "train.head(3)"], "execution_count": null, "metadata": {"_uuid": "bd8004c5014e64c8f4b2d17ed9a8bbdfc3ed9fb1", "_cell_guid": "059028d2-bbfa-4b76-ba11-69c4a5375d81"}, "cell_type": "code", "outputs": []}, {"source": ["#EXTRACT DATA FOR KERAS MODEL\n", "from keras.preprocessing.sequence import pad_sequences\n", "def get_keras_data(dataset, maxlen=20):\n", "    X = {\n", "        \"stem_input\": pad_sequences(dataset.seq_text_stem, maxlen=maxlen)\n", "    }\n", "    return X\n", "\n", "\n", "maxlen = 60\n", "dtrain, dvalid = train_test_split(train, random_state=123, train_size=0.85)\n", "X_train = get_keras_data(dtrain, maxlen)\n", "y_train = np.array(dtrain[target_vars])\n", "X_valid = get_keras_data(dvalid, maxlen)\n", "y_valid = np.array(dvalid[target_vars])\n", "X_test = get_keras_data(test, maxlen)\n", "\n", "n_stem_seq = np.max( [np.max(X_valid[\"stem_input\"]), np.max(X_train[\"stem_input\"])])+1"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "ec8a3b2b723389190443fdf6cbf67bb7d9a038e7", "_cell_guid": "9d27806b-22bd-47ce-b49b-4981c897ad63"}, "cell_type": "code", "outputs": []}, {"source": ["import keras.backend as K\n", "import tensorflow as tf\n", "from keras import initializers, layers\n", "\n", "class CapsuleLayer(Layer):\n", "    \"\"\"\n", "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n", "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n", "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\\n", "    [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1.\n", "    \n", "    :param num_capsule: number of capsules in this layer\n", "    :param dim_vector: dimension of the output vectors of the capsules in this layer\n", "    :param num_routings: number of iterations for the routing algorithm\n", "    \"\"\"\n", "    def __init__(self, num_capsule, dim_vector, num_routing=3,\n", "                 kernel_initializer='glorot_uniform',\n", "                 bias_initializer='zeros',\n", "                 **kwargs):\n", "        super(CapsuleLayer, self).__init__(**kwargs)\n", "        self.num_capsule = num_capsule\n", "        self.dim_vector = dim_vector\n", "        self.num_routing = num_routing\n", "        self.kernel_initializer = initializers.get(kernel_initializer)\n", "        self.bias_initializer = initializers.get(bias_initializer)\n", "\n", "    def build(self, input_shape):\n", "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]\"\n", "        self.input_num_capsule = input_shape[1]\n", "        self.input_dim_vector = input_shape[2]\n", "\n", "        # Transform matrix\n", "        self.W = self.add_weight(shape=[self.input_num_capsule, self.num_capsule, self.input_dim_vector, self.dim_vector],\n", "                                 initializer=self.kernel_initializer,\n", "                                 name='W')\n", "\n", "        # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation.\n", "        self.bias = self.add_weight(shape=[1, self.input_num_capsule, self.num_capsule, 1, 1],\n", "                                    initializer=self.bias_initializer,\n", "                                    name='bias',\n", "                                    trainable=False)\n", "        self.built = True\n", "\n", "    def call(self, inputs, training=None):\n", "        # inputs.shape=[None, input_num_capsule, input_dim_vector]\n", "        # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector]\n", "        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n", "\n", "        # Replicate num_capsule dimension to prepare being multiplied by W\n", "        # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector]\n", "        inputs_tiled = K.tile(inputs_expand, [1, 1, self.num_capsule, 1, 1])\n", "\n", "        \"\"\"  \n", "        # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size.\n", "        # Now W has shape  = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector]\n", "        w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1])\n", "        \n", "        # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n", "        inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3])\n", "        \"\"\"\n", "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow.\n", "        # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n", "        inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]),\n", "                             elems=inputs_tiled,\n", "                             initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n", "        \"\"\"\n", "        # Routing algorithm V1. Use tf.while_loop in a dynamic way.\n", "        def body(i, b, outputs):\n", "            c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n", "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n", "            b = b + K.sum(inputs_hat * outputs, -1, keepdims=True)\n", "            return [i-1, b, outputs]\n", "        cond = lambda i, b, inputs_hat: i > 0\n", "        loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)]\n", "        _, _, outputs = tf.while_loop(cond, body, loop_vars)\n", "        \"\"\"\n", "        # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance\n", "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n", "        for i in range(self.num_routing):\n", "            c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n", "            # outputs.shape=[None, 1, num_capsule, 1, dim_vector]\n", "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n", "\n", "            # last iteration needs not compute bias which will not be passed to the graph any more anyway.\n", "            if i != self.num_routing - 1:\n", "                # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True))\n", "                self.bias += K.sum(inputs_hat * outputs, -1, keepdims=True)\n", "            # tf.summary.histogram('BigBee', self.bias)  # for debugging\n", "        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])\n", "    \n", "class Mask(layers.Layer):\n", "    \"\"\"\n", "    Mask a Tensor with shape=[None, d1, d2] by the max value in axis=1.\n", "    Output shape: [None, d2]\n", "    \"\"\"\n", "    def call(self, inputs, **kwargs):\n", "        # use true label to select target capsule, shape=[batch_size, num_capsule]\n", "        if type(inputs) is list:  # true label is provided with shape = [batch_size, n_classes], i.e. one-hot code.\n", "            assert len(inputs) == 2\n", "            inputs, mask = inputs\n", "        else:  # if no true label, mask by the max length of vectors of capsules\n", "            x = inputs\n", "            # Enlarge the range of values in x to make max(new_x)=1 and others < 0\n", "            x = (x - K.max(x, 1, True)) / K.epsilon() + 1\n", "            mask = K.clip(x, 0, 1)  # the max value in x clipped to 1 and other to 0\n", "\n", "        # masked inputs, shape = [batch_size, dim_vector]\n", "        inputs_masked = K.batch_dot(inputs, mask, [1, 1])\n", "        return inputs_masked\n", "\n", "    def compute_output_shape(self, input_shape):\n", "        if type(input_shape[0]) is tuple:  # true label provided\n", "            return tuple([None, input_shape[0][-1]])\n", "        else:\n", "            return tuple([None, input_shape[-1]])"], "execution_count": null, "metadata": {}, "cell_type": "code", "outputs": []}, {"source": ["#KERAS MODEL DEFINITION\n", "from keras.layers import Dense, Dropout, Embedding\n", "from keras.layers import Flatten, Input, SpatialDropout1D, Reshape\n", "from keras.models import Model\n", "from keras.optimizers import Adam \n", "\n", "def get_model():\n", "    embed_dim = 50\n", "    dropout_rate = 0.9\n", "    emb_dropout_rate = 0.9\n", "   \n", "    input_text = Input(shape=[maxlen], name=\"stem_input\")\n", "    \n", "    emb_lstm = SpatialDropout1D(emb_dropout_rate) (Embedding(n_stem_seq, embed_dim\n", "                                                ,input_length = maxlen\n", "                                                               ) (input_text))\n", "    dense = Dropout(dropout_rate) (Dense(1024) (Flatten() (emb_lstm)))\n", "    dense = Reshape((128, 8)) (dense)\n", "    dense = Flatten() (Mask()(CapsuleLayer(128, 8))(dense))\n", "    \n", "    output = Dense(3, activation=\"softmax\")(dense)\n", "\n", "    model = Model([input_text], output)\n", "\n", "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n", "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n", "    return model\n", "\n", "model = get_model()\n", "model.summary()\n", "    "], "execution_count": null, "metadata": {"_uuid": "93410aec5ce40adf620a8a523fea2905603c53b2", "_cell_guid": "6c5b7b62-a184-41fb-83a5-fd1616a81665"}, "cell_type": "code", "outputs": []}, {"source": ["#TRAIN KERAS MODEL\n", "model = get_model()\n", "model.fit(X_train, y_train, epochs=27\n", "          , validation_data=[X_valid, y_valid]\n", "         , batch_size=1024)"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "0ac4bd8abbff316692538ee6eca702c34c946949", "_cell_guid": "03fe721e-3ba8-4873-9f6a-0d2516f84abe"}, "cell_type": "code", "outputs": []}, {"source": ["#MODEL EVALUATION\n", "from sklearn.metrics import log_loss\n", "\n", "preds_train = model.predict(X_train)\n", "preds_valid = model.predict(X_valid)\n", "\n", "print(log_loss(y_train, preds_train))\n", "print(log_loss(y_valid, preds_valid))"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "5aaba806022361ebdae3565f300637587ffa94d1", "_cell_guid": "984c13bd-9e37-4d5b-9b30-08ed8356b53c"}, "cell_type": "code", "outputs": []}, {"source": ["#PREDICTION\n", "preds = pd.DataFrame(model.predict(X_test), columns=target_vars)\n", "submission = pd.concat([test[\"id\"],preds], 1)\n", "submission.to_csv(\"./submission.csv\", index=False)\n", "submission.head()"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "88664b3d84bf80f8870258a52c39da9ae338adbb", "_cell_guid": "d32a8f06-2d0c-4a76-97c3-a799eee45f86"}, "cell_type": "code", "outputs": []}, {"source": [], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "ca2423fdc3ad79bbfd6ea8d3e91a9969dfd26bff", "_cell_guid": "ca042ea9-a05d-40bb-876d-37630b76fbe1"}, "cell_type": "code", "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "mimetype": "text/x-python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.3", "name": "python"}}, "nbformat": 4}