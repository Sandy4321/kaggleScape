{"cells":[{"metadata":{"_uuid":"c20c67f2226b24051e2d64a63555059d96969829","collapsed":true,"_cell_guid":"37070448-9c94-480d-8ad4-55d84b7349b0","trusted":false},"cell_type":"code","source":"#changes:\n#optimizing Ridge and applying lightgbm\n#using thread and gc to optimize memory\n#reference from great notebook of Peter\nimport time\nstart_time = time.time()\n\nSUBMIT_MODE = True\n\n\nimport pandas as pd\nimport numpy as np\nimport time\nimport gc\nimport string\nimport re\n\nfrom nltk.corpus import stopwords\n\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection.univariate_selection import SelectKBest, f_regression\nfrom sklearn.preprocessing import LabelBinarizer\n\nimport wordbatch\nfrom wordbatch.extractors import WordBag\nfrom wordbatch.models import FM_FTRL\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.naive_bayes import MultinomialNB\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41233bd862bb32f02ecb44fcd50dd6aeb97cf457","collapsed":true,"_cell_guid":"dd34009a-f007-487c-9acf-f653dbf05083","trusted":false},"cell_type":"code","source":"def rmse(predicted, actual):\n    return np.sqrt(((predicted - actual) ** 2).mean())\ndef split_cat(text):\n    try:\n        return text.split(\"/\")\n    except:\n        return (\"No Label\", \"No Label\", \"No Label\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f75efe121b9cad2b497563b82344628cba3dc17","collapsed":true,"_cell_guid":"123ba717-28ce-49b1-84d0-3ce55a7ca282","trusted":false},"cell_type":"code","source":"class TargetEncoder:\n    # Adapted from https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\n    def __repr__(self):\n        return 'TargetEncoder'\n\n    def __init__(self, cols, smoothing=1, min_samples_leaf=1, noise_level=0, keep_original=False):\n        self.cols = cols\n        self.smoothing = smoothing\n        self.min_samples_leaf = min_samples_leaf\n        self.noise_level = noise_level\n        self.keep_original = keep_original\n\n    @staticmethod\n    def add_noise(series, noise_level):\n        return series * (1 + noise_level * np.random.randn(len(series)))\n\n    def encode(self, train, test, target):\n        for col in self.cols:\n            if self.keep_original:\n                train[col + '_te'], test[col + '_te'] = self.encode_column(train[col], test[col], target)\n            else:\n                train[col], test[col] = self.encode_column(train[col], test[col], target)\n        return train, test\n\n    def encode_column(self, trn_series, tst_series, target):\n        temp = pd.concat([trn_series, target], axis=1)\n        # Compute target mean\n        averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n        # Compute smoothing\n        smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - self.min_samples_leaf) / self.smoothing))\n        # Apply average function to all target data\n        prior = target.mean()\n        # The bigger the count the less full_avg is taken into account\n        averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n        averages.drop(['mean', 'count'], axis=1, inplace=True)\n        # Apply averages to trn and tst series\n        ft_trn_series = pd.merge(\n            trn_series.to_frame(trn_series.name),\n            averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n            on=trn_series.name,\n            how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n        # pd.merge does not keep the index so restore it\n        ft_trn_series.index = trn_series.index\n        ft_tst_series = pd.merge(\n            tst_series.to_frame(tst_series.name),\n            averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n            on=tst_series.name,\n            how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n        # pd.merge does not keep the index so restore it\n        ft_tst_series.index = tst_series.index\n        return self.add_noise(ft_trn_series, self.noise_level), self.add_noise(ft_tst_series, self.noise_level)  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5945a746a7997c90056a17feaa3af3d79cd9eb4c","collapsed":true,"_cell_guid":"22985136-c61b-4f3c-a320-bac073878f77","trusted":false},"cell_type":"code","source":"def to_number(x):\n    try:\n        if not x.isdigit():\n            return 0\n        x = int(x)\n        if x > 100:\n            return 100\n        else:\n            return x\n    except:\n        return 0\n\ndef sum_numbers(desc):\n    if not isinstance(desc, str):\n        return 0\n    try:\n        return sum([to_number(s) for s in desc.split()])\n    except:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83e049e4bfccf0ba6ab3ca0486827ce27d07a4a5","collapsed":true,"_cell_guid":"7dcee798-a2ba-4f4a-944b-6a836d4ddc16","trusted":false},"cell_type":"code","source":"# Define helpers for text normalization\nstopwords = {x: 1 for x in stopwords.words('english')}\nnon_alphanums = re.compile(u'[^A-Za-z0-9]+')\nnon_alphanumpunct = re.compile(u'[^A-Za-z0-9\\.?!,; \\(\\)\\[\\]\\'\\\"\\$]+')\nRE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])\n\ndef normalize_text(text):\n    return u\" \".join(\n        [x for x in [y for y in non_alphanums.sub(' ', text).lower().strip().split(\" \")] \\\n         if len(x) > 1 and x not in stopwords])\n\ndef clean_name(x):\n    if len(x):\n        x = non_alphanums.sub(' ', x).split()\n        if len(x):\n            return x[0].lower()\n    return ''\n\n    \nprint('[{}] Finished defining stuff'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1d25c3f25809724dc55e485d2695f0da1492ec0","collapsed":true,"_cell_guid":"d7a6a94b-c2ff-4789-81a0-990ed36f00f6","trusted":false},"cell_type":"code","source":"train = pd.read_table('../input/train.tsv', engine='c', \n                      dtype={'item_condition_id': 'category',\n                             'shipping': 'category',\n                            }, \n                     converters={'category_name': split_cat})\ntest = pd.read_table('../input/test.tsv', engine='c', \n                      dtype={'item_condition_id': 'category',\n                             'shipping': 'category',\n                            },\n                    converters={'category_name': split_cat})\nprint('[{}] Finished load data'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c14fa36bc47651c57298a0303a0525faa5fa786","collapsed":true,"_cell_guid":"bca48fae-c3e8-4ad4-aea0-0dca47ceb585","trusted":false},"cell_type":"code","source":"train['is_train'] = 1\ntest['is_train'] = 0\nprint('[{}] Compiled train / test'.format(time.time() - start_time))\nprint('Train shape: ', train.shape)\nprint('Test shape: ', test.shape)\n\ntrain = train[train.price != 0].reset_index(drop=True)\nprint('[{}] Removed nonzero price'.format(time.time() - start_time))\nprint('Train shape: ', train.shape)\nprint('Test shape: ', test.shape)\n\ny = np.log1p(train['price'])\nnrow_train = train.shape[0]\n\nmerge = pd.concat([train, test])\nsubmission = test[['test_id']]\nprint('[{}] Compiled merge'.format(time.time() - start_time))\nprint('Merge shape: ', merge.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7136c1720c280f3cdc93722d4cb00b2d0bd66af6","collapsed":true,"_cell_guid":"9e51d42f-14ca-430a-96ee-949a69fbfe94","trusted":false},"cell_type":"code","source":"del train\ndel test\nmerge.drop(['train_id', 'test_id', 'price'], axis=1, inplace=True)\ngc.collect()\nprint('[{}] Garbage collection'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba80d123885d83a3c6f2ecbdeac9957154b46a37","collapsed":true,"_cell_guid":"b7337e85-85ec-408c-ae19-971610a1c060","trusted":false},"cell_type":"code","source":"merge['gencat_name'] = merge['category_name'].str.get(0).replace('', 'missing').astype('category')\nmerge['subcat1_name'] = merge['category_name'].str.get(1).fillna('missing').astype('category')\nmerge['subcat2_name'] = merge['category_name'].str.get(2).fillna('missing').astype('category')\nmerge.drop('category_name', axis=1, inplace=True)\nprint('[{}] Split categories completed.'.format(time.time() - start_time))\n\nmerge['item_condition_id'] = merge['item_condition_id'].cat.add_categories(['missing']).fillna('missing')\nmerge['shipping'] = merge['shipping'].cat.add_categories(['missing']).fillna('missing')\nmerge['item_description'].fillna('missing', inplace=True)\nmerge['brand_name'] = merge['brand_name'].fillna('missing').astype('category')\nprint('[{}] Handle missing completed.'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b25891ef65980adbcc9f431acaaa90fbac258551","collapsed":true,"_cell_guid":"471cafdf-dfa3-45b9-8fb6-88b514bee94e","trusted":false},"cell_type":"code","source":"merge['name_first'] = merge['name'].apply(clean_name)\nprint('[{}] FE 1/37'.format(time.time() - start_time))\nmerge['name_first_count'] = merge.groupby('name_first')['name_first'].transform('count')\nprint('[{}] FE 2/37'.format(time.time() - start_time))\nmerge['gencat_name_count'] = merge.groupby('gencat_name')['gencat_name'].transform('count')\nprint('[{}] FE 3/37'.format(time.time() - start_time))\nmerge['subcat1_name_count'] = merge.groupby('subcat1_name')['subcat1_name'].transform('count')\nprint('[{}] FE 4/37'.format(time.time() - start_time))\nmerge['subcat2_name_count'] = merge.groupby('subcat2_name')['subcat2_name'].transform('count')\nprint('[{}] FE 5/37'.format(time.time() - start_time))\nmerge['brand_name_count'] = merge.groupby('brand_name')['brand_name'].transform('count')\nprint('[{}] FE 6/37'.format(time.time() - start_time))\nmerge['NameLower'] = merge.name.str.count('[a-z]')\nprint('[{}] FE 7/37'.format(time.time() - start_time))\nmerge['DescriptionLower'] = merge.item_description.str.count('[a-z]')\nprint('[{}] FE 8/37'.format(time.time() - start_time))\nmerge['NameUpper'] = merge.name.str.count('[A-Z]')\nprint('[{}] FE 9/37'.format(time.time() - start_time))\nmerge['DescriptionUpper'] = merge.item_description.str.count('[A-Z]')\nprint('[{}] FE 10/37'.format(time.time() - start_time))\nmerge['name_len'] = merge['name'].apply(lambda x: len(x))\nprint('[{}] FE 11/37'.format(time.time() - start_time))\nmerge['des_len'] = merge['item_description'].apply(lambda x: len(x))\nprint('[{}] FE 12/37'.format(time.time() - start_time))\nmerge['name_desc_len_ratio'] = merge['name_len']/merge['des_len']\nprint('[{}] FE 13/37'.format(time.time() - start_time))\nmerge['desc_word_count'] = merge['item_description'].apply(lambda x: len(x.split()))\nprint('[{}] FE 14/37'.format(time.time() - start_time))\nmerge['mean_des'] = merge['item_description'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) / len(x)) * 10\nprint('[{}] FE 15/37'.format(time.time() - start_time))\nmerge['name_word_count'] = merge['name'].apply(lambda x: len(x.split()))\nprint('[{}] FE 16/37'.format(time.time() - start_time))\nmerge['mean_name'] = merge['name'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) / len(x))  * 10\nprint('[{}] FE 17/37'.format(time.time() - start_time))\nmerge['desc_letters_per_word'] = merge['des_len'] / merge['desc_word_count']\nprint('[{}] FE 18/37'.format(time.time() - start_time))\nmerge['name_letters_per_word'] = merge['name_len'] / merge['name_word_count']\nprint('[{}] FE 19/37'.format(time.time() - start_time))\nmerge['NameLowerRatio'] = merge['NameLower'] / merge['name_len']\nprint('[{}] FE 20/37'.format(time.time() - start_time))\nmerge['DescriptionLowerRatio'] = merge['DescriptionLower'] / merge['des_len']\nprint('[{}] FE 21/37'.format(time.time() - start_time))\nmerge['NameUpperRatio'] = merge['NameUpper'] / merge['name_len']\nprint('[{}] FE 22/37'.format(time.time() - start_time))\nmerge['DescriptionUpperRatio'] = merge['DescriptionUpper'] / merge['des_len']\nprint('[{}] FE 23/37'.format(time.time() - start_time))\nmerge['NamePunctCount'] = merge.name.str.count(RE_PUNCTUATION)\nprint('[{}] FE 24/37'.format(time.time() - start_time))\nmerge['DescriptionPunctCount'] = merge.item_description.str.count(RE_PUNCTUATION)\nprint('[{}] FE 25/37'.format(time.time() - start_time))\nmerge['NamePunctCountRatio'] = merge['NamePunctCount'] / merge['name_word_count']\nprint('[{}] FE 26/37'.format(time.time() - start_time))\nmerge['DescriptionPunctCountRatio'] = merge['DescriptionPunctCount'] / merge['desc_word_count']\nprint('[{}] FE 27/37'.format(time.time() - start_time))\nmerge['NameDigitCount'] = merge.name.str.count('[0-9]')\nprint('[{}] FE 28/37'.format(time.time() - start_time))\nmerge['DescriptionDigitCount'] = merge.item_description.str.count('[0-9]')\nprint('[{}] FE 29/37'.format(time.time() - start_time))\nmerge['NameDigitCountRatio'] = merge['NameDigitCount'] / merge['name_word_count']\nprint('[{}] FE 30/37'.format(time.time() - start_time))\nmerge['DescriptionDigitCountRatio'] = merge['DescriptionDigitCount']/merge['desc_word_count']\nprint('[{}] FE 31/37'.format(time.time() - start_time))\nmerge['stopword_ratio_desc'] = merge['item_description'].apply(lambda x: len([w for w in x.split() if w in stopwords])) / merge['desc_word_count']\nprint('[{}] FE 32/37'.format(time.time() - start_time))\nmerge['num_sum'] = merge['item_description'].apply(sum_numbers) \nprint('[{}] FE 33/37'.format(time.time() - start_time))\nmerge['weird_characters_desc'] = merge['item_description'].str.count(non_alphanumpunct)\nprint('[{}] FE 34/37'.format(time.time() - start_time))\nmerge['weird_characters_name'] = merge['name'].str.count(non_alphanumpunct)\nprint('[{}] FE 35/37'.format(time.time() - start_time))\nmerge['prices_count'] = merge['item_description'].str.count('[rm]')\nprint('[{}] FE 36/37'.format(time.time() - start_time))\nmerge['price_in_name'] = merge['item_description'].str.contains('[rm]', regex=False).astype('int')\nprint('[{}] FE 37/37'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2333785d7904b5e5737f65dceda78cca46db74d3","collapsed":true,"_cell_guid":"e4a5c8ca-418d-4103-872c-2b80072da6c0","trusted":false},"cell_type":"code","source":"cols = set(merge.columns.values)\nbasic_cols = {'name', 'item_condition_id', 'brand_name',\n  'shipping', 'item_description', 'gencat_name',\n  'subcat1_name', 'subcat2_name', 'name_first', 'is_train'}\n\ncols_to_normalize = cols - basic_cols - {'price_in_name'}\nother_cols = basic_cols | {'price_in_name'}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c160921896125404845b0db6907f1ba303788e2a","collapsed":true,"_cell_guid":"2098d226-b931-44b3-88c3-d896e2fd7d23","trusted":false},"cell_type":"code","source":"merge_to_normalize = merge[list(cols_to_normalize)]\nmerge_to_normalize = (merge_to_normalize - merge_to_normalize.mean()) / (merge_to_normalize.max() - merge_to_normalize.min())\nprint('[{}] FE Normalized'.format(time.time() - start_time))\n\nmerge = merge[list(other_cols)]\nmerge = pd.concat([merge, merge_to_normalize],axis=1)\nprint('[{}] FE Merged'.format(time.time() - start_time))\n\ndel(merge_to_normalize)\ngc.collect()\nprint('[{}] Garbage collection'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85cb2c78700346b6c5f996b86c2ff3b7d8127c69","collapsed":true,"_cell_guid":"42d71ff2-d807-4171-92aa-e4d03d04b357","trusted":false},"cell_type":"code","source":"df_test = merge.loc[merge['is_train'] == 0]\ndf_train = merge.loc[merge['is_train'] == 1]\ndel merge\ngc.collect()\ndf_test = df_test.drop(['is_train'], axis=1)\ndf_train = df_train.drop(['is_train'], axis=1)\n\nif SUBMIT_MODE:\n    y_train = y\n    del y\n    gc.collect()\nelse:\n    df_train, df_test, y_train, y_test = train_test_split(df_train, y, test_size=0.2, random_state=144)\n\nprint('[{}] Splitting completed.'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13f12690658d9c7ad9b28444d0c7abfaf0a24d61","collapsed":true,"_cell_guid":"61820d8d-18e4-45b7-8ece-7a020363c9e6","trusted":false},"cell_type":"code","source":"wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2,\n                                                              \"hash_ngrams_weights\": [1.5, 1.0],\n                                                              \"hash_size\": 2 ** 29,\n                                                              \"norm\": None,\n                                                              \"tf\": 'binary',\n                                                              \"idf\": None,\n                                                              }), procs=8)\nwb.dictionary_freeze = True\nX_name_train = wb.fit_transform(df_train['name'])\nX_name_test = wb.transform(df_test['name'])\ndel(wb)\nmask = np.where(X_name_train.getnnz(axis=0) > 3)[0]\nX_name_train = X_name_train[:, mask]\nX_name_test = X_name_test[:, mask]\nprint('[{}] Vectorize `name` completed.'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5f942ea7df04a9ad6a72a9d2ba4d014ed28cce1","collapsed":true,"_cell_guid":"43cdc11d-987c-43c3-8c39-3b53f805a925","trusted":false},"cell_type":"code","source":"wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2,\n                                                              \"hash_ngrams_weights\": [1.0, 1.0],\n                                                              \"hash_size\": 2 ** 28,\n                                                              \"norm\": \"l2\",\n                                                              \"tf\": 1.0,\n                                                              \"idf\": None}), procs=8)\nwb.dictionary_freeze = True\nX_description_train = wb.fit_transform(df_train['item_description'])\nX_description_test = wb.transform(df_test['item_description'])\ndel(wb)\nmask = np.where(X_description_train.getnnz(axis=0) > 3)[0]\nX_description_train = X_description_train[:, mask]\nX_description_test = X_description_test[:, mask]\nprint('[{}] Vectorize `item_description` completed.'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"307994e93d09ed18f42511346607aa1d93a702b4","collapsed":true,"_cell_guid":"48b569b1-6474-44a0-9785-e69747f0f940","trusted":false},"cell_type":"code","source":"X_train_1, X_train_2, y_train_1, y_train_2 = train_test_split(X_description_train, y_train,\n                                                              test_size = 0.5,\n                                                              shuffle = False)\nprint('[{}] Finished splitting'.format(time.time() - start_time))\n\n# Ridge adapted from https://www.kaggle.com/object/more-effective-ridge-script?scriptVersionId=1851819\nmodel = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha=3.3)\nmodel.fit(X_train_1, y_train_1)\nprint('[{}] Finished to train desc ridge (1)'.format(time.time() - start_time))\ndesc_ridge_preds1 = model.predict(X_train_2)\ndesc_ridge_preds1f = model.predict(X_description_test)\nprint('[{}] Finished to predict desc ridge (1)'.format(time.time() - start_time))\nmodel = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha=3.3)\nmodel.fit(X_train_2, y_train_2)\nprint('[{}] Finished to train desc ridge (2)'.format(time.time() - start_time))\ndesc_ridge_preds2 = model.predict(X_train_1)\ndesc_ridge_preds2f = model.predict(X_description_test)\nprint('[{}] Finished to predict desc ridge (2)'.format(time.time() - start_time))\ndesc_ridge_preds_oof = np.concatenate((desc_ridge_preds2, desc_ridge_preds1), axis=0)\ndesc_ridge_preds_test = (desc_ridge_preds1f + desc_ridge_preds2f) / 2.0\nprint('RMSLE OOF: {}'.format(rmse(desc_ridge_preds_oof, y_train)))\nif not SUBMIT_MODE:\n    print('RMSLE TEST: {}'.format(rmse(desc_ridge_preds_test, y_test)))\n\n\nX_train_1, X_train_2, y_train_1, y_train_2 = train_test_split(X_name_train, y_train,\n                                                              test_size = 0.5,\n                                                              shuffle = False)\nprint('[{}] Finished splitting'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81954961f2ad2292c07a2e4d91234c98984b65fb","collapsed":true,"_cell_guid":"3775c9ea-2378-497e-890c-72153125f25c","trusted":false},"cell_type":"code","source":"model = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha=3.3)\nmodel.fit(X_train_1, y_train_1)\nprint('[{}] Finished to train name ridge (1)'.format(time.time() - start_time))\nname_ridge_preds1 = model.predict(X_train_2)\nname_ridge_preds1f = model.predict(X_name_test)\nprint('[{}] Finished to predict name ridge (1)'.format(time.time() - start_time))\nmodel = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha=3.3)\nmodel.fit(X_train_2, y_train_2)\nprint('[{}] Finished to train name ridge (2)'.format(time.time() - start_time))\nname_ridge_preds2 = model.predict(X_train_1)\nname_ridge_preds2f = model.predict(X_name_test)\nprint('[{}] Finished to predict name ridge (2)'.format(time.time() - start_time))\nname_ridge_preds_oof = np.concatenate((name_ridge_preds2, name_ridge_preds1), axis=0)\nname_ridge_preds_test = (name_ridge_preds1f + name_ridge_preds2f) / 2.0\nprint('RMSLE OOF: {}'.format(rmse(name_ridge_preds_oof, y_train)))\nif not SUBMIT_MODE:\n    print('RMSLE TEST: {}'.format(rmse(name_ridge_preds_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62193396e947d68cf0f373d7bb4383fc1a09077b","collapsed":true,"_cell_guid":"748dc70c-4bd7-4d18-8542-d0a292d21712","trusted":false},"cell_type":"code","source":"del X_train_1\ndel X_train_2\ndel y_train_1\ndel y_train_2\ndel name_ridge_preds1\ndel name_ridge_preds1f\ndel name_ridge_preds2\ndel name_ridge_preds2f\ndel desc_ridge_preds1\ndel desc_ridge_preds1f\ndel desc_ridge_preds2\ndel desc_ridge_preds2f\ngc.collect()\nprint('[{}] Finished garbage collection'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cce7ca99b7975553b46d26c41962bdbdc71715fc","collapsed":true,"_cell_guid":"46d5e739-51b0-42f3-9852-ce99d8b933b4","trusted":false},"cell_type":"code","source":"lb = LabelBinarizer(sparse_output=True)\nX_brand_train = lb.fit_transform(df_train['brand_name'])\nX_brand_test = lb.transform(df_test['brand_name'])\nprint('[{}] Finished label binarize `brand_name`'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b456243373b248bf06c58b297aee327624e596d1","collapsed":true,"_cell_guid":"bcaca1b8-9e84-4297-a9cd-cd404db5154a","trusted":false},"cell_type":"code","source":"X_cat_train = lb.fit_transform(df_train['gencat_name'])\nX_cat_test = lb.transform(df_test['gencat_name'])\nX_cat1_train = lb.fit_transform(df_train['subcat1_name'])\nX_cat1_test = lb.transform(df_test['subcat1_name'])\nX_cat2_train = lb.fit_transform(df_train['subcat2_name'])\nX_cat2_test = lb.transform(df_test['subcat2_name'])\nprint('[{}] Finished label binarize categories'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86969ca51c692476c8ddae695bb44f3cc263126e","collapsed":true,"_cell_guid":"959fa758-b63a-4f14-bb25-09472ffe2dd1","trusted":false},"cell_type":"code","source":"X_dummies_train = csr_matrix(\n    pd.get_dummies(df_train[list(cols - (basic_cols - {'item_condition_id', 'shipping'}))],\n                   sparse=True).values)\nprint('[{}] Create dummies completed - train'.format(time.time() - start_time))\n\nX_dummies_test = csr_matrix(\n    pd.get_dummies(df_test[list(cols - (basic_cols - {'item_condition_id', 'shipping'}))],\n                   sparse=True).values)\nprint('[{}] Create dummies completed - test'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2da7925f4ec5d77a050c4027853e7a0fc9d81cf2","collapsed":true,"_cell_guid":"77c4e98f-7373-4adc-804d-71af2f15185f","trusted":false},"cell_type":"code","source":"sparse_merge_train = hstack((X_dummies_train, X_description_train, X_brand_train, X_cat_train,\n                             X_cat1_train, X_cat2_train, X_name_train)).tocsr()\ndel X_description_train, lb, X_name_train, X_dummies_train\ngc.collect()\nprint('[{}] Create sparse merge train completed'.format(time.time() - start_time))\n\nsparse_merge_test = hstack((X_dummies_test, X_description_test, X_brand_test, X_cat_test,\n                             X_cat1_test, X_cat2_test, X_name_test)).tocsr()\ndel X_description_test, X_name_test, X_dummies_test\ngc.collect()\nprint('[{}] Create sparse merge test completed'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bc6002d43eead54d1f1f9fa11b3881efb35a9de","collapsed":true,"_cell_guid":"8c7ea4b1-5213-4ad1-bce8-d613bbe582eb","trusted":false},"cell_type":"code","source":"if SUBMIT_MODE:\n    iters = 3\nelse:\n    iters = 1\n    rounds = 3\n\nmodel = FM_FTRL(alpha=0.035, beta=0.001, L1=0.00001, L2=0.15, D=sparse_merge_train.shape[1],\n                alpha_fm=0.05, L2_fm=0.0, init_fm=0.01,\n                D_fm=100, e_noise=0, iters=iters, inv_link=\"identity\", threads=4)\n\nif SUBMIT_MODE:\n    model.fit(sparse_merge_train, y_train)\n    print('[{}] Train FM completed'.format(time.time() - start_time))\n    predsFM = model.predict(sparse_merge_test)\n    print('[{}] Predict FM completed'.format(time.time() - start_time))\nelse:\n    for i in range(rounds):\n        model.fit(sparse_merge_train, y_train)\n        predsFM = model.predict(sparse_merge_test)\n        print('[{}] Iteration {}/{} -- RMSLE: {}'.format(time.time() - start_time, i + 1, rounds, rmse(predsFM, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fa864035e870f5931f6d6452289693da8bd3de9","collapsed":true,"_cell_guid":"9afaab63-b336-4c77-bf7f-33ce4802a340","trusted":false},"cell_type":"code","source":"del model\ngc.collect()\nif not SUBMIT_MODE:\n    print(\"FM_FTRL dev RMSLE:\", rmse(predsFM, y_test))\n\n\nfselect = SelectKBest(f_regression, k=48000)\ntrain_features = fselect.fit_transform(sparse_merge_train, y_train)\ntest_features = fselect.transform(sparse_merge_test)\nprint('[{}] Select best completed'.format(time.time() - start_time))\n\n\ndel sparse_merge_train\ndel sparse_merge_test\ngc.collect()\nprint('[{}] Garbage collection'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03ae1c3be88f72f5c0b0649a5aeafbd20986139c","collapsed":true,"_cell_guid":"48207c9c-f963-440c-aca7-21b14d81ceca","trusted":false},"cell_type":"code","source":"tv = TfidfVectorizer(max_features=250000,\n                     ngram_range=(1, 3),\n                     stop_words=None)\nX_name_train = tv.fit_transform(df_train['name'])\nprint('[{}] Finished TFIDF vectorize `name` (1/2)'.format(time.time() - start_time))\nX_name_test = tv.transform(df_test['name'])\nprint('[{}] Finished TFIDF vectorize `name` (2/2)'.format(time.time() - start_time))\n\ntv = TfidfVectorizer(max_features=500000,\n                     ngram_range=(1, 3),\n                     stop_words=None)\nX_description_train = tv.fit_transform(df_train['item_description'])\nprint('[{}] Finished TFIDF vectorize `item_description` (1/2)'.format(time.time() - start_time))\nX_description_test = tv.transform(df_test['item_description'])\nprint('[{}] Finished TFIDF vectorize `item_description` (2/2)'.format(time.time() - start_time))\n\nX_dummies_train = csr_matrix(\n    pd.get_dummies(df_train[['item_condition_id', 'shipping']], sparse=True).values)\nX_dummies_test = csr_matrix(\n    pd.get_dummies(df_test[['item_condition_id', 'shipping']], sparse=True).values)\n\nsparse_merge_train = hstack((X_description_train, X_brand_train, X_cat_train,\n                             X_cat1_train, X_cat2_train, X_name_train)).tocsr()\ndel X_dummies_train, X_description_train, X_brand_train, X_cat_train\ndel X_cat1_train, X_cat2_train, X_name_train\ngc.collect()\nprint('[{}] Create sparse merge train completed'.format(time.time() - start_time))\n\nsparse_merge_test = hstack((X_description_test, X_brand_test, X_cat_test,\n                            X_cat1_test, X_cat2_test, X_name_test)).tocsr()\ndel X_dummies_test, X_description_test, X_brand_test, X_cat_test\ndel X_cat1_test, X_cat2_test, X_name_test\ngc.collect()\nprint('[{}] Create sparse merge test completed'.format(time.time() - start_time))\n\n\nX_train_1, X_train_2, y_train_1, y_train_2 = train_test_split(sparse_merge_train, y_train,\n                                                              test_size = 0.5,\n                                                              shuffle = False)\nprint('[{}] Finished splitting'.format(time.time() - start_time))\n\n\nmodel = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha=3.3)\nmodel.fit(X_train_1, y_train_1)\nprint('[{}] Finished to train ridge (1)'.format(time.time() - start_time))\nridge_preds1 = model.predict(X_train_2)\nridge_preds1f = model.predict(sparse_merge_test)\nprint('[{}] Finished to predict ridge (1)'.format(time.time() - start_time))\nmodel = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha=3.3)\nmodel.fit(X_train_2, y_train_2)\nprint('[{}] Finished to train ridge (2)'.format(time.time() - start_time))\nridge_preds2 = model.predict(X_train_1)\nridge_preds2f = model.predict(sparse_merge_test)\nprint('[{}] Finished to predict ridge (2)'.format(time.time() - start_time))\nridge_preds_oof = np.concatenate((ridge_preds2, ridge_preds1), axis=0)\nridge_preds_test = (ridge_preds1f + ridge_preds2f) / 2.0\nprint('RMSLE OOF: {}'.format(rmse(ridge_preds_oof, y_train)))\nif not SUBMIT_MODE:\n    print('RMSLE TEST: {}'.format(rmse(ridge_preds_test, y_test)))\n\n\nmodel = MultinomialNB(alpha=0.01)\nmodel.fit(X_train_1, y_train_1 >= 4)\nprint('[{}] Finished to train MNB (1)'.format(time.time() - start_time))\nmnb_preds1 = model.predict_proba(X_train_2)[:, 1]\nmnb_preds1f = model.predict_proba(sparse_merge_test)[:, 1]\nprint('[{}] Finished to predict MNB (1)'.format(time.time() - start_time))\nmodel = MultinomialNB(alpha=0.01)\nmodel.fit(X_train_2, y_train_2 >= 4)\nprint('[{}] Finished to train MNB (2)'.format(time.time() - start_time))\nmnb_preds2 = model.predict_proba(X_train_1)[:, 1]\nmnb_preds2f = model.predict_proba(sparse_merge_test)[:, 1]\nprint('[{}] Finished to predict MNB (2)'.format(time.time() - start_time))\nmnb_preds_oof = np.concatenate((mnb_preds2, mnb_preds1), axis=0)\nmnb_preds_test = (mnb_preds1f + mnb_preds2f) / 2.0\n\n\ndel ridge_preds1\ndel ridge_preds1f\ndel ridge_preds2\ndel ridge_preds2f\ndel mnb_preds1\ndel mnb_preds1f\ndel mnb_preds2\ndel mnb_preds2f\ndel X_train_1\ndel X_train_2\ndel y_train_1\ndel y_train_2\ndel sparse_merge_train\ndel sparse_merge_test\ndel model\ngc.collect()\nprint('[{}] Finished garbage collection'.format(time.time() - start_time))\n\n\ndf_train['ridge'] = ridge_preds_oof\ndf_train['name_ridge'] = name_ridge_preds_oof\ndf_train['desc_ridge'] = desc_ridge_preds_oof\ndf_train['mnb'] = mnb_preds_oof\ndf_test['ridge'] = ridge_preds_test\ndf_test['name_ridge'] = name_ridge_preds_test\ndf_test['desc_ridge'] = desc_ridge_preds_test\ndf_test['mnb'] = mnb_preds_test\nprint('[{}] Finished adding submodels'.format(time.time() - start_time))\n\n\nf_cats = ['brand_name', 'gencat_name', 'subcat1_name', 'subcat2_name', 'name_first']\ntarget_encode = TargetEncoder(min_samples_leaf=100, smoothing=10, noise_level=0.01,\n                              keep_original=True, cols=f_cats)\ndf_train, df_test = target_encode.encode(df_train, df_test, y_train)\nprint('[{}] Finished target encoding'.format(time.time() - start_time))\n\n\ndf_train.drop(f_cats, axis=1, inplace=True)\ndf_test.drop(f_cats, axis=1, inplace=True)\ndel mnb_preds_oof\ndel mnb_preds_test\ndel ridge_preds_oof\ndel ridge_preds_test\ngc.collect()\nprint('[{}] Finished garbage collection'.format(time.time() - start_time))\n\n\ncols = ['gencat_name_te', 'brand_name_te', 'subcat1_name_te', 'subcat2_name_te',\n        'name_first_te', 'mnb', 'desc_ridge', 'name_ridge', 'ridge']\ntrain_dummies = csr_matrix(df_train[cols].values)\nprint('[{}] Finished dummyizing model 1/5'.format(time.time() - start_time))\ntest_dummies = csr_matrix(df_test[cols].values)\nprint('[{}] Finished dummyizing model 2/5'.format(time.time() - start_time))\ndel df_train\ndel df_test\ngc.collect()\nprint('[{}] Finished dummyizing model 3/5'.format(time.time() - start_time))\ntrain_features = hstack((train_features, train_dummies)).tocsr()\nprint('[{}] Finished dummyizing model 4/5'.format(time.time() - start_time))\ntest_features = hstack((test_features, test_dummies)).tocsr()\nprint('[{}] Finished dummyizing model 5/5'.format(time.time() - start_time))\n\n\nd_train = lgb.Dataset(train_features, label=y_train)\ndel train_features; gc.collect()\nif SUBMIT_MODE:\n    watchlist = [d_train]\nelse:\n    d_valid = lgb.Dataset(test_features, label=y_test)\n    watchlist = [d_train, d_valid]\n\nparams = {\n    'learning_rate': 0.15,\n    'application': 'regression',\n    'max_depth': 13,\n    'num_leaves': 400,\n    'verbosity': -1,\n    'metric': 'RMSE',\n    'data_random_seed': 1,\n    'bagging_fraction': 0.8,\n    'feature_fraction': 0.6,\n    'nthread': 4,\n    'lambda_l1': 10,\n    'lambda_l2': 10\n}\nprint('[{}] Finished compiling LGB'.format(time.time() - start_time))\n\nmodelL = lgb.train(params,\n                  train_set=d_train,\n                  num_boost_round=1350,\n                  valid_sets=watchlist,\n                  verbose_eval=50)\n\npredsL = modelL.predict(test_features)\npredsL[predsL < 0] = 0\n\nif not SUBMIT_MODE:\n    print(\"LGB RMSLE:\", rmse(predsL, y_test))\n\ndel d_train\ndel modelL\nif not SUBMIT_MODE:\n    del d_valid\ngc.collect()\n\n\npreds_final = predsFM * 0.33 + predsL * 0.67\nif not SUBMIT_MODE:\n    print('Final RMSE: ', rmse(preds_final, y_test))\n\n\nif SUBMIT_MODE:\n    preds_final = np.expm1(preds_final)\n    submission['price'] = preds_final\n    submission.to_csv('lgb_and_fm.csv', index=False)\n    print('[{}] Writing submission done'.format(time.time() - start_time))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}