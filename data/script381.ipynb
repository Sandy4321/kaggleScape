{"cells":[{"metadata":{"collapsed":true,"_uuid":"4dd86826913b2f8779885ec7536aa9a483a01ce7","_cell_guid":"0f3ab6b4-5b89-447c-841b-82ea3139295d","trusted":false},"cell_type":"markdown","source":"I start with [Alex Papiu's](https://www.kaggle.com/apapiu/ridge-script) script where he preprocesses the data into a ```scipy.sparse``` matrix and train a neural network. Keras does not like ```scipy.sparse``` matrices and converting the entire training set to a matrix will lead to computer memory issues; so the model is trained in batches: 32 samples at a time, and these few samples can be converted to matrices and fed into the network. \n              \nAlso, thanks to Pavel (Pasha) Gyrya's contribution for improving the model. Now there is no need to convert batches to np matrices. \n\nThis requieres a batch generator, which I pieced together from this [stack overflow question](https://stackoverflow.com/questions/41538692/using-sparse-matrices-with-keras-and-tensorflow) and I set up an iterator to make it threadsafe for parallelization. ~~Kagle allows the use of 32 cores which speeds up the training~~. Seems like kaggle only allows four cores.  \n\nI have been tuning the network and it seems like a smaller network with longer epochs yields better results. Currently I have a two hidden layers with 25  and 10 nodes. This is quite small but, with the input layer considered, this network still yields approximately 1.5M parameters!\n\nGive it a try and let me know what you think. There are still plenty of things on can try:\n* Add a validation set for early stopping. \n* Tune `batch_size`, `samples_per_epoch`, and nodes in hidden layers.\n* Add dropout.\n* Add L1 and/or L2 regularization.\n   \n\n","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"bddce000193062c3a129f2125d253e18926a2f71","_cell_guid":"4a79269a-c302-48af-bd96-1a426a651c18","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport scipy\nimport time\nimport gc\n\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dense, Dropout, Activation, Input\n\n\nstart_time = time.time()\n\n\ndef avg_predictions(L):\n    N = len(L)\n    f = L[0]\n    for p in range(1,N):\n        f = f + L[p]\n        f = (1/N)*f\n    return f\n\ndef define_model(data, nodes1, nodes2, drop1, drop2):\n    x = Input(shape = (data.shape[1], ), dtype = 'float32', sparse = True)     \n    d1 = Dense(nodes1, activation='relu')(x)\n    d2 = Dropout(drop1)(d1)\n    d3 = Dense(nodes2, activation='sigmoid')(d2)\n    d4 = Dropout(drop2)(d3)\n    out= Dense(1, activation = 'linear')(d4)\n    model = Model(x,out)\n    return model\n    \n\ndef preprocess(num_brands, name_min, max_feat_desc, ngrams):\n    print(\"Preprocessing Data...\")\n    NUM_BRANDS = num_brands\n    NAME_MIN_DF = name_min\n    MAX_FEAT_DESCP = max_feat_desc\n    \n    df_train = pd.read_csv('../input/train.tsv', sep='\\t')\n    df_train = df_train.reindex(np.random.permutation(df_train.index))\n    df_train.reset_index(inplace=True, drop=True)\n    \n    df_test = pd.read_csv('../input/test.tsv', sep='\\t')\n    \n    df = pd.concat([df_train, df_test], 0)\n    nrow_train = df_train.shape[0]\n    Y = np.log1p(df_train[\"price\"])\n    \n    del df_train\n    gc.collect()\n    \n    df[\"category_name\"] = df[\"category_name\"].fillna(\"Other\").astype(\"category\")\n    df[\"brand_name\"] = df[\"brand_name\"].fillna(\"unknown\")\n    \n    pop_brands = df[\"brand_name\"].value_counts().index[:NUM_BRANDS]\n    df.loc[~df[\"brand_name\"].isin(pop_brands), \"brand_name\"] = \"Other\"\n    \n    df[\"item_description\"] = df[\"item_description\"].fillna(\"None\")\n    df[\"item_condition_id\"] = df[\"item_condition_id\"].astype(\"category\")\n    df[\"brand_name\"] = df[\"brand_name\"].astype(\"category\")\n    \n    \n    #print(\"Encodings...\")\n    count = CountVectorizer(min_df=NAME_MIN_DF)\n    X_name = count.fit_transform(df[\"name\"])\n    \n    #print(\"Category Encoders...\")\n    unique_categories = pd.Series(\"/\".join(df[\"category_name\"].unique().astype(\"str\")).split(\"/\")).unique()\n    count_category = CountVectorizer()\n    X_category = count_category.fit_transform(df[\"category_name\"])\n    \n    #print(\"Descp encoders...\")\n    count_descp = TfidfVectorizer(max_features = MAX_FEAT_DESCP, \n                                  ngram_range = (1, ngrams),\n                                  stop_words = \"english\")\n    X_descp = count_descp.fit_transform(df[\"item_description\"])\n    \n    #print(\"Brand encoders...\")\n    vect_brand = LabelBinarizer(sparse_output=True)\n    X_brand = vect_brand.fit_transform(df[\"brand_name\"])\n    \n    #print(\"Dummy Encoders...\")\n    X_dummies = scipy.sparse.csr_matrix(pd.get_dummies(df[[\n        \"item_condition_id\", \"shipping\"]], sparse = True).values)\n    \n    X = scipy.sparse.hstack((X_dummies, \n                             X_descp,\n                             X_brand,\n                             X_category,\n                             X_name)).tocsr()\n\n    \n    return X[:nrow_train], Y, X[nrow_train:], df_test\n    \n    \ndef set_split(X_data, y_data, test_size):\n    \n    N = int(X_data.shape[0]*(1-test_size))\n    \n    return(X_data[:N], X_data[N:], y_data[:N], y_data[N:])\n\n\ndef hms_string(sec_elapsed):\n    h = int(sec_elapsed / (60 * 60))\n    m = int((sec_elapsed % (60 * 60)) / 60)\n    s = sec_elapsed % 60\n    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n\n\n\n\nstart_time = time.time()\n\nX, Y, X_test, df_test = preprocess(2500, 10, 50000, 3)\n\nx_train, x_val, y_train, y_val = set_split(X, Y, test_size=0.10)\n\nelapsed_time = time.time() - start_time\nprint(\"Preprocessing Time: {}\".format(hms_string(elapsed_time)))\n\ntpoint1 = time.time()\nprint(\"Fitting Model...\")\n\nnodes1 = 64\nnodes2 = 32\ndrop1 =  0.30\ndrop2 =  0.25\n\nprint(\"Training Model...\")\n    \nmodel = define_model(x_train, nodes1, nodes2, drop1, drop2)\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=3, verbose=1, mode='auto')\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\nmodel.fit(x=x_train, y=y_train,\n          batch_size=600,\n          callbacks=[monitor],\n          validation_data=(x_val, y_val),\n          epochs=10, verbose=0)\n    \ntpoint2 = time.time()\nprint(\"Time Training: {}\".format(hms_string(tpoint2-tpoint1)))\n    \npred = model.predict(x=X_test, batch_size=8000, verbose=0)\n\n\ntpoint3 = time.time()\nprint(\"Time for Predicting: {}\".format(hms_string(tpoint3-tpoint2)))\n\ndf_test[\"price\"] = np.expm1(pred)\ndf_test[[\"test_id\", \"price\"]].to_csv(\"submission_NN.csv\", index = False)\n\nelapsed_time = time.time() - start_time\nprint(\"Total Time: {}\".format(hms_string(elapsed_time)))\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"1a256c7b96241236b13e9b420d400eec50861d9b","_cell_guid":"3d5cf0c9-0866-4c35-a3df-37b24931e5d8","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}