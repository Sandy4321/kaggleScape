{"nbformat_minor": 0, "metadata": {"_is_fork": false, "_change_revision": 0, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.1", "name": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "file_extension": ".py", "nbconvert_exporter": "python"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "cells": [{"outputs": [], "metadata": {"_cell_guid": "a1e11ceb-ecb4-e83c-9571-dee02a834580", "_uuid": "595424eb4c5c162239022da0cd17e6635986556c"}, "cell_type": "markdown", "execution_count": null, "source": "# A 1D convolutional net in Keras\nVery little preprocessing was needed. Batch normalization made a huge difference, and made it possible to achieve perfect classification within the Kaggle kernel."}, {"outputs": [], "metadata": {"_execution_state": "idle", "_cell_guid": "61aec30a-f65c-02dd-77b0-67aec83e8632", "_uuid": "6ce895e9b32b7216d59f64c276b7d69068b287e8", "trusted": false}, "cell_type": "code", "execution_count": null, "source": "#We import libraries for linear algebra, graphs, and evaluation of results\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom scipy.ndimage.filters import uniform_filter1d"}, {"outputs": [], "metadata": {"_execution_state": "idle", "_cell_guid": "d4f7e51a-b964-8eaf-220b-7a924aa7932e", "_uuid": "3dab37c32ac2c1edef4bae4fc3534b92355911ae", "trusted": false}, "cell_type": "code", "execution_count": null, "source": "#Keras is a high level neural networks library, based on either tensorflow or theano\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv1D, MaxPool1D, Dense, Dropout, Flatten, \\\nBatchNormalization, Input, concatenate, Activation\nfrom keras.optimizers import Adam"}, {"outputs": [], "metadata": {"_cell_guid": "b3da338c-5beb-d12c-9cb4-088c1eb86069", "_uuid": "68a1f6dba3b023e7eb59c6983fdde57851c030ce"}, "cell_type": "markdown", "execution_count": null, "source": ["## Load the data"]}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "68ec0ee6eb9d8256998b7c15d37ec62b3e61eb5b"}, "cell_type": "markdown", "execution_count": null, "source": "As the data format is so simple, we do not need pandas."}, {"outputs": [], "metadata": {"_execution_state": "idle", "_cell_guid": "2763eb6d-1867-7254-7278-099e63ddd562", "_uuid": "1cfdd50387bac260912e23444d1234540d5c1b02", "trusted": false}, "cell_type": "code", "execution_count": null, "source": "INPUT_LIB = '../input/'\nraw_data = np.loadtxt(INPUT_LIB + 'exoTrain.csv', skiprows=1, delimiter=',')\nx_train = raw_data[:, 1:]\ny_train = raw_data[:, 0, np.newaxis] - 1.\nraw_data = np.loadtxt(INPUT_LIB + 'exoTest.csv', skiprows=1, delimiter=',')\nx_test = raw_data[:, 1:]\ny_test = raw_data[:, 0, np.newaxis] - 1.\ndel raw_data"}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "a0b077fb1c89e5afe853f137ded82ffbfcb0449f"}, "cell_type": "markdown", "execution_count": null, "source": "Scale each observation to zero mean and unit variance."}, {"outputs": [], "metadata": {"_execution_state": "idle", "_cell_guid": "17cbb5bb-29d7-9a0c-193c-9c132cdea389", "_uuid": "76dd566fee56a9feaf06b9cff2fa54f5239e33cb", "trusted": false}, "cell_type": "code", "execution_count": null, "source": "x_train = ((x_train - np.mean(x_train, axis=1).reshape(-1,1)) / \n           np.std(x_train, axis=1).reshape(-1,1))\nx_test = ((x_test - np.mean(x_test, axis=1).reshape(-1,1)) / \n          np.std(x_test, axis=1).reshape(-1,1))"}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "e2aa36f3133a1fad819e48dc60e120aed6283ce2"}, "cell_type": "markdown", "execution_count": null, "source": "This is our only preprocessing step: We add an input corresponding to the running average over\n200 time steps. This helps the net ignore high frequency noise and instead look at non-local\ninformation. Look at the graphs below to see what it does."}, {"outputs": [], "metadata": {"_execution_state": "busy", "_cell_guid": "2c006e96-8688-72e4-6abe-65d5d8f22fc6", "_uuid": "f4d354b6d9d6b2fb5c788355c99e61a62699323c", "trusted": false}, "cell_type": "code", "execution_count": null, "source": "x_train = np.stack([x_train, uniform_filter1d(x_train, axis=1, size=200)], axis=2)\nx_test = np.stack([x_test, uniform_filter1d(x_test, axis=1, size=200)], axis=2)"}, {"outputs": [], "metadata": {"_cell_guid": "c90d56a7-dec3-65a3-cac3-32e36c6bf31c", "_uuid": "6651259e2b8bab4c98f4649263e583201e494e45"}, "cell_type": "markdown", "execution_count": null, "source": ["## Train the model"]}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "a6eeb9020d7eae0925578490c692217659cd4a6b"}, "cell_type": "markdown", "execution_count": null, "source": "With the Sequential API for Keras, we only need to add the layers one at a time. Each 1D convolutional layers corresponds to a local filter, and then a pooling layer reduces the data length by approximately a factor 4. At the end, there are two dense layers, just as we would in a typical image classifier. Batch normalization layers speed up convergence. "}, {"outputs": [], "metadata": {"_execution_state": "busy", "_cell_guid": "3637a7be-c21b-3b78-b6b6-9146332f3972", "_uuid": "bbbee746cbd798b62d349a2b4d828e2fb889e164", "trusted": false}, "cell_type": "code", "execution_count": null, "source": "model = Sequential()\nmodel.add(Conv1D(filters=8, kernel_size=11, activation='relu', input_shape=x_train.shape[1:]))\nmodel.add(MaxPool1D(strides=4))\nmodel.add(BatchNormalization())\nmodel.add(Conv1D(filters=16, kernel_size=11, activation='relu'))\nmodel.add(MaxPool1D(strides=4))\nmodel.add(BatchNormalization())\nmodel.add(Conv1D(filters=32, kernel_size=11, activation='relu'))\nmodel.add(MaxPool1D(strides=4))\nmodel.add(BatchNormalization())\nmodel.add(Conv1D(filters=64, kernel_size=11, activation='relu'))\nmodel.add(MaxPool1D(strides=4))\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))"}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "5d25c5ad8a6ddd1319af7c56075a05fbfec8749b"}, "cell_type": "markdown", "execution_count": null, "source": "The data here is extremely unbalanced, with only a few positive examples. To correct for this, I use the positive examples a lot more often, so that the net sees 50% of each over each bats. Also, I generate new examples by rotation them randomly in time. This is called augmentation and is similar to when we rotate/shift examples in image classification."}, {"outputs": [], "metadata": {"_execution_state": "busy", "_cell_guid": "4ba8b520-7302-747c-7f84-8a816508e651", "_uuid": "0604716b9a92c9113dcd16631b6fc48ff006fd53", "trusted": false}, "cell_type": "code", "execution_count": null, "source": ["def batch_generator(x_train, y_train, batch_size=32):\n", "    \"\"\"\n", "    Gives equal number of positive and negative samples, and rotates them randomly in time\n", "    \"\"\"\n", "    half_batch = batch_size // 2\n", "    x_batch = np.empty((batch_size, x_train.shape[1], x_train.shape[2]), dtype='float32')\n", "    y_batch = np.empty((batch_size, y_train.shape[1]), dtype='float32')\n", "    \n", "    yes_idx = np.where(y_train[:,0] == 1.)[0]\n", "    non_idx = np.where(y_train[:,0] == 0.)[0]\n", "    \n", "    while True:\n", "        np.random.shuffle(yes_idx)\n", "        np.random.shuffle(non_idx)\n", "    \n", "        x_batch[:half_batch] = x_train[yes_idx[:half_batch]]\n", "        x_batch[half_batch:] = x_train[non_idx[half_batch:batch_size]]\n", "        y_batch[:half_batch] = y_train[yes_idx[:half_batch]]\n", "        y_batch[half_batch:] = y_train[non_idx[half_batch:batch_size]]\n", "    \n", "        for i in range(batch_size):\n", "            sz = np.random.randint(x_batch.shape[1])\n", "            x_batch[i] = np.roll(x_batch[i], sz, axis = 0)\n", "     \n", "        yield x_batch, y_batch"]}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "fd043b67ebf19352b1b8ec2a063ad8a50765e70d"}, "cell_type": "markdown", "execution_count": null, "source": "The hyperparameters here are chosen to finish training within the Kernel, rather than to get optimal results. On a GPU, I might have chosen a smaller learning rate, and perhaps SGD instead of Adam. As it turned out, results were brilliant anyway."}, {"outputs": [], "metadata": {"_execution_state": "idle", "_cell_guid": "fe6afcb2-7e0a-501d-bf19-759c0b3fda77", "_uuid": "c8844e546b1d4677b3be321cbc203c8c617ef7c4", "trusted": false}, "cell_type": "code", "execution_count": null, "source": "#Start with a slightly lower learning rate, to ensure convergence\nmodel.compile(optimizer=Adam(1e-5), loss = 'binary_crossentropy', metrics=['accuracy'])\nhist = model.fit_generator(batch_generator(x_train, y_train, 32), \n                           validation_data=(x_test, y_test), \n                           verbose=0, epochs=5,\n                           steps_per_epoch=x_train.shape[1]//32)"}, {"outputs": [], "metadata": {"_execution_state": "busy", "_cell_guid": "964bdbf2-a325-11da-98c8-4dd56b31c9ad", "_uuid": "6030dfe0cb763066fdecf81a27dff897cce69af4", "trusted": false}, "cell_type": "code", "execution_count": null, "source": "#Then speed things up a little\nmodel.compile(optimizer=Adam(4e-5), loss = 'binary_crossentropy', metrics=['accuracy'])\nhist = model.fit_generator(batch_generator(x_train, y_train, 32), \n                           validation_data=(x_test, y_test), \n                           verbose=2, epochs=40,\n                           steps_per_epoch=x_train.shape[1]//32)"}, {"outputs": [], "metadata": {"_execution_state": "busy", "collapsed": false, "_uuid": "31d3be9df5d7387bf68eb9ca2c1c73c1590c8681"}, "cell_type": "markdown", "execution_count": null, "source": "#Evaluate the model"}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "d8a74931a02d8ceb43c03f9f687f934581e11e9d"}, "cell_type": "markdown", "execution_count": null, "source": "First we look at convergence"}, {"outputs": [], "metadata": {"_execution_state": "idle", "_cell_guid": "020d5851-762a-d02c-77fa-319c3af6a4af", "_uuid": "6c22b94c662b6b25fd33c85fd05892aca54e5d86", "trusted": false}, "cell_type": "code", "execution_count": null, "source": ["plt.plot(hist.history['loss'], color='b')\n", "plt.plot(hist.history['val_loss'], color='r')\n", "plt.show()\n", "plt.plot(hist.history['acc'], color='b')\n", "plt.plot(hist.history['val_acc'], color='r')\n", "plt.show()"]}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "62f58f451ef2de7c4895f8bdb449dae44a122871"}, "cell_type": "markdown", "execution_count": null, "source": "We then use our trained net to classify the test set."}, {"outputs": [], "metadata": {"_execution_state": "idle", "_cell_guid": "8ef32235-eebb-91e2-4571-1b585aa86ad4", "_uuid": "62e80b4160b0bfd27bf1acdb2e5b6baed971fc01", "trusted": false}, "cell_type": "code", "execution_count": null, "source": ["non_idx = np.where(y_test[:,0] == 0.)[0]\n", "yes_idx = np.where(y_test[:,0] == 1.)[0]\n", "y_hat = model.predict(x_test)[:,0]"]}, {"outputs": [], "metadata": {"_execution_state": "idle", "_cell_guid": "5f37a9af-ceab-8b5a-3f77-fefd6eb38710", "_uuid": "4b002ba89846d5f0c25fd29466c9ceb4801fc7e5", "trusted": false}, "cell_type": "code", "execution_count": null, "source": ["plt.plot([y_hat[i] for i in yes_idx], 'bo')\n", "plt.show()\n", "plt.plot([y_hat[i] for i in non_idx], 'ro')\n", "plt.show()"]}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "ffabc4d6783302adc67cf15ac6ca038030ff8842"}, "cell_type": "markdown", "execution_count": null, "source": "These graphs show that the five positive examples all get 0.95-1.00 score. Also, almost all negative examples get score close to zero, except a few in the 0.9-1.0 range. This is encouraging."}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "088fa46309212bf4a02766ea44e918d561643718"}, "cell_type": "markdown", "execution_count": null, "source": "We now choose an optimal cutoff score for classification. Sklearn can help us with this."}, {"outputs": [], "metadata": {"_execution_state": "busy", "_cell_guid": "3d10d764-1a5f-7315-ea45-9e9481261ea4", "_uuid": "38e7322f333dc2afa38186f208e5b96f4bd83fb9", "trusted": false}, "cell_type": "code", "execution_count": null, "source": "y_true = (y_test[:, 0] + 0.5).astype(\"int\")\nfpr, tpr, thresholds = roc_curve(y_true, y_hat)\nplt.plot(thresholds, 1.-fpr)\nplt.plot(thresholds, tpr)\nplt.show()\ncrossover_index = np.min(np.where(1.-fpr <= tpr))\ncrossover_cutoff = thresholds[crossover_index]\ncrossover_specificity = 1.-fpr[crossover_index]\nprint(\"Crossover at {0:.2f} with specificity {1:.2f}\".format(crossover_cutoff, crossover_specificity))\nplt.plot(fpr, tpr)\nplt.show()\nprint(\"ROC area under curve is {0:.2f}\".format(roc_auc_score(y_true, y_hat)))"}, {"outputs": [], "metadata": {"_cell_guid": "67e27578-3029-3e51-448f-bc2b7a8f24ab", "_uuid": "d15012aa5570172dee9f99f79dc962a06aedd63c"}, "cell_type": "markdown", "execution_count": null, "source": "Let's take a look at the misclassified data (if any):"}, {"outputs": [], "metadata": {"_execution_state": "idle", "_cell_guid": "28f895b0-597b-4f46-1a7f-7377d97ade05", "_uuid": "ce60fdc5bcf2b2d08f68c22d581116383d6b6c45", "trusted": false}, "cell_type": "code", "execution_count": null, "source": "false_positives = np.where(y_hat * (1. - y_test) > 0.5)[0]\nfor i in non_idx:\n    if y_hat[i] > crossover_cutoff:\n        print(i)\n        plt.plot(x_test[i])\n        plt.show()"}, {"outputs": [], "metadata": {"_cell_guid": "3c3c8bbf-9638-8f4f-050e-695a66a8b84a", "_uuid": "84ee7f7a1f17f0c2d5f309fa957a0cb8bbcd38ba"}, "cell_type": "markdown", "execution_count": null, "source": "It seems NASA missed one planet. I take this opportunity to claim it, and hereby name it Kaggle alpha :)"}, {"outputs": [], "metadata": {"_execution_state": "idle", "_cell_guid": "0bebfd57-3ff5-90d1-79a2-4fb500116cc6", "_uuid": "0e60c2111bc64417f9320ecb5f6adab298423606", "trusted": false}, "cell_type": "code", "execution_count": null, "source": ""}]}