
# coding: utf-8

# # Intro
# 
# **This is Lesson 6 in the [Deep Learning](https://www.kaggle.com/learn/deep-learning) track**  
# 
# At the end of this lesson, you will understand how stochastic gradient descent and back-propagation are used to set the weights in a deep learning model. These topics are complex, but many experts view them as the most important ideas in deep learning.
# 
# # Lesson
# 

# In[1]:


from IPython.display import YouTubeVideo
YouTubeVideo('kQmHaI5Jw1c', width=800, height=450)


# ---
# ** Links Mentioned**
# 
# [ReLU activation function](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning)
