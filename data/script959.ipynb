{"cells":[{"metadata":{"_cell_guid":"77fdaca8-e26a-4e6c-9259-659be6bb1b4b","_uuid":"440a26d54c92a5cfef3265b409a7b96c261fcafc"},"cell_type":"markdown","source":"# Introduction:\nWhat is Medium? Medium is a dynamically developing international publishing platform for people to write, read and clap easily online. It is like the russian [habrahabr.ru](http://habrahabr.ru) just a little worse. We have two JSON files that contain published articles on Medium till 2018, March. There is number of claps to each article in the first file and there is no ones in the second file. Our goal is to predict the number of \"claps\" for articles in test. \nLet's start our EDA journey!"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import Ridge\nfrom scipy.sparse import csr_matrix, hstack\nfrom scipy.stats import probplot\nimport pickle\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\nimport seaborn as sns \nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\nimport time\n\ncolor = sns.color_palette()\nsns.set_style(\"whitegrid\")\nsns.set_context(\"paper\")\nsns.palplot(color)\n\nimport os\nPATH = \"../input\"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"be16838e-9c52-4f9d-8225-a79e0a4ee9ef","_uuid":"cccb3e54b3b43651788a2b95733c5870201f7bf8","trusted":false,"collapsed":true},"cell_type":"code","source":"!du -l ../input/*","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"711af372-cbe3-4b72-b8f4-8b07931dbb57","_uuid":"3b9b29fd9fff6e4e3be16307df48f2d0d3dd8bd3"},"cell_type":"markdown","source":"# 1. Data preprocessing\n## 1.1. Supplementary functions"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_kg_hide-input":false,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"trusted":false},"cell_type":"code","source":"def read_json_line(line=None):\n    result = None\n    try:        \n        result = json.loads(line)\n    except Exception as e:      \n        # Find the offending character index:\n        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      \n        # Remove the offending character:\n        new_line = list(line)\n        new_line[idx_to_replace] = ' '\n        new_line = ''.join(new_line)     \n        return read_json_line(line=new_line)\n    return result\n\nfrom html.parser import HTMLParser\n\nclass MLStripper(HTMLParser):\n    def __init__(self):\n        self.reset()\n        self.strict = False\n        self.convert_charrefs= True\n        self.fed = []\n    def handle_data(self, d):\n        self.fed.append(d)\n    def get_data(self):\n        return ''.join(self.fed)\n\ndef strip_tags(html):\n    s = MLStripper()\n    s.feed(html)\n    return s.get_data()\n\ndef extract_features(path_to_data):\n    \n    content_list = [] \n    published_list = [] \n    title_list = []\n    author_list = []\n    domain_list = []\n    tags_list = []\n    url_list = []\n    \n    with open(path_to_data, encoding='utf-8') as inp_json_file:\n        for line in inp_json_file:\n            json_data = read_json_line(line)\n            content = json_data['content'].replace('\\n', ' ').replace('\\r', ' ')\n            content_no_html_tags = strip_tags(content)\n            content_list.append(content_no_html_tags)\n            published = json_data['published']['$date']\n            published_list.append(published) \n            title = json_data['meta_tags']['title'].split('\\u2013')[0].strip() #'Medium Terms of Service – Medium Policy – Medium'\n            title_list.append(title) \n            author = json_data['meta_tags']['author'].strip()\n            author_list.append(author) \n            domain = json_data['domain']\n            domain_list.append(domain)\n            url = json_data['url']\n            url_list.append(url)\n            \n            tags_str = []\n            soup = BeautifulSoup(content, 'lxml')\n            try:\n                tag_block = soup.find('ul', class_='tags')\n                tags = tag_block.find_all('a')\n                for tag in tags:\n                    tags_str.append(tag.text.translate({ord(' '):None, ord('-'):None}))\n                tags = ' '.join(tags_str)\n            except Exception:\n                tags = 'None'\n            tags_list.append(tags)\n            \n    return content_list, published_list, title_list, author_list, domain_list, tags_list, url_list","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"06aa8abd-d199-478c-94a9-e285543a42a3","_uuid":"69af89dd753639281213e6d772b2bb48eac20652"},"cell_type":"markdown","source":"## 1.2. Data extraction"},{"metadata":{"scrolled":true,"_cell_guid":"26ae1dd4-b962-44d6-bf18-1e2e49453488","_uuid":"2928c74776ff42cc91a38a2a687cbc9b6b40519c","trusted":false,"collapsed":true},"cell_type":"code","source":"content_list, published_list, title_list, author_list, domain_list, tags_list, url_list = extract_features(os.path.join(PATH, 'how-good-is-your-medium-article/train.json'))\ntrain = pd.DataFrame()\ntrain['content'] = content_list\ntrain['published'] = pd.to_datetime(published_list, format='%Y-%m-%dT%H:%M:%S.%fZ')\ntrain['title'] = title_list\ntrain['author'] = author_list\ntrain['domain'] = domain_list\ntrain['tags'] = tags_list\ntrain['length'] = train['content'].apply(len)\ntrain['url'] = url_list\n\ncontent_list, published_list, title_list, author_list, domain_list, tags_list, url_list = extract_features(os.path.join(PATH, 'how-good-is-your-medium-article/test.json'))\ntest = pd.DataFrame()\ntest['content'] = content_list\ntest['published'] = pd.to_datetime(published_list, format='%Y-%m-%dT%H:%M:%S.%fZ')\ntest['title'] = title_list\ntest['author'] = author_list\ntest['domain'] = domain_list\ntest['tags'] = tags_list\ntest['length'] = test['content'].apply(len)\ntest['url'] = url_list\n\ntrain_target = pd.read_csv(os.path.join(PATH, 'how-good-is-your-medium-article/train_log1p_recommends.csv'), index_col='id')\ny_train = train_target['log_recommends'].values\n\ndel content_list, published_list, title_list, author_list, domain_list, tags_list, url_list\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1a291341-248a-4bb3-9ea2-16862a52c92e","_uuid":"ddea81f54a5c84003304854959b1850414928c3f"},"cell_type":"markdown","source":"## 1.3. Feature engineering"},{"metadata":{"_cell_guid":"edd89bc9-cb0f-4244-9cd9-d6bb03ddc1fb","_uuid":"4718fa8fcdacd716a72f9dba5daa12708968d8c7","trusted":false,"collapsed":true},"cell_type":"code","source":"idx_split = len(train)\ndf_full = pd.concat([train, test])\n\ndf_full['dow'] = df_full['published'].apply(lambda x: x.dayofweek)\ndf_full['year'] = df_full['published'].apply(lambda x: x.year)\ndf_full['month'] = df_full['published'].apply(lambda x: x.month)\ndf_full['hour'] = df_full['published'].apply(lambda x: x.hour)\ndf_full['number_of_tags'] = df_full['tags'].apply(lambda x: len(x.split()))\n\ntrain = df_full.iloc[:idx_split, :]\ntest = df_full.iloc[idx_split:, :]\n\ntrain['target'] = y_train\ntrain.sort_values(by='published', inplace=True)\ntrain.reset_index(drop=True, inplace=True)\n\nprint('TRAIN: {}'.format(train.shape))\nprint('TEST: {}'.format(test.shape))\ndel df_full\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9049dd3b-cab1-48b7-af97-2cbeff8ffb72","_uuid":"8028e493108552fe543c8415dfd735eda94ddd9a"},"cell_type":"markdown","source":"The train data contains 62313 articles and the test one contains 34645.\nLet us look at the data a little closer."},{"metadata":{"_cell_guid":"da818888-f782-406c-9fb3-6284c84bfccb","_uuid":"6f48fd2225c752b98ad4a0b0cddcad9d2aed287b"},"cell_type":"markdown","source":"# 2. EDA\nThe five earliest articles on Medium:"},{"metadata":{"_cell_guid":"4e0f9c06-4c82-406e-8c24-70734bbd95df","_uuid":"5efafcfe4fd358d7058df272443f8b350ebf02ef","trusted":false,"collapsed":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0c17abb8-d1d4-4632-954d-14d7f9eba4c6","_uuid":"62b7b37ea5a68ddbd03c29100765a367a9eb52a8"},"cell_type":"markdown","source":"As we can see, first articles were published about 50 years ago. Great :)"},{"metadata":{"_cell_guid":"2d4135c0-c123-4c1f-b282-7eda18630fa4","_uuid":"d0141520e406ab863176c0a971b21754961adeb1"},"cell_type":"markdown","source":"## 2.1. Target variable\n\nThe target variable is number of claps and it was log1p transformed in advance. So, keep that in mind."},{"metadata":{"_cell_guid":"3481d7d2-2378-4b25-94d9-f5e9ee488378","_uuid":"14a1011be1155858135c3316ebd1e5db01961f22","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\nplt.suptitle(\"Target variable\",fontsize=20)\ngridspec.GridSpec(2,2)\n\nplt.subplot2grid((2,2),(0,0))\nplt.xlim(0, 12)\nsns.distplot(train.target.values, hist=False, color=color[0], kde_kws={\"shade\": True, \"lw\": 2})\nplt.title(\"Number of claps (log1p transformed)\")\n\nplt.subplot2grid((2,2),(1,0))\nplt.xlim(0, 12)\nsns.boxplot(train.target.values)\n\nplt.subplot2grid((2,2),(0,1), rowspan=2)\nplt.ylim(0, 12)\n# plt.grid(False)\nprobplot(train.target.values, dist=\"norm\", plot=plt);","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f6ed9974-c8f7-46bc-85c4-a65ea4be44a3","_uuid":"54a592d9ee1bb7b77f1bc001ded2f4cbe40fb59d"},"cell_type":"markdown","source":"Well, bad news. The distribution of the target variable is far from normal even though  it was log transformed. It means that we cant use parametric statistical tests in the future at least. Just non-parametric ones. Some articles was claped about 70 000 times. Maybe it's a mistake in the data? Lets look the most popular article and its number of claps."},{"metadata":{"_cell_guid":"5835649c-821f-4bd7-8c39-d2c8f27bef5d","_uuid":"6042754555ac3613ce946eaef23bed6038a14f1d","trusted":false,"collapsed":true},"cell_type":"code","source":"train.sort_values(by='target', ascending=False).reset_index(drop=True).loc[0, 'url']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"88c94c76-833c-4827-b0dc-c08a97588b5f","_uuid":"c83cb9e51ca6c37a1d181bbae10065756e008446"},"cell_type":"markdown","source":"<img src='https://image.prntscr.com/image/OryvX61BTkCRzM2GTXi_PA.png'>"},{"metadata":{"_cell_guid":"c600d8fa-8d03-4072-bb32-e3833cd42bb5","_uuid":"9d5920c6cc082ed4f4dea9a280e8ad427adbf0ae"},"cell_type":"markdown","source":"![](http://)No mistake. It was really claped more than 79 000 times. "},{"metadata":{"_cell_guid":"d61b2f4a-86e6-4d24-aa1f-c2285509a520","_uuid":"637bbb825886fc0f6383e9ae819441abb00e7657"},"cell_type":"markdown","source":"## 2.2. Time series features"},{"metadata":{"_cell_guid":"f0baad27-c332-4712-b01a-161f638f164a","_uuid":"24e2948c3dbbd0f7add8b04dc181c3e27e6311f4","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.suptitle(\"                       Posts distribution across years\",fontsize=20)\n\nax1 = plt.subplot2grid((1,5),(0,0), colspan=3)\nax1 = sns.countplot(x='year', data=train, alpha=0.8, color=color[2])\nplt.ylabel('Overall posts', fontsize=12)\nplt.xlabel('Year', fontsize=12)\nplt.title('Train data', fontsize=15)\nplt.grid(False)\n\nfor p in ax1.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax1.annotate('{}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom')\n    \nax2 = plt.subplot2grid((1,5),(0,3), colspan=2, sharey=ax1)\nax2 = sns.countplot(x='year', data=test, alpha=0.8, color=color[9])\nplt.xlabel('Year', fontsize=12)\nplt.title('Test data', fontsize=15)\nplt.yticks([])\nplt.ylabel('')\n\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax2.annotate('{}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8567b730-15f9-4fea-bb68-7484a1979267","_uuid":"039a6151785ad87508360dacf524be67bf12c5f3"},"cell_type":"markdown","source":"The largest number of post falls in the last years. Let`s leave only 2015-2017."},{"metadata":{"collapsed":true,"_cell_guid":"a666d095-34ea-4409-a4e7-99bdd2fec43c","_uuid":"10680913014764a952899ed683b8b863f6e6874c","trusted":false},"cell_type":"code","source":"train = train[train.year >= 2015]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c26f1504-a866-49e0-98ce-0aa8caa8d23a","_uuid":"888295e200d344f8aeb50b23ac6c1150d6e089ef","trusted":false,"collapsed":true},"cell_type":"code","source":"temp=pd.concat([train.groupby(['year','month'])['hour'].count(), test.groupby(['year','month'])['hour'].count().iloc[:-1]])\nplt.figure(figsize=(12,4))\nsns.pointplot(temp.index,temp.values, alpha=0.8, color=color[1],)\nplt.ylabel('Overall posts', fontsize=12)\nplt.xlabel('Month', fontsize=12)\nplt.title('Monthly posts variation', fontsize=15)\nplt.xticks(rotation='vertical');\n\ntemp=train.groupby(['year','month']).aggregate({'hour':np.size,'year':np.min,'month':np.min})\ntemp.reset_index(drop=True, inplace=True)\nplt.figure(figsize=(12,6))\nplt.plot(range(1,13),temp.iloc[0:12,0],label=\"2015\", marker='o')\nplt.plot(range(1,13),temp.iloc[12:24,0],label=\"2016\", marker='o')\nplt.plot(range(1,7),temp.iloc[24:30,0],label=\"2017-train\", marker='o')\nconnect_point = temp.iloc[29,0]\n\ntemp=test.groupby(['year','month']).aggregate({'hour':np.size,'year':np.min,'month':np.min})\ntemp.reset_index(drop=True, inplace=True)\nplt.plot(range(6,8),[connect_point,temp.iloc[0,0]], color='r',label=None)\nplt.plot(range(7,13),temp.iloc[0:6,0], color='r',label=\"2017-test\", marker='o')\nplt.plot(range(1,3),temp.iloc[6:8,0],label=\"2018-test\", marker='o')\nplt.ylabel('Overall posts', fontsize=12)\nplt.xlabel('Month', fontsize=12)\nplt.title('Monthly posts variation', fontsize=15)\nplt.xticks(np.arange(1, 13, 1.0))\nplt.xlim(1, 12)\nplt.legend(loc='upper right', fontsize=11)\nplt.xticks(rotation='horizontal');","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"94a7b667-f9c0-4ca6-a97b-3cbce92b5c96","_uuid":"825cc801e329d2b72a41c1771c07af26018f2544"},"cell_type":"markdown","source":"It's very interesting. First of all, Medium is rapidly becoming the popular platform. Then, April and September have less published articles than in previous month year after year. But in March, May and October is opposite situation. And the shocking upsurge of popularity in the 2018!"},{"metadata":{"_cell_guid":"1b0d947f-27c1-4677-af5f-e8ab0b5d0589","_uuid":"c6b31c72956e9a49396941de14023ab9b10debf7","trusted":false,"collapsed":true},"cell_type":"code","source":"temp=train.groupby(['year','month'])['target'].sum()\nplt.figure(figsize=(13,4))\nsns.pointplot(temp.index,temp.values, alpha=0.8, color=color[4],)\nplt.ylabel('Overall claps (log1p transformed)', fontsize=12)\nplt.xlabel('Month', fontsize=12)\nplt.title('Monthly claps variation', fontsize=15)\nplt.xticks(rotation='vertical');","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"50db8607-9203-4ea9-ab14-83fcd794bef4","_uuid":"beef9cad2c9faaf87d9f70c4f1c9c2ebf67b4d64"},"cell_type":"markdown","source":"And we can see the closely related situation to number of claps."},{"metadata":{"_cell_guid":"c74c406d-5674-4648-b298-ae1510aa03b5","_uuid":"73b0b8c8dbf8b9e4fdaf1093532fbc1fef8ee3f8","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\n\nplt.subplot(121)\nax1 = sns.boxplot(y='target',x='dow', data=train)\nplt.ylabel('Claps by post', fontsize=12)\nplt.xlabel('Day of week', fontsize=12)\nplt.title('Claps distribution across day of week', fontsize=15)\nax1.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n\nplt.subplot(122)\ntemp = train.groupby('dow')['target'].sum()\nax2 = sns.barplot(temp.index,np.round(temp.values))\nplt.ylabel('Number of claps', fontsize=12)\nplt.xlabel('Day of week', fontsize=12)\nplt.title('Count of claps across day of week', fontsize=15)\nax2.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax2.annotate('{}'.format(int(p.get_height())), (x.mean(), y), ha='center', va='bottom');","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2b8fe348-f3bd-4870-b142-4dfb6ff64529","_uuid":"c9f8a36a5109b98ecccd86d41765e79950cc42e2"},"cell_type":"markdown","source":"Median and number of claps is greater to articles published earlier in the week. I think, that articles get the highest number of claps in the first few hours after publishing. Therefore the first thing is not surprising, people are much kinder after weekends :) The second thing is easy to understand too. What do you do on Monday at work? You are reading habr`s posts and like them :) And what do you do on weekend? Right, you are cycling, skydiving, swimming anв etc."},{"metadata":{"_cell_guid":"041e18bd-98f2-416c-8ca6-2458871e0c35","_uuid":"8e2812486bcdee7e49f931676d8bc488f919f205","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\n\nplt.subplot(121)\nax1 = sns.boxplot(y='target',x='hour', data=train, color=color[9])\nplt.ylabel('Claps by post', fontsize=12)\nplt.xlabel('Hour', fontsize=12)\nplt.title('Claps distribution across hour', fontsize=15)\n\nplt.subplot(122)\ntemp = train.groupby('hour')['target'].sum()\nax2 = sns.barplot(temp.index,temp.values, alpha=0.8, color=color[9])\nplt.ylabel('Number of claps', fontsize=12)\nplt.xlabel('Hour', fontsize=12)\nplt.title('Count of claps across hour', fontsize=15);","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"812b7f7a-79bf-462f-ae81-3b5256ab16c3","_uuid":"3502fd3b470702fb0a9c388719e56477713e456e"},"cell_type":"markdown","source":"The similar situation like in previous plot."},{"metadata":{"_cell_guid":"58c327ee-a93a-41de-98f1-ae9bbfb15b1b","_uuid":"71bb70cf454085213b4cde8706e29f85b4759260","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.title('Claps distribution across hour and day of week', fontsize=20)\ntemp = train.pivot_table(index='dow', columns='hour', values='target', aggfunc='mean')\nax = sns.heatmap(temp, annot=True, fmt='.2f', cmap='viridis')\nax.set(xlabel='Hour', ylabel='Day of week')\nax.set_yticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\nplt.yticks(rotation='horizontal');","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"68f05cbc-93c6-491d-b66b-57f0f29c2122","_uuid":"878570332639e647cb9ed0e756b20cac0f40718c"},"cell_type":"markdown","source":"Mean of claps is greater to posts published earlier in the week in the second half of the day. But it is interesting situation here. Posts published on Monday at night and on weekend in the afternoon have the high mean value of claps."},{"metadata":{"_cell_guid":"0d6874ec-1110-4794-8011-cf9c1993467c","_uuid":"709925657f0d2d4073a12972af59369e8731301d"},"cell_type":"markdown","source":"## 2.3. Other features"},{"metadata":{"_cell_guid":"0aa8b8de-902d-4c47-bf87-a01f1465e1c7","_uuid":"85f9666567c86ac953fea6baeb6cb1f6931a8a78","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.suptitle(\"Posts distribution across domains\",fontsize=20)\n\nax1 = plt.subplot2grid((1,2),(0,0))\nax1 = sns.countplot(x='domain', data=train, alpha=0.8, color=color[2], order=train.domain.value_counts().iloc[:10].index)\nplt.ylabel('Number of posts', fontsize=12)\nplt.xlabel('Domain', fontsize=12)\nplt.title('Train data', fontsize=15)\nplt.grid(False)\nplt.xticks(rotation=90)\n\nfor p in ax1.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax1.annotate('{}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom')\n    \nax2 = plt.subplot2grid((1,2),(0,1), sharey=ax1)\nax2 = sns.countplot(x='domain', data=test, alpha=0.8, color=color[9], order=test.domain.value_counts().iloc[:10].index)\nplt.xlabel('Domain', fontsize=12)\nplt.title('Test data', fontsize=15)\nplt.yticks([])\nplt.ylabel('')\nplt.xticks(rotation=90)\n\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax2.annotate('{}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom');","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eb344f8b-3bc4-4971-b33b-87d13a9577ba","_uuid":"2a88d29798b8f20ff4831d36e449975d5c560c40"},"cell_type":"markdown","source":"There are TOP-10 domains by posts on the plot.  The most significant share is published on two ones - medium.com and hackermoon.com."},{"metadata":{"_cell_guid":"71541bbb-6ed0-4e2e-a262-14f52e1c5a7d","_uuid":"1797a2980d64501fd3a6f65d07820f3b8cf8996a","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\n\nplt.subplot(121)\ntemp = train[train.domain.isin(['medium.com', 'hackernoon.com'])]\nax1 = sns.boxplot(y='target',x='year', hue='domain', data=temp)\nplt.ylabel('Claps by post', fontsize=12)\nplt.xlabel('Domain', fontsize=12)\nplt.title('Claps distribution across domains', fontsize=15)\n\nplt.subplot(122)\ntemp = temp.groupby('domain')['target'].sum().iloc[:2]\nax2 = sns.barplot(temp.index,temp.values)\nplt.ylabel('Number of claps', fontsize=12)\nplt.xlabel('Domain', fontsize=12)\nplt.title('Count of claps across domain', fontsize=15);","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a22f84a7-06e7-4e06-befe-59dc62ea6037","_uuid":"77cd90491dae869a5d1e003a1659cf1d7783a3a0"},"cell_type":"markdown","source":" What we're seeing here? Medium`s posts get fewer and fewer claps year by year. And the mean value of claps on hackernoon is significant higher. But  the bulk of posts there is on medium.com."},{"metadata":{"_cell_guid":"112cba09-74e7-4d9e-b510-af3f550f4d6b","_uuid":"205ccc6a25d4287f01a3032619d4f49ccffa24ce","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.suptitle(\"Count of posts across authors\",fontsize=20)\n\nax1 = plt.subplot2grid((1,2),(0,0))\nax1 = sns.countplot(x='author', data=train, alpha=0.8, color=color[2], order=train.author.value_counts().iloc[:10].index)\nplt.ylabel('Overall posts', fontsize=12)\nplt.xlabel('Author', fontsize=12)\nplt.title('Train data', fontsize=15)\nplt.grid(False)\nplt.xticks(rotation=90, fontsize=12)\n\nfor p in ax1.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax1.annotate('{}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom')\n    \nax2 = plt.subplot2grid((1,2),(0,1), sharey=ax1)\nax2 = sns.countplot(x='author', data=test, alpha=0.8, color=color[9], order=test.author.value_counts().iloc[:10].index)\nplt.xlabel('Author', fontsize=12)\nplt.title('Test data', fontsize=15)\nplt.yticks([])\nplt.ylabel('')\nplt.xticks(rotation=90, fontsize=12)\n\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax2.annotate('{}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom');","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"29204462-b166-4a1d-b352-ab429aa4f5c8","_uuid":"65e5d30dbeedb83b0c959db6cba0ee122a8b728e"},"cell_type":"markdown","source":"There are new active authors and some old popular authors disappear. Thats normal to every popular platform. People change.\nAnd an interesting observation is that some authors are corporate blogs like ODS on the habr."},{"metadata":{"_cell_guid":"cc3d971d-c26f-4b01-8aa3-254df24023d7","_uuid":"56af10126bcf687685a4fff742c6e471c41e7dc2","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(18,6))\n\n\ntemp = train.groupby('author')['target'].sum().sort_values(ascending=False).iloc[:30]\nax1 = sns.barplot(temp.index,np.round(temp.values, 1), alpha=0.8, color=color[3])\nplt.ylabel('Number of claps', fontsize=12)\nplt.xlabel('Author', fontsize=12)\nplt.title('Number of claps across authors', fontsize=15)\nplt.grid(False)\nplt.xticks(rotation=90, fontsize=12)\n\nfor p in ax1.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax1.annotate('{}'.format(int(p.get_height())), (x.mean(), y), ha='center', va='bottom')\n    \nplt.figure(figsize=(18,6))\n\n\ntemp = train.groupby('author')['target'].median().sort_values(ascending=False).iloc[:30]\nax2 = sns.barplot(temp.index,np.round(temp.values, 1), alpha=0.8, color=color[4])\nplt.ylabel('Median of claps by post', fontsize=12)\nplt.xlabel('Author', fontsize=12)\nplt.title('Median of claps across authors', fontsize=15)\nplt.grid(False)\nplt.xticks(rotation=90, fontsize=12)\n\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax2.annotate('{}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom');","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6199aba3-ad75-44d9-95c1-5c4204adf70d","_uuid":"ae950e725af2a1b408c9b92f732a9c874fe40de2"},"cell_type":"markdown","source":"The most popular author is Dina Leygermann. Lets look at her blog. In this way we make it possible to forge a common understanding of the Medium audience.\n\n<img src='https://image.prntscr.com/image/K0xjSl99SvuFt-qnsgtPXQ.png'>\n\nWell, sitcoms, politic... thats not the habr )"},{"metadata":{"_cell_guid":"cc9bf3f0-d99f-4bf6-8afe-69dab5064b08","_uuid":"66ee977332680ed7a2f04dc4a74cea9f3bbc85ce","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\nplt.suptitle(\"Length of post distribution\",fontsize=20)\ngridspec.GridSpec(2,1)\n\nplt.subplot2grid((2,1),(0,0))\nplt.xlim(0, 450000)\nsns.distplot(train.length.values, hist=False, color=color[0], kde_kws={\"shade\": True, \"lw\": 2})\nplt.title(\"Number of chars\")\n\nplt.subplot2grid((2,1),(1,0))\nplt.xlim(0, 450000)\nsns.boxplot(train.length.values);\n\nplt.figure(figsize=(15,6))\nplt.suptitle(\"Length of post distribution (log1p transformed)\",fontsize=20)\ngridspec.GridSpec(2,1)\n\nplt.subplot2grid((2,1),(0,0))\nsns.distplot(np.log1p(train.length.values), hist=False, color=color[0], kde_kws={\"shade\": True, \"lw\": 2})\nplt.title(\"Number of chars (log1p transformed)\")\n\nplt.subplot2grid((2,1),(1,0))\nsns.boxplot(np.log1p(train.length.values));","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5a3e03d7-40c1-468a-97fb-42ee54e1c13a","_uuid":"4be143f941500fb9adea0097ddf970892d97002c"},"cell_type":"markdown","source":"Some posts have about 400 000 chars. That's the equivalent of about 100 pages in Microsoft Word with font Times New Roman and size 12. Fantastic! "},{"metadata":{"_cell_guid":"e144911e-19fc-452b-ba62-78be95ec6a04","_uuid":"ba1dda27014247a49435c484d952bb175bf0282a","trusted":false,"collapsed":true},"cell_type":"code","source":"ax = sns.jointplot(x=np.log1p(train[\"length\"]), y=train[\"target\"], kind='kde', size=9)\nax.set_axis_labels(\"Length of article\", \"Number of claps\");","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"16f2b60a-5461-48a2-9565-24d037faae4d","_uuid":"24c6bbf235f4bcf8a31f261d0294ecefad1aee01"},"cell_type":"markdown","source":"There is almost no correlation between this two variables."},{"metadata":{"_cell_guid":"abd729c4-ca7f-4c4f-b9e8-0c1324700d26","_uuid":"9be048c522baa114d5998d3c3a04b50c0287ac30"},"cell_type":"markdown","source":"## 2.4. Words features"},{"metadata":{"_cell_guid":"ea72c881-b3ed-453e-bc12-58fe193f6bea","_uuid":"fe1eb036af013ad8285b9f949921d6867d9fe4da","trusted":false,"collapsed":true},"cell_type":"code","source":"%%time\ncv_train_tags = CountVectorizer(ngram_range=(1, 1), min_df=5)\nX_train_tags = cv_train_tags.fit_transform(train.tags.values).toarray()\ncv_test_tags = CountVectorizer(ngram_range=(1, 1), min_df=5)\nX_test_tags = cv_test_tags.fit_transform(test.tags.values).toarray()\n\nmatrix_freq = X_train_tags.sum(axis=0).ravel()\nX_train_freq = np.array([np.array(cv_train_tags.get_feature_names()), matrix_freq])\nmatrix_freq = X_test_tags.sum(axis=0).ravel()\nX_test_freq = np.array([np.array(cv_test_tags.get_feature_names()), matrix_freq])\n\ndf_train_tags = pd.DataFrame()\ndf_train_tags['tag'] = X_train_freq[0]\ndf_train_tags['number_of_posts'] = X_train_freq[1]\ndf_train_tags['mean_claps'] = [0]*len(X_train_freq[1])\ndf_train_tags['sum_claps'] = [0]*len(X_train_freq[1])\n\ndf_test_tags = pd.DataFrame()\ndf_test_tags['tag'] = X_test_freq[0]\ndf_test_tags['number_of_posts'] = X_test_freq[1]\n\ndf=pd.DataFrame(X_train_tags)\ndf['target'] = train.target.values\nfor col in range(df.shape[1]-1):\n    temp=df[df[col]==1]\n    df_train_tags.loc[col,'mean_claps']=temp['target'].mean()\n    df_train_tags.loc[col,'sum_claps']=temp['target'].sum()\n    \ndf_train_tags['tag'] = df_train_tags['tag'].astype(str)\ndf_train_tags['number_of_posts'] = df_train_tags['number_of_posts'].astype(int)\ndf_test_tags['tag'] = df_test_tags['tag'].astype(str)\ndf_test_tags['number_of_posts'] = df_test_tags['number_of_posts'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dd7966b4-5c58-4a6b-a11f-ce14f5a3c6ab","_uuid":"ba0b31a1b8177d6caf0ecbc283568339a07c9093","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.suptitle(\"Top-15 tags by number of occurrences in posts\", fontsize=18)\n\nax1 = plt.subplot2grid((1,2),(0,0))\ntemp = df_train_tags.sort_values(by='number_of_posts', ascending=False).iloc[:15]\nax1 = sns.barplot(temp.tag, temp.number_of_posts, alpha=0.8, color=color[7])\nplt.ylabel('Number of occurrences', fontsize=12)\nplt.xlabel('Tag', fontsize=12)\nplt.title('Train data', fontsize=15)\nplt.grid(False)\nplt.xticks(rotation=90, fontsize=12)\n\nfor p in ax1.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax1.annotate('{}'.format(int(p.get_height())), (x.mean(), y), ha='center', va='bottom')\n    \nax2 = plt.subplot2grid((1,2),(0,1), sharey=ax1)\ntemp = df_test_tags.sort_values(by='number_of_posts', ascending=False).iloc[:15]\nax2 = sns.barplot(temp.tag, temp.number_of_posts, alpha=0.8, color=color[8])\nplt.xlabel('Tag', fontsize=12)\nplt.title('Test data', fontsize=15)\nplt.yticks([])\nplt.ylabel('')\nplt.xticks(rotation=90, fontsize=12)\n\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax2.annotate('{}'.format(int(p.get_height())), (x.mean(), y), ha='center', va='bottom');","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a97c097a-6007-4051-bc77-1e0b13f814ec","_uuid":"617c7ba60e081c5ae95f631b47aebbf67c60d216"},"cell_type":"markdown","source":"Lately, blockchain and bitcoin become more important than politics )"},{"metadata":{"_cell_guid":"e0c64457-b8ec-4dbe-b3f1-c170ca7003b9","_uuid":"f71e55caf75ff207862ef7dd156d62be7151218e","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(18,6))\ntemp = df_train_tags.sort_values(by='sum_claps', ascending=False).iloc[:30]\nax1 = sns.barplot(temp.tag, temp.sum_claps, alpha=0.8, color=color[3])\nplt.ylabel('Overall claps', fontsize=12)\nplt.xlabel('Tag', fontsize=12)\nplt.title('Top-30 tags by total number of claps', fontsize=15)\nplt.grid(False)\nplt.xticks(rotation=90, fontsize=12)\n\nfor p in ax1.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax1.annotate('{}'.format(int(p.get_height())), (x.mean(), y), ha='center', va='bottom')\n    \nplt.figure(figsize=(18,6))\ntemp = df_train_tags.sort_values(by='mean_claps', ascending=False).iloc[:30]\nax2 = sns.barplot(temp.tag, temp.mean_claps, alpha=0.8, color=color[4])\nplt.ylabel('Median of claps by post', fontsize=12)\nplt.xlabel('Tag', fontsize=12)\nplt.title('Top-30 tags by median of claps', fontsize=15)\nplt.grid(False)\nplt.xticks(rotation=90, fontsize=12)\n\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax2.annotate('{}'.format(np.round(p.get_height(),1)), (x.mean(), y), ha='center', va='bottom');","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1937b122-8d81-4c5e-bfc6-f6aa68bf74f8","_uuid":"f69198b5bf16cd367cb135849631d45461c5efa4","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\n\nplt.subplot(121)\nax1 = sns.boxplot(y='target',x='number_of_tags', data=train)\nplt.ylabel('Claps distribution', fontsize=12)\nplt.xlabel('Number of tags in an article', fontsize=12)\nplt.title('Claps distribution across article with different number of tags', fontsize=15)\n\nplt.subplot(122)\ntemp = train.groupby('number_of_tags')['target'].sum()\nax2 = sns.barplot(temp.index,np.round(temp.values))\nplt.ylabel('Number of claps', fontsize=12)\nplt.xlabel('Number of tags in an article', fontsize=12)\nplt.title('Count of claps across article with different number of tags', fontsize=15)\n\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax2.annotate('{}'.format(int(p.get_height())), (x.mean(), y), ha='center', va='bottom');","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"12a6ffd0-86be-4495-961a-a35e8139a1e5","_uuid":"af7d796433e8598aeb4aa9481ba5dfe371e4d736","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.suptitle(\"Count of articles with different number of tags\",fontsize=20)\n\nax1 = plt.subplot2grid((1,2),(0,0))\nax1 = sns.countplot(x='number_of_tags', data=train, alpha=0.8, color=color[1])\nplt.ylabel('Overall articles', fontsize=12)\nplt.xlabel('Number of tags in an article', fontsize=12)\nplt.title('Train data', fontsize=15)\nplt.grid(False)\n\n\nfor p in ax1.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax1.annotate('{}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom')\n    \nax2 = plt.subplot2grid((1,2),(0,1), sharey=ax1)\nax2 = sns.countplot(x='number_of_tags', data=test, alpha=0.8, color=color[2])\nplt.xlabel('Number of tags in an article', fontsize=12)\nplt.title('Test data', fontsize=15)\nplt.yticks([])\nplt.ylabel('')\n\n\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax2.annotate('{}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom');","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"36aa3224-7063-42c0-8b85-f9535d5ce64a","_uuid":"6f135068ef8781e93b2f4118d5959cf6c3983ea3"},"cell_type":"markdown","source":"# 3. Baseline and feature importance"},{"metadata":{"collapsed":true,"_cell_guid":"f3a6e802-8396-45a6-915c-a08dc2bee3b6","_uuid":"367429eb80a24e3ba67bbb3b711426c1b48b2371","trusted":false},"cell_type":"code","source":"content_train = train['content'].values.tolist()\ntitle_train = train['title'].values.tolist()\ntags_train = train['tags'].values.tolist()\ny_train = train['target'].values\ntrain.drop(['content', 'title', 'target', 'tags', 'published', 'length', 'url'], axis=1, inplace=True)\n\ncontent_test = test['content'].values.tolist()\ntitle_test = test['title'].values.tolist()\ntags_test = test['tags'].values.tolist()\ntest.drop(['content', 'title', 'tags', 'published', 'length', 'url'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5faabb03-eff3-46bb-824b-76a893cca63c","_uuid":"583ea6855034ed208a2d3ab6a98e1a97e85cb88e","trusted":false,"collapsed":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e328d046-daad-4ee7-9f1d-dc217d4e735c","_uuid":"082c651754c2881c9288abca34f9be8abea877fa","trusted":false,"collapsed":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ceadddea-609e-4b91-9d39-6663d599e032","_uuid":"058759f8da59cce72d7dcd01143a253bd1b074c0","trusted":false,"collapsed":true},"cell_type":"code","source":"%%time\n\nidx_split = len(train)\ndf_full = pd.concat([train, test])\n\nlist_to_dums = ['author', 'dow', 'month', 'hour', 'domain', 'year']\ndummies = pd.get_dummies(df_full, columns = list_to_dums, drop_first=True,\n                            prefix=list_to_dums, sparse=False)\n\nX_train_feats = dummies.iloc[:idx_split, :]\nX_test_feats = dummies.iloc[idx_split:, :]\n\nprint('TRAIN feats: {}'.format(X_train_feats.shape))\nprint('TEST feats: {}'.format(X_test_feats.shape))\ndel dummies, df_full\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4319e887-e340-49dc-9993-8bfbce580465","_uuid":"99c5b9da9e2c23e9ba20ed0d539a16ce7a6ca59a","trusted":false,"collapsed":true},"cell_type":"code","source":"%%time\n# cv_title = CountVectorizer(max_features=30000)\ncv_content = CountVectorizer(max_features=50000)\ncv_tags = CountVectorizer(max_features=1000)\n\n# X_train_title = cv_title.fit_transform(title_train)\n# X_test_title = cv_title.transform(title_test)\nX_train_content = cv_content.fit_transform(content_train)\nX_test_content = cv_content.transform(content_test)\nX_train_tags = cv_tags.fit_transform(tags_train)\nX_test_tags = cv_tags.transform(tags_test)\n\nprint('TRAIN content: {}, tags: {}'.format(X_train_content.shape, X_train_tags.shape))\nprint('TEST content: {}, tags: {}'.format(X_test_content.shape, X_test_tags.shape))\ndel content_train, content_test, title_train, title_test, tags_train, tags_test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"9a37c1e5-61c3-4f5f-848a-157651845b28","_uuid":"a43591cddb8582b67849df8c09025c502ab9104f","trusted":false},"cell_type":"code","source":"# %%time\n# del train, test\n# X_train_sparse = csr_matrix(hstack([X_train_content, X_train_tags, X_train_feats.values])) \n# X_test_sparse = csr_matrix(hstack([X_test_content, X_test_tags, X_test_feats.values]))\n# print(X_train_sparse.shape, X_test_sparse.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0e2dbf78-a76f-46d0-b61e-18aa4425aabc","_uuid":"94970896a990e6e2f2c2e029f9f548225c299e5b","trusted":false,"collapsed":true},"cell_type":"code","source":"def load_sparse_csr(filename):\n    loader = np.load(filename)\n    return csr_matrix((loader['data'], loader['indices'], loader['indptr']), shape=loader['shape'])\n\nX_train_sparse = load_sparse_csr(os.path.join(PATH, 'mediumeda/train_eda_csr.npz'))\nX_test_sparse = load_sparse_csr(os.path.join(PATH, 'mediumedatest/test_eda_csr.npz'))\nprint(X_train_sparse.shape, X_test_sparse.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a139003d-145b-4d8a-a843-c5449cb6758d","_uuid":"8a4e0aecf01344b2c9c5443d467ff63fb2b92317","trusted":false,"collapsed":true},"cell_type":"code","source":"%%time\ndef write_submission_file(prediction, path_to_sample=os.path.join(PATH, 'how-good-is-your-medium-article/sample_submission.csv')):\n    submission = pd.read_csv(path_to_sample, index_col='id')\n    \n    submission['log_recommends'] = prediction\n    submission.to_csv('submission.csv')\n    \nridge = Ridge(random_state=17)                          \nridge_pred = ridge.fit(X_train_sparse, y_train).predict(X_test_sparse)      \nwrite_submission_file(ridge_pred)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ba9c3ee3-4958-497f-8495-f42fe8f76c46","_uuid":"7d7d3efc7eefa797538af24f436b65244c4f751d","trusted":false,"collapsed":true},"cell_type":"code","source":"top30_plus = np.argsort(ridge.coef_)[-30:][::-1]\ntop30_plus","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0c70447f-d3f6-4214-9be2-f5ca00ec9078","_uuid":"9252aada5ff59e025ffb8e574711509bf143be73","trusted":false,"collapsed":true},"cell_type":"code","source":"top30_minus = np.argsort(ridge.coef_)[:30]\ntop30_minus","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f3975a43-e9f3-4543-8af5-6723858c0da7","_uuid":"242e8a51e3758c134b6d1ffef4e49a798921245e","trusted":false,"collapsed":true},"cell_type":"code","source":"feats_plus=[]\nfeats_minus=[]\nfor idx in top30_plus:\n    if idx<X_train_content.shape[1]:\n        feats_plus.append(list(cv_content.vocabulary_.keys())[list(cv_content.vocabulary_.values()).index(idx)])\n#     elif (idx>=shape(X_train_content)[1] & idx<2*shape(X_train_content)[1]):\n#         feats_plus.append(list(cv_title.vocabulary_.keys())[list(cv_title.vocabulary_.values()).index(idx)])\n    elif idx>=(X_train_content.shape[1]+X_train_tags.shape[1]):\n        feats_plus.append(X_train_feats.columns[idx-(X_train_content.shape[1]+X_train_tags.shape[1])])\n    else:\n        feats_plus.append(list(cv_tags.vocabulary_.keys())[list(cv_tags.vocabulary_.values()).index(idx-X_train_content.shape[1])])\nfor idx in top30_minus:\n    if idx<X_train_content.shape[1]:\n        feats_minus.append(list(cv_content.vocabulary_.keys())[list(cv_content.vocabulary_.values()).index(idx)])\n#     elif (idx>=shape(X_train_content)[1] & idx<2*shape(X_train_content)[1]):\n#         feats_minus.append(list(cv_title.vocabulary_.keys())[list(cv_title.vocabulary_.values()).index(idx)])\n    elif idx>=(X_train_content.shape[1]+X_train_tags.shape[1]):\n        feats_minus.append(X_train_feats.columns[idx-(X_train_content.shape[1]+X_train_tags.shape[1])])\n    else:\n        feats_minus.append(list(cv_tags.vocabulary_.keys())[list(cv_tags.vocabulary_.values()).index(idx-X_train_content.shape[1])])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"511bf920-8309-4eca-b543-a9430cb7d7f9","_uuid":"6ca7c94bdbba1e92a000d4667bca59448be7f319","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(18,6))\nax1 = sns.barplot(feats_plus,ridge.coef_[top30_plus],color=color[2])\nplt.ylabel('Value', fontsize=12)\nplt.xlabel('Feature name', fontsize=12)\nplt.title('Feature importance (positive coefficients)', fontsize=15)\nplt.grid(False)\nplt.xticks(rotation=90, fontsize=12)\n    \nplt.figure(figsize=(18,6))\nax2 = sns.barplot(feats_minus,ridge.coef_[top30_minus],color=color[3])\nplt.ylabel('Value', fontsize=12)\nplt.xlabel('Feature name', fontsize=12)\nplt.title('Feature importance (negative coefficients)', fontsize=15)\nplt.grid(False)\nplt.xticks(rotation=90, fontsize=12);","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}