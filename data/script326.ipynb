{"cells":[{"metadata":{"_cell_guid":"e8b8fa48-afd2-484e-844b-b45093f1b06d","_uuid":"882a3cdde35c17715b89112a5e6f6dadea63ab1f"},"cell_type":"markdown","source":" This kernel is only a  simple lgbm model.  If you find it is useful, please give me an upvote.   The running finished within 1 hour.  I am sure the parameters can be further tuned to get better result.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"5b2aa633-66f2-4a44-8cb8-5afe39f73cf2","collapsed":true,"_uuid":"5e492c083ffbd2d163864e49cbe3d4492ced5196","trusted":false},"cell_type":"code","source":"import lightgbm as lgbm\nfrom sklearn.metrics import mean_squared_error\nfrom scipy import sparse as ssp\nimport random\nimport string\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport time\nimport re\nimport collections\nimport gc\n\n\nt00 = time.time()\n#  stop-word, can add any wording I want to replace\nstopwords=set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \n              'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \n              'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',\n              'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', \n              'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', \n              'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', \n              'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', \n              'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to',\n              'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', \n              'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few',\n              'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n              'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', \n              've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', \n              'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn',\n               '&','brand new','new','\\[rm\\]','free ship.*?','free home',\n               'rm','price firm','no description yet'               \n              ])\n\npattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\ntrain = pd.read_csv('../input/train.tsv', sep=\"\\t\",encoding='utf-8',\n                    converters={'item_description':lambda x:  pattern.sub('',x.lower()),\n                               'name':lambda x:  pattern.sub('',x.lower())}\n                   )\ntest = pd.read_csv('../input/test.tsv', sep=\"\\t\",encoding='utf-8',\n                    converters={'item_description':lambda x:  pattern.sub('',x.lower()),\n                               'name':lambda x:  pattern.sub('',x.lower())}\n                    )\nsimulation=Fa\n\n# ...then introduce random new words\ndef introduce_new_unseen_words(desc):\n    desc = desc.split(' ')\n    if random.randrange(0, 10) == 0: # 10% chance of adding an unseen word\n        new_word = ''.join(random.sample(string.ascii_letters, random.randrange(3, 15)))\n        desc.insert(0, new_word)\n    return ' '.join(desc)\n\nif (simulation==True):\n    test = pd.concat([test.copy(), test.copy(), test.copy(), test.copy(), test.copy()], axis=0)\n    test.item_description = test.item_description.apply(introduce_new_unseen_words)\n\n\ntrain_label = np.log1p(train['price'])\ntrain_texts = train['name'].tolist()\ntest_texts = test['name'].tolist()\nprint('load tsv completed')\n\n# \n#  replace missing word\n# \ntrain['category_name'].fillna('other', inplace=True)\ntest['category_name'].fillna('other', inplace=True)\n\ntrain['brand_name'].fillna('missing', inplace=True)\ntest['brand_name'].fillna('missing', inplace=True)\n\ntest['item_description'].fillna('none', inplace=True)\ntrain['item_description'].fillna('none', inplace=True)\n\ntest['nm_word_len']=list(map(lambda x: len(x.split()), test_texts))\ntrain['nm_word_len']=list(map(lambda x: len(x.split()),train_texts))\ntest['desc_word_len']=list(map(lambda x: len(x.split()), test['item_description'].tolist()))\ntrain['desc_word_len']=list(map(lambda x: len(x.split()), train['item_description'].tolist()))\ntest['nm_len']=list(map(lambda x: len(x),test_texts))\ntrain['nm_len']=list(map(lambda x: len(x),train_texts))\ntest['desc_len']=list(map(lambda x: len(x), test['item_description'].tolist()))\ntrain['desc_len']=list(map(lambda x: len(x), train['item_description'].tolist()))\nnrow_train = train.shape[0]\ntest_id=test['test_id']\n\n\ndef split_cat(text):\n    try:\n        cat_nm=text.split(\"/\")\n        if len(cat_nm)>=3:\n            return cat_nm[0],cat_nm[1],cat_nm[2]\n        if len(cat_nm)==2:\n            return cat_Nm[0],cat_nm[1],'missing'\n        if len(cat_nm)==1:\n            return cat_nm[0],'missing','missing'\n    except: return (\"missing\", \"missing\", \"missing\")\ntrain['subcat_0'], train['subcat_1'], train['subcat_2'] = \\\nzip(*train['category_name'].apply(lambda x: split_cat(x)))\ntest['subcat_0'], test['subcat_1'], test['subcat_2'] = \\\nzip(*test['category_name'].apply(lambda x: split_cat(x)))\n                                 \nNAME_MIN_DF=30\ncount = CountVectorizer(min_df=NAME_MIN_DF)\nX_name_mix = count.fit_transform(train['name'].append(test['name']))\nX_name=X_name_mix[:nrow_train]\nX_t_name = X_name_mix[nrow_train:]\n\n\nMAX_FEATURES_ITEM_DESCRIPTION=25000\ntv = TfidfVectorizer(max_features=MAX_FEATURES_ITEM_DESCRIPTION,\n                         ngram_range=(1,3))\nX_description_mix = tv.fit_transform(train['item_description'].append(test['item_description']))\nX_description=X_description_mix[:nrow_train]\nX_t_description = X_description_mix[nrow_train:]\n\n\nprint('make categorical features')\ncat_features=['subcat_2','subcat_1','subcat_0','brand_name','category_name','item_condition_id','shipping']\nfor c in cat_features:\n    newlist=train[c].append(test[c])\n    le = LabelEncoder()\n    le.fit(newlist)\n    train[c] = le.transform(train[c])\n    test[c] = le.transform(test[c])\nenc = OneHotEncoder()\nenc.fit(train[cat_features].append(test[cat_features]))\nX_cat = enc.transform(train[cat_features])\nX_t_cat = enc.transform(test[cat_features])\n    \ntrain_feature=['desc_word_len','nm_word_len','desc_len','nm_len']\ntrain_list = [train[train_feature].values,X_description,X_name,X_cat]\ntest_list = [test[train_feature].values,X_t_description,X_t_name,X_t_cat]\nX = ssp.hstack(train_list).tocsr()\nX_test = ssp.hstack(test_list).tocsr()\n\nprint (' finish feature for training')\n\nNFOLDS = 3\nkfold =KFold(n_splits=NFOLDS, shuffle=True, random_state=128)\n\n\n\n\nnum_boost_round = 1000\nparams = {\"objective\": \"regression\",\n          \"min_data_in_leaf\":1000,\n          \"boosting_type\": \"gbdt\",\n          \"learning_rate\": 0.65,\n          \"num_leaves\": 128,\n          \"feature_fraction\": 0.5, \n          \"bagging_freq\": 10,\n          \"bagging_fraction\": 0.9,\n          \"tree_learner\":\"voting\",\n          \"verbosity\": 0,\n          \"metric\": \"l2_root\",\n          \"nthread\": 4\n          }\ncv_pred = np.zeros(len(test_id))\nkf = kfold.split(X)\nfor i, (train_fold, test_fold) in enumerate(kf):\n    train_t0 = time.time()\n    X_train, X_validate, label_train, label_validate = \\\n            X[train_fold, :], X[test_fold, :], train_label[train_fold], train_label[test_fold]\n    dtrain = lgbm.Dataset(X_train, label_train)\n    dvalid = lgbm.Dataset(X_validate, label_validate, reference=dtrain)\n    bst = lgbm.train(params, dtrain, num_boost_round, valid_sets=dvalid, verbose_eval=100,early_stopping_rounds=100)\n    cv_pred += bst.predict(X_test, num_iteration=bst.best_iteration)\n    print ('training & predict time',time.time()-train_t0)\n    gc.collect()\n\ncv_pred /= NFOLDS\ncv_pred = np.expm1(cv_pred)\nsubmission = test[[\"test_id\"]]\nsubmission[\"price\"] = cv_pred\nsubmission.to_csv(\"./myNNsubmission.csv\", index=False)\nprint ('overall time',time.time()-t00)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3cfa3351-a801-4743-a2fc-8a6349bcd51f","collapsed":true,"_uuid":"63f8c7b84c10bc9f71a4f60c41a6ebae3fa4014d","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"mimetype":"text/x-python","version":"3.6.4","pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}