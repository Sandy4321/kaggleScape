
# coding: utf-8

# # Where to Start
# **This best starting point for most users is [Kaggle Learn](https://www.kaggle.com/learn)**.  This list was created before Kaggle Learn was launched, but **[this](https://www.kaggle.com/learn)** is the best and most up-to-date resource about learning data science on Kaggle.
# 
# # Python Based
# 
# ### Learn Machine Learning: Intro
# | Name  | Description
# |:----- |:-----|
# | [How Models Work](https://www.kaggle.com/dansbecker/how-models-work)  |  The first step if you are new to machine learning
# | [Starting Your ML Project](https://www.kaggle.com/dansbecker/starting-your-ml-project) | Loading data, and setting up your computing environment for your hands-on project
# | [Selecting and Filtering Data in Pandas](https://www.kaggle.com/dansbecker/selecting-and-filtering-in-pandas) | Getting your data ready for modeling
# | [Running Your First Model](https://www.kaggle.com/dansbecker/your-first-scikit-learn-model) | Building your first model. Hurray!
# | [Model Validation](https://www.kaggle.com/dansbecker/model-validation) | Measuring the performance of your model. This opens up the possibilities for trying and comparing alternative models
# | [Underfitting, Overfitting and Model Optimization](https://www.kaggle.com/dansbecker/underfitting-overfitting-and-model-optimization) | Fine-tune your model for better performance.
# | [Random Forests](https://www.kaggle.com/dansbecker/random-forests) | Using a more sophisticated machine learning algorithm.
# | [Submitting To A Competition](https://www.kaggle.com/dansbecker/submitting-from-a-kernel) | Take pride in what you've built, and start tracking your ongoing progress through a Kaggle Competition.
# 
# 
# ### Learn Machine Learning: Intermediate
# | Name  | Description
# |:----- |:-----|
# | [Handling Missing Values](https://www.kaggle.com/dansbecker/handling-missing-values)  |  Learn multiple approaches for dealing with missing data fields
# | [Using Categorical Data](https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding) | Handle this important but challenging data type
# | [Gradient Boosting with XGBoost](https://www.kaggle.com/dansbecker/learning-to-use-xgboost/) | The most important technique for building high-performance models on conventional data (the type that fits in tables or data frames.
# | [Partial Dependence Plots](https://www.kaggle.com/dansbecker/partial-dependence-plots/) | Extract insights from your models. Insights many didn't even realize were possible.
# | [Scikit-Learn Pipelines](https://www.kaggle.com/dansbecker/pipelines/) | Make your machine learning code cleaner and more professional
# | [Cross-Validation](https://www.kaggle.com/dansbecker/cross-validation) |Improve how you compare and choose models and data preprocessing
# | [Data Leakage](https://www.kaggle.com/dansbecker/data-leakage/) | Identify and avoid one of the most common and costly mistakes in machine learning.
# 
# ### Other
# | Name  | Approx Length (Minutes) | Pre-Reqs | Description
# |:----- |:-----:|:----- |:----- |:-----|
# | [A Whirlwind Tour of Python](https://www.kaggle.com/sohier/whirlwind-tour-of-python-index) | Flexible | | An introduction to the Python language. From excellent book by Jake VanderPlas |
# | [Learn Machine Learning](https://www.kaggle.com/dansbecker/learn-machine-learning) | | **The best starting point for learning machine learning**
# | [Merging Multiple Datasets](https://www.kaggle.com/crawford/python-merge-tutorial) |  30 | Basic knowledge of Pandas | A critical skill for real-world data science 
# | [Pandas Groupby Command](https://www.kaggle.com/crawford/python-groupby-tutorial) |  30 | See statistics for different subgroups in data. Also key to combining different types of data.
# | [Regular Expressions](https://www.kaggle.com/sohier/introduction-to-regular-expressions) | 30 | | A powerful tool for working with text data
# | [Basic Network Analysis](https://www.kaggle.com/crailtap/basic-network-analysis-tutorial) | 60 | Linear Algebra |  Unique branch of data science with applications to social networks, transportation and more.
# | [Starting Kit for PyTorch Deep Learning](https://www.kaggle.com/mratsim/starting-kit-for-pytorch-deep-learning) | 45 | Intro to Data Science | Computer vision (convolutional networks) with PyTorch
# | [Getting Started with TensorFlow](https://www.kaggle.com/fuzzyfroghunter/getting-started-with-tensorflow) | 45 | Intro to Data Science | Computer vision (convolutional networks) with TensorFlow
# 
# ---
# 
# # R Based
# | Name  | Approx Length (Minutes) | Pre-Reqs | Description
# |:----- |:-----:|:----- |:----- |:-----|
# | [Getting staRted in R: First Steps](https://www.kaggle.com/rtatman/getting-started-in-r-first-steps/) | 120 | Minimal programming experience |  Intro to the language, loading data, basic manipulation and graphing
# | [Reading JSON data](https://www.kaggle.com/rtatman/reading-json-data-into-r) | 15 | Getting StaRted in R |  The first step when your data comes in .json files
# | [Joyplots](https://www.kaggle.com/rtatman/joyplots-tutorial-with-insect-data) | 20 | Getting StaRted in R |  Make some very cool looking graphs
# | [Tidy TitaRnic](https://www.kaggle.com/headsortails/tidy-titarnic/notebook) | 30 | Getting StaRted in R | Introduction to the important Tidyverse set of R libraries
# | [Word Tokenization](https://www.kaggle.com/rtatman/tokenization-tutorial) | 20 | Getting StaRted in R |  A first step towards working with text
# | [Getting N-Grams](https://www.kaggle.com/rtatman/tutorial-getting-n-grams) | 30 | Word Tokenization |  Use word groupings for better natural language processing
# | [Sentiment Analysis](https://www.kaggle.com/rtatman/tutorial-sentiment-analysis-in-r/) | 60 | Word Tokenization | Analyze the positive or negative sentiments in text. A key application in natural language processing.
# 
# 
# *Notes*
# - *Many lessons include hands-on coding exercises. Lengths include time to complete exercises at average pace*.
# - *Prior knowledge can replace listed pre-reqs*
# 
# **Want a different approach to learning that jumps in with guided exercises?  Work through the [5-day Challenge](https://www.kaggle.com/rtatman/the-5-day-data-challenge/)**
