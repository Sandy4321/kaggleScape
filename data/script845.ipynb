{"cells":[{"metadata":{"_cell_guid":"96e71462-a750-4d8b-aad3-dacef2a61485","_uuid":"b257b38b8f97f29126238e757834a73331772b9f"},"cell_type":"markdown","source":"**The Choice is Yours**\n\nThis Kernel demonstrates a few feature engineering options, lets get started by importing the python libraries that support comma delimited file data loads for analysis"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport glob\n\ndatafiles = sorted(glob.glob('../input/donorschoose-application-screening/**.csv'))\ndatafiles = {file.split('/')[-1].split('.')[0]: pd.read_csv(file, encoding='latin-1', low_memory=True) for file in datafiles}\nprint([k for k in datafiles])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"11804437-4719-4424-b7ed-e77f37332581","_uuid":"7e4a7e10a85d86082ad0963646d1833f43096eff"},"cell_type":"markdown","source":"Lets review the file contents"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false,"collapsed":true},"cell_type":"code","source":"from IPython.display import display\nfor key in datafiles:\n    print(key, len(datafiles[key]))\n    display(datafiles[key].head(2))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ce7de9f1-a690-473d-9c8a-708d349c3f00","_uuid":"d79f8521fc98109d5d18d7cc3a00c78af14c92f2"},"cell_type":"markdown","source":"Lets utilize a wordcloud to visualize the important keywords in approved project resources"},{"metadata":{"_cell_guid":"f5ae109e-47d9-4972-9a70-afc08e459ab7","_uuid":"12c2be72a2b9e3d47db4c9dd0557be22dda85c54","trusted":false,"collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\nwc = WordCloud(background_color='white', max_words=2000, width=1600, height=1000, stopwords=STOPWORDS)\nwc_string = pd.merge(datafiles['train'], datafiles['resources'], how='left', on='id')[['description','project_is_approved']]\nwc_string = wc_string[wc_string['project_is_approved']==1]['description'].astype(str).values\nwc_string = ' '.join(wc_string)\nwc.generate(wc_string)\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"73619c97-cb89-4b65-8373-bbd8bbdfd0b6","_uuid":"10e8626d59677d1bade5f72cec27608ee059e194"},"cell_type":"markdown","source":"Here we create a few aggregate resource quantity and price sheet features and combine them with the train and test projects\n* It is good practice to address NA values in left joins, an advance feature engineering option here can be to use sklearn.preprocessing.Imputer"},{"metadata":{"_cell_guid":"72224c31-6660-4157-a3b5-81f15e8ae897","_uuid":"d2fe84e7b72e6822d059e8e6d82ec425d75d19b0","trusted":false,"collapsed":true},"cell_type":"code","source":"print(datafiles['train'].shape, datafiles['test'].shape)\ndatafiles['resources']['resources_total'] = datafiles['resources']['quantity'] * datafiles['resources']['price']\ndfr = datafiles['resources'].groupby(['id'], as_index=False)[['resources_total']].sum()\ndatafiles['train'] = pd.merge(datafiles['train'], dfr, how='left', on='id').fillna(-1)\ndatafiles['test'] = pd.merge(datafiles['test'], dfr, how='left', on='id').fillna(-1)\n\ndfr = datafiles['resources'].groupby(['id'], as_index=False)[['resources_total']].mean()\ndfr = dfr.rename(columns={'resources_total':'resources_total_mean'})\ndatafiles['train'] = pd.merge(datafiles['train'], dfr, how='left', on='id').fillna(-1)\ndatafiles['test'] = pd.merge(datafiles['test'], dfr, how='left', on='id').fillna(-1)\n\ndfr = datafiles['resources'].groupby(['id'], as_index=False)[['quantity']].count()\ndfr = dfr.rename(columns={'quantity':'resources_quantity_count'})\ndatafiles['train'] = pd.merge(datafiles['train'], dfr, how='left', on='id').fillna(-1)\ndatafiles['test'] = pd.merge(datafiles['test'], dfr, how='left', on='id').fillna(-1)\n\ndfr = datafiles['resources'].groupby(['id'], as_index=False)[['quantity']].sum()\ndfr = dfr.rename(columns={'quantity':'resources_quantity_sum'})\ndatafiles['train'] = pd.merge(datafiles['train'], dfr, how='left', on='id').fillna(-1)\ndatafiles['test'] = pd.merge(datafiles['test'], dfr, how='left', on='id').fillna(-1)\nprint(datafiles['train'].shape, datafiles['test'].shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"303522ce-7625-4c04-a788-04200cfba707","_uuid":"5a78245d15b8e55e2ec906ea7913c174c3408692"},"cell_type":"markdown","source":"Here we encode categories\n* Advanced encoding options include hashing and weighting"},{"metadata":{"_cell_guid":"d43ab0b1-5331-4931-84d3-2bf1320109c4","_uuid":"a253e80482be72b4deb05882a59ef14e5af2bbd1","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn import *\n\nfor c in ['teacher_id','teacher_prefix','school_state', 'project_grade_category']:\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(datafiles['train'][c].unique())+list(datafiles['test'][c].unique()))\n    datafiles['train'][c] = lbl.fit_transform(datafiles['train'][c].astype(str))\n    datafiles['test'][c] = lbl.fit_transform(datafiles['test'][c].astype(str))\n    print(c)\n\n    for c in ['project_subject_categories', 'project_subject_subcategories']:\n        for i in range(4):\n            lbl = preprocessing.LabelEncoder()\n            labels = list(datafiles['train'][c].unique())+list(datafiles['test'][c].unique())\n            labels = [str(l).split(',')[i] if len(str(l).split(','))>i else '' for l in labels]\n            lbl.fit(labels)\n            datafiles['train'][c + str(i+1)] = lbl.fit_transform(datafiles['train'][c].map(lambda x: str(x).split(',')[i] if len(str(x).split(','))>i else '').astype(str))\n            datafiles['test'][c + str(i+1)] = lbl.fit_transform(datafiles['test'][c].map(lambda x: str(x).split(',')[i] if len(str(x).split(','))>i else '').astype(str))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ce98024f-fc2e-4492-bce4-d5997389bfaa","_uuid":"6e97598733a02bb8e81eaf578ebe1fe1cdba4b98","trusted":false,"collapsed":true},"cell_type":"code","source":"datafiles['test'].head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e4d47fb2-abf0-40f8-8b56-e8f817aea4fb","_uuid":"61e02f26b4fdec52db4b189c5874144a702648b4"},"cell_type":"markdown","source":"Here we add some date features\n* Based on the data description note on essay data, a feature here can also include a value for when the paragraph responses where changed on February 18, 2010"},{"metadata":{"_cell_guid":"3e2095f6-04c9-42f3-93f8-3339cf1b6cc5","_uuid":"cb6b423fba88366d3e71f4b151887ade61b1f032","trusted":false,"collapsed":true},"cell_type":"code","source":"import datetime\n\nfor c in ['project_submitted_datetime']:\n    for t in ['train','test']:\n        datafiles[t][c] = pd.to_datetime(datafiles[t][c])\n        datafiles[t][c+'question_balance'] = (datafiles[t][c] < datetime.date(2010, 2, 18)).astype(np.int)\n        datafiles[t][c+'quarter'] = datafiles[t][c].dt.year\n        datafiles[t][c+'quarter'] = datafiles[t][c].dt.quarter\n        datafiles[t][c+'month'] = datafiles[t][c].dt.month\n        datafiles[t][c+'day'] = datafiles[t][c].dt.day\n        datafiles[t][c+'dow'] = datafiles[t][c].dt.dayofweek\n        datafiles[t][c+'wd'] = datafiles[t][c].dt.weekday\n        datafiles[t][c+'hr'] = datafiles[t][c].dt.hour\n        datafiles[t][c+'m'] = datafiles[t][c].dt.minute\n    print(c)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5010f8ad-d565-4d3a-9936-1b425062ef43","_uuid":"666b9dcc913a1dcef3d6f9542ceeb1fe3961d1b3"},"cell_type":"markdown","source":"Here we add description features including:\n* Word length and count features\n* Term Frequency Inverse Document Frequency features (cutoff at 200 features for performance)\n\nFor advanced options you can explore Natural Language Toolkits which include stemming and tokenizing options"},{"metadata":{"_cell_guid":"cddaa3e6-e7f7-4596-8dca-40fa189a9412","_uuid":"9d585dafc181b58cf08adc2d0c163e41cf2e7a13","trusted":false,"collapsed":true},"cell_type":"code","source":"max_features_ = 200\nprint(datafiles['train'].shape, datafiles['test'].shape)\nfor c in ['project_resource_summary', 'project_title', 'project_essay_1', 'project_essay_2', 'project_essay_3', 'project_essay_4']:\n    tfidf = feature_extraction.text.TfidfVectorizer(ngram_range=(1, 2), stop_words='english', max_df=0.9, min_df=3, max_features=max_features_)\n    tfidf.fit(datafiles['train'][c].astype(str))\n    for t in ['train','test']:\n        datafiles[t][c+'_len'] = datafiles[t][c].map(lambda x: len(str(x)))\n        datafiles[t][c+'_wc'] = datafiles[t][c].map(lambda x: len(str(x).split(' ')))\n        features = pd.DataFrame(tfidf.transform(datafiles[t][c].astype(str)).toarray())\n        features.columns = [c + str(i) for i in range(max_features_)]\n        datafiles[t] = pd.concat((datafiles[t], pd.DataFrame(features)), axis=1, ignore_index=False).reset_index(drop=True)\n    print(c)\nprint(datafiles['train'].shape, datafiles['test'].shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7e261c93-377c-4866-b09d-99cb54c98beb","_uuid":"379230ddb23b31a3dd90360c076cd3ed0f6e1a11"},"cell_type":"markdown","source":"Below are the feature columns that will be used and excluded for training and testing"},{"metadata":{"_cell_guid":"586851ac-0e84-400d-9a2f-2816617fe878","_uuid":"7e498a0fb10962a3544203f1dbcf6b51b63cd22c","collapsed":true,"trusted":false},"cell_type":"code","source":"col = ['id', 'project_is_approved', 'project_resource_summary', 'project_title', 'project_essay_1', 'project_essay_2', 'project_essay_3', 'project_essay_4', 'project_submitted_datetime', 'project_subject_categories', 'project_subject_subcategories']\ncol = [c for c in datafiles['train'].columns if c not in col]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b8875147-801a-436e-931d-dfc88ed2f600","_uuid":"30e728eded8d1584641f4caec67320a012d675ee"},"cell_type":"markdown","source":"Here we use the XGBoost supervised learning model with an AUC metric and Binary Logistic objective\n* The training file is split as a best practice for better validation metric outputs\n* The learning rate (eta) is set to a high 0.1 for performance with an early stopping option 20 rounds"},{"metadata":{"_cell_guid":"56744394-5dab-4bcc-8866-2e3ec0b5befa","_uuid":"8d5025f69bc624ab84055a7edb189f1e5ab8ffe8"},"cell_type":"markdown","source":"Next we blend the results  of a LightGBM supervised learning model with an AUC metric and Binary Logistic objective\n* The training file is split as a best practice for better validation metric outputs\n* The learning rate is set to a high 0.1 for performance with an early stopping option 20 rounds"},{"metadata":{"_cell_guid":"d90215a4-2702-482f-959c-aae81f378ab7","_uuid":"7fdd4930884d290ee7f978c8333a8a39d23f4bc8","trusted":false,"collapsed":true},"cell_type":"code","source":"import lightgbm as lgb\n\nx1, x2, y1, y2 = model_selection.train_test_split(datafiles['train'][col],datafiles['train']['project_is_approved'], test_size=0.20, random_state=19)\n\nparams = {'learning_rate': 0.1, 'max_depth': 7, 'boosting': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'is_training_metric': True, 'seed': 19}\nmodel2 = lgb.train(params, lgb.Dataset(x1, label=y1), 450, lgb.Dataset(x2, label=y2), verbose_eval=10, early_stopping_rounds=20)\ndatafiles['test']['project_is_approved'] = model2.predict(datafiles['test'][col], num_iteration=model2.best_iteration)\ndatafiles['test']['project_is_approved'] = datafiles['test']['project_is_approved'].clip(0+1e12, 1-1e12)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b5e20451-e210-44c5-9d6e-a7d22b62a70e","_uuid":"9c5de2a1f5ee0129d452ece3ed6bae39ef2ec5a7","collapsed":true},"cell_type":"markdown","source":"Here we blend the results of a great [LightGBM and Tf-idf Starter](https://www.kaggle.com/opanichev/lightgbm-and-tf-idf-starter) by Oleg Panichev using the Add Data Source option of Kaggle Kernels for even better results\n"},{"metadata":{"_cell_guid":"e7ec89f3-4502-4b33-b013-5ca1f36e59a5","_uuid":"a2cd1b35659d44fe8a7800b6d2e33556da219f79","collapsed":true,"trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/lightgbm-and-tf-idf-starter/submission.csv')\ndf = pd.merge(datafiles['test'][['id','project_is_approved']], df, on='id')\ndf['project_is_approved'] = (df['project_is_approved_x'] + df['project_is_approved_y']*3) / 4\ndf[['id','project_is_approved']].to_csv('blend_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f8056159-4a8e-48c5-97df-be9df997c74b","_uuid":"484c6fc87745e064d8fd3e9bfbd0e2ef3674bb4b"},"cell_type":"markdown","source":"Enjoy the public leaderboard score and the journey of feature engineering!"},{"metadata":{"_cell_guid":"9114bade-e499-4693-9dfb-5866166eb93b","_uuid":"835993b8c9b167ccd28eb2acf147ff0baace8722"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}