{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "46a8d66e-f820-56c9-f5a6-f1f4c97251d5"
      },
      "source": [
        "I'll be using this notebook as a platform to flex  my newly acquired network analysis muscles. I'll be putting this project together over the next month or so as I learn to use the networkx library in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b0ab2aaa-c31b-a23d-e191-f1b3d2d5f12d"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "87ab967a-b63a-1a5a-1ce0-f7500f9e5b84"
      },
      "source": [
        "This is a monster of a file and it makes my life easier to test out my function-writing skills on smaller pieces of it. I'll use a tiny chunksize to give me something to play with. Once I know exactly how I'm going to get the job done I'll (try to) scale back up to the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9787182d-2647-813d-2763-9944b5a4f84b"
      },
      "outputs": [],
      "source": [
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "chunk = pd.read_csv('../input/emails.csv', chunksize=500)\n",
        "data = next(chunk)\n",
        "\n",
        "data.info()\n",
        "print(data.message[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e151b4fc-84b6-8d32-2255-8dc177236781"
      },
      "source": [
        "Looks like all these messages share the same structure, which is quite convenient. It means I can split the text by newline ('\\n\\') and then simply slice the resulting list to access specific elements of each email. Then I can assign them to new dataframe columns and clean them up for further exploration. \n",
        "\n",
        "Let's write some functions (thank you DataCamp and Hugo Bowne-Anderson)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2acac705-cec2-1f1f-fd97-c0f995d2f76c"
      },
      "outputs": [],
      "source": [
        "def get_text(Series, row_num_slicer):\n",
        "    \"\"\"returns a Series with text sliced from a list split from each message. Row_num_slicer\n",
        "    tells function where to slice split text to find only the body of the message.\"\"\"\n",
        "    result = pd.Series(index=Series.index)\n",
        "    for row, message in enumerate(Series):\n",
        "        message_words = message.split('\\n')\n",
        "        del message_words[:row_num_slicer]\n",
        "        result.iloc[row] = message_words\n",
        "    return result\n",
        "\n",
        "def get_row(Series, row_num):\n",
        "    \"\"\"returns a single row split out from each message. Row_num is the index of the specific\n",
        "    row that you want the function to return.\"\"\"\n",
        "    result = pd.Series(index=Series.index)\n",
        "    for row, message in enumerate(Series):\n",
        "        message_words = message.split('\\n')\n",
        "        message_words = message_words[row_num]\n",
        "        result.iloc[row] = message_words\n",
        "    return result\n",
        "\n",
        "def get_address(df, Series, num_cols=1):\n",
        "    \"\"\"returns a specified email address from each row in a Series\"\"\"\n",
        "    address = re.compile('[\\w\\.-]+@[\\w\\.-]+\\.\\w+')\n",
        "    addresses = []\n",
        "    result1 = pd.Series(index=df.index)\n",
        "    result2 = pd.Series(index=df.index)\n",
        "    result3 = pd.Series(index=df.index)\n",
        "    for i in range(len(df)):\n",
        "        for message in Series:\n",
        "            correspondents = re.findall(address, message)\n",
        "            addresses.append(correspondents)\n",
        "            result1[i] = addresses[i][0]\n",
        "        if num_cols >= 2:\n",
        "            if len(addresses[i]) >= 3:\n",
        "                result2[i] = addresses[i][1]\n",
        "                if num_cols == 3:\n",
        "                    if len(addresses[i]) >= 4:\n",
        "                        result3[i] = addresses[i][2]\n",
        "    return result1, result2, result3\n",
        "\n",
        "def standard_format(df, Series, string, slicer):\n",
        "    \"\"\"Drops rows containing messages without some specified value in the expected locations. \n",
        "    Returns original dataframe without these values. Don't forget to reindex after doing this!!!\"\"\"\n",
        "    rows = []\n",
        "    for row, message in enumerate(Series):\n",
        "        message_words = message.split('\\n')\n",
        "        if string not in message_words[slicer]:\n",
        "            rows.append(row)\n",
        "    df = df.drop(df.index[rows])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "eaa27775-b6d0-b8f0-5348-ef0e2cf3f704"
      },
      "source": [
        "My standard_format() function is meant to get rid of messages that are missing crucial information that I need to do my analysis. For instance several messages have no To: line, which means I have no one to connect these messages to in my network. I need to get rid of these or they will screw up my get_row() function when I try to pull out the To: line information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f35b334d-84ac-311c-c4c8-3caff78727eb"
      },
      "outputs": [],
      "source": [
        "x = len(data.index)\n",
        "headers = ['Message-ID: ', 'Date: ', 'From: ', 'To: ', 'Subject: ']\n",
        "for i, v in enumerate(headers):\n",
        "    data = standard_format(data, data.message, v, i)\n",
        "data = data.reset_index()\n",
        "print(\"Got rid of {} useless emails! That's {}% of the total number of messages in this dataset.\".format(x - len(data.index), np.round(((x - len(data.index)) / x) * 100, decimals=2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d1ba52bd-b7bb-0c4b-279c-f05d127310c2"
      },
      "source": [
        "Got rid of 11 offenders. Not bad! Now to pull out the rest of the data I want to examine. Playing around with my chunksize it looks like when I scale this up I'll lose nearly a quarter of my emails, mostly because they don't have an addressee."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "83c4cb92-889e-3030-5c0c-2c32e457c213"
      },
      "outputs": [],
      "source": [
        "data['text'] = get_text(data.message, 15)\n",
        "data['date'] = get_row(data.message, 1)\n",
        "data['senders'] = get_row(data.message, 2)\n",
        "data['recipients'] = get_row(data.message, 3)\n",
        "data['subject'] = get_row(data.message, 4)\n",
        "\n",
        "data.date = data.date.str.replace('Date: ', '')\n",
        "data.date = pd.to_datetime(data.date)\n",
        "\n",
        "data.subject = data.subject.str.replace('Subject: ', '')\n",
        "\n",
        "data['recipient1'], data['recipient2'], data['recipient3'] = get_address(data, data.recipients, num_cols=3)\n",
        "data['sender'], x, y = get_address(data, data.senders)\n",
        "\n",
        "del data['recipients']\n",
        "del data['senders']\n",
        "del data['file']\n",
        "del data['message']\n",
        "\n",
        "data = data[['date', 'sender', 'recipient1', 'recipient2', 'recipient3', 'subject', 'text']]\n",
        "\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c6dba21e-f582-5eaf-5e33-7ec6b1b86016"
      },
      "source": [
        "OK, looks good. I have what I want to work with now. Obviously I have discarded quite a bit of information as well, but a lot of the content in each message simply doesn't interest me at the moment. I don't care, for instance, how the messages were encoded. So for now I'm happy with what I've got.\n",
        "\n",
        "Let's draw a network. There are quite a few options available, of which I'll plot just three or four. First up...\n",
        "\n",
        "ArcPlot\n",
        "----------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "751e88d8-4c37-1bb5-34df-8b9b8380ff66"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import nxviz as nv\n",
        "\n",
        "G = nx.from_pandas_dataframe(data, 'sender', 'recipient1', edge_attr=['date', 'subject'])\n",
        "plot = nv.ArcPlot(G)\n",
        "plot.draw()\n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e6188cdc-ad19-f910-127c-d2088bc19827"
      },
      "source": [
        "We can see our 500 nodes (employees) at the bottom and the dominance of the one node on the right side.  Please bear in mind, of course, that I am taking the first 500 rows of a 517,000 row dataset here, so we are only looking at emails from one of the 150 Enron executives contained in the full dataset.\n",
        "\n",
        "Next we'll bend the ends of this plot and join them together to form a circle, otherwise known as a \n",
        "\n",
        "CircosPlot\n",
        "----------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2f27143a-756b-0b0c-c224-e74edac53fe3"
      },
      "outputs": [],
      "source": [
        "plot = nv.CircosPlot(G)\n",
        "plot.draw()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3561d963-480d-451f-1a66-62bdd453d89c"
      },
      "source": [
        "Same deal here, although in my opinion a lot clearer and easier to comprehend.\n",
        "\n",
        "Still, this leaves a lot to be desired. It just doesn't *look* like what most folks expect from a network graph. So next we will use nxviz to \n",
        "\n",
        "Draw the Network\n",
        "----------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "72237a95-cc3b-ab1f-d60b-8ae34fdd37b2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "pos = nx.spring_layout(G, k=.1)\n",
        "nx.draw_networkx(G, pos, node_size=25, node_color='red', with_labels=True, edge_color='blue')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9cb4eb78-1267-31f2-2e0b-9ea605a965a7"
      },
      "source": [
        "I plotted this a few times with different \"tension\" on the spring layout to show some different options for visualizing this network. I've been finding that it pays to use a small 'k' value and plot these with a large figsize (200 x 200 or larger). This produces a big enough graphic to allow you to really zoom in and examine minor details without getting overwhelmed by the number of labels. Of course, if you choose to turn the labels off with with_labels=False you can avoid that issue, but this is no fun (and not particularly interesting) without the names attached.\n",
        "\n",
        "Just my opinion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "99d7d627-bb11-306a-52da-5431e2a86048"
      },
      "source": [
        "Next we can look at degree centrality, which basically measures the extent to which a given node is connected to all the other nodes. \n",
        "\n",
        "Degree Centrality\n",
        "-----------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ea03b8a0-4c80-80d7-687a-22edcd4f8ef2"
      },
      "outputs": [],
      "source": [
        "cent = nx.degree_centrality(G)\n",
        "name = []\n",
        "centrality = []\n",
        "\n",
        "for key, value in cent.items():\n",
        "    name.append(key)\n",
        "    centrality.append(value)\n",
        "\n",
        "cent = pd.DataFrame()    \n",
        "cent['name'] = name\n",
        "cent['centrality'] = centrality\n",
        "cent = cent.sort_values(by='centrality', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 25))\n",
        "_ = sns.barplot(x='centrality', y='name', data=cent[:15], orient='h')\n",
        "_ = plt.xlabel('Degree Centrality')\n",
        "_ = plt.ylabel('Correspondent')\n",
        "_ = plt.title('Top 15 Degree Centrality Scores in Enron Email Network')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d46212cb-8400-0839-d223-7f95d408b6c6"
      },
      "source": [
        "As you can see, Mr. Allen is connected to everyone. Not surprising, considering we're only really looking at his emails. Next we can look at betweenness centrality, which measures the degree to which each node is the 'single point of contact', so to speak, between other nodes or cliques.\n",
        "\n",
        "Betweenness Centrality\n",
        "----------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b5bbe46f-b35d-2e7f-fa09-0be73d3d017c"
      },
      "outputs": [],
      "source": [
        "between = nx.betweenness_centrality(G)\n",
        "name = []\n",
        "betweenness = []\n",
        "\n",
        "for key, value in between.items():\n",
        "    name.append(key)\n",
        "    betweenness.append(value)\n",
        "\n",
        "bet = pd.DataFrame()\n",
        "bet['name'] = name\n",
        "bet['betweenness'] = betweenness\n",
        "bet = bet.sort_values(by='betweenness', ascending=False)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 25))\n",
        "_ = sns.barplot(x='betweenness', y='name', data=bet[:15], orient='h')\n",
        "_ = plt.xlabel('Degree Betweenness Centrality')\n",
        "_ = plt.ylabel('Correspondent')\n",
        "_ = plt.title('Top 15 Betweenness Centrality Scores in Hillary Clinton Email Network')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b65581e5-3165-165a-a57d-01a953bca081"
      },
      "source": [
        "Unfortunately that is as far as I can go on this analysis. When I try to scale it up and actually analyze the whole dataset I kill (murder, actually, in cold blood) my kernel. With more computing power, however, I could knock this whole deal out with the same code. So that's something..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b0c405f8-ab46-431c-2b53-53bee13cbca6"
      },
      "source": [
        "Let me know what you think. UPVOTE if you like what you see."
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}