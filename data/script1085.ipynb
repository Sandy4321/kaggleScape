{"metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"version": "3.6.1", "pygments_lexer": "ipython3", "file_extension": ".py", "mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python"}, "anaconda-cloud": {}}, "nbformat": 4, "cells": [{"metadata": {"_uuid": "77c844eccae1919e94ed779f429b5ec05988fbec", "_cell_guid": "2661ec78-7389-4fbc-89b4-9c498a996dbe"}, "source": ["This LightGBM runs on Kaggle as well as on my 3-year old i5/8GB Surface laptop in less than 15 minutes to generate a submission. It has 19 features and produced LB 0.386 (19% when first submitted). Part of it is similar to the many public kernels (such as Fabien Vavrand, Khaled Elshamouty, Paul-Antoine Nguyen, China, etc. These kernels have all in common a global single threshold of around 0.21 and they lead to F1 around 0.375 - 0.38 (perhaps corresponding to the peak in the distribution of LB score just at this level). What is different here is that this kernel runs on Kaggle in the allowed time of 20 minutes using 100% of the data. With a single threshold it also gives 0.38 but when this is relaxed and the calculation is repeated for several thresholds (eg 0.17, 0.21, 0.25) and then a second classifier is applied to train again on the \"train\" data to select the best F1 of the three thresholds for each order we get meaningfull improvement to 0.386 and a jump of 130 places on LB. The new features for the second clf are the mean,max,min of thw reorder probabilities for each of the three thresholds (chosen just for simplicity).\n", "\n", "I started this exercise before I had a chance to look at the theoretical papers on multilabel classification such as Nan Ye at al, Optimizing F-measures, 2012 and Z. Lipton et al, Thresholding classifiers, 2014 that are probably used by people who achieved score over 0.4 (ie Faron's kernel). I entered this competition too late to dig into it deaper. I was initially puzzled by the fact that maximum F1 gives a much bigger cart size (about 8 products instead of 6) - see the chart below. Since the deadline is tomorrow I implemented at least a partial optimization of F1 with three separate thresholds.\n", "\n", "This was my first real Kaggle competition and I have learned great deal doing it - thank you all.\n"], "cell_type": "markdown"}, {"metadata": {"_uuid": "444c60d5d818b00f785962efbf86d58fa180e17b", "_cell_guid": "f17a99d0-84fe-4d92-862f-80d4bdf792cf"}, "source": ["![alt text](F1_vs_mean_cart_size.jpg \"F1 vs Mean Cart Size\")"], "cell_type": "markdown"}, {"metadata": {"_uuid": "51f1abaa042e1a3359c23c64b13cf1a056e4b23d", "_cell_guid": "ccbed9e2-0123-43ce-a11a-a8b45f54f972"}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import gc\n", "import lightgbm as lgb\n", "from sklearn.model_selection import train_test_split\n", "import matplotlib.pyplot as plt\n", "\n", "myfolder = '../input/'\n", "print('loading files ...')\n", "\n", "prior = pd.read_csv(myfolder + 'order_products__prior.csv', dtype={'order_id': np.uint32,\n", "           'product_id': np.uint16, 'reordered': np.uint8, 'add_to_cart_order': np.uint8})\n", "\n", "train_orders = pd.read_csv(myfolder + 'order_products__train.csv', dtype={'order_id': np.uint32,\n", "           'product_id': np.uint16, 'reordered': np.int8, 'add_to_cart_order': np.uint8 })\n", "\n", "orders = pd.read_csv(myfolder + 'orders.csv', dtype={'order_hour_of_day': np.uint8,\n", "           'order_number': np.uint8, 'order_id': np.uint32, 'user_id': np.uint32,\n", "           'order_dow': np.uint8, 'days_since_prior_order': np.float16})\n", "\n", "orders.eval_set = orders.eval_set.replace({'prior': 0, 'train': 1, 'test':2}).astype(np.uint8)\n", "orders.days_since_prior_order = orders.days_since_prior_order.fillna(30).astype(np.uint8)\n", "\n", "products = pd.read_csv(myfolder + 'products.csv', dtype={'product_id': np.uint16,\n", "            'aisle_id': np.uint8, 'department_id': np.uint8},\n", "             usecols=['product_id', 'aisle_id', 'department_id'])\n", "\n", "print('done loading')"], "cell_type": "code", "execution_count": 2}, {"metadata": {}, "outputs": [], "source": ["print('merge prior and orders and keep train separate ...')\n", "\n", "orders_products = orders.merge(prior, how = 'inner', on = 'order_id')\n", "train_orders = train_orders.merge(orders[['user_id','order_id']], left_on = 'order_id', right_on = 'order_id', how = 'inner')\n", "\n", "del prior\n", "gc.collect()"], "cell_type": "code", "execution_count": 3}, {"metadata": {}, "outputs": [], "source": ["print('Creating features I ...')\n", "\n", "# sort orders and products to get the rank or the reorder frequency\n", "prdss = orders_products.sort_values(['user_id', 'order_number', 'product_id'], ascending=True)\n", "prdss['product_time'] = prdss.groupby(['user_id', 'product_id']).cumcount()+1\n", "\n", "# getting products ordered first and second times to calculate probability later\n", "sub1 = prdss[prdss['product_time'] == 1].groupby('product_id').size().to_frame('prod_first_orders')\n", "sub2 = prdss[prdss['product_time'] == 2].groupby('product_id').size().to_frame('prod_second_orders')\n", "sub1['prod_orders'] = prdss.groupby('product_id')['product_id'].size()\n", "sub1['prod_reorders'] = prdss.groupby('product_id')['reordered'].sum()\n", "sub2 = sub2.reset_index().merge(sub1.reset_index())\n", "sub2['prod_reorder_probability'] = sub2['prod_second_orders']/sub2['prod_first_orders']\n", "sub2['prod_reorder_ratio'] = sub2['prod_reorders']/sub2['prod_orders']\n", "prd = sub2[['product_id', 'prod_orders','prod_reorder_probability', 'prod_reorder_ratio']]\n", "\n", "del sub1, sub2, prdss\n", "gc.collect()"], "cell_type": "code", "execution_count": 4}, {"metadata": {}, "outputs": [], "source": ["print('Creating features II ...')\n", "\n", "# extracting prior information (features) by user\n", "users = orders[orders['eval_set'] == 0].groupby(['user_id'])['order_number'].max().to_frame('user_orders')\n", "users['user_period'] = orders[orders['eval_set'] == 0].groupby(['user_id'])['days_since_prior_order'].sum()\n", "users['user_mean_days_since_prior'] = orders[orders['eval_set'] == 0].groupby(['user_id'])['days_since_prior_order'].mean()\n", "\n", "# merging features about users and orders into one dataset\n", "us = orders_products.groupby('user_id').size().to_frame('user_total_products')\n", "us['eq_1'] = orders_products[orders_products['reordered'] == 1].groupby('user_id')['product_id'].size()\n", "us['gt_1'] = orders_products[orders_products['order_number'] > 1].groupby('user_id')['product_id'].size()\n", "us['user_reorder_ratio'] = us['eq_1'] / us['gt_1']\n", "us.drop(['eq_1', 'gt_1'], axis = 1, inplace = True)\n", "us['user_distinct_products'] = orders_products.groupby(['user_id'])['product_id'].nunique()\n", "\n", "# the average basket size of the user\n", "users = users.reset_index().merge(us.reset_index())\n", "users['user_average_basket'] = users['user_total_products'] / users['user_orders']\n", "\n", "us = orders[orders['eval_set'] != 0]\n", "us = us[['user_id', 'order_id', 'eval_set', 'days_since_prior_order']]\n", "users = users.merge(us)\n", "\n", "del us\n", "gc.collect()"], "cell_type": "code", "execution_count": 5}, {"metadata": {}, "outputs": [], "source": ["print('Finalizing features and the main data file  ...')\n", "# merging orders and products and grouping by user and product and calculating features for the user/product combination\n", "data = orders_products.groupby(['user_id', 'product_id']).size().to_frame('up_orders')\n", "data['up_first_order'] = orders_products.groupby(['user_id', 'product_id'])['order_number'].min()\n", "data['up_last_order'] = orders_products.groupby(['user_id', 'product_id'])['order_number'].max()\n", "data['up_average_cart_position'] = orders_products.groupby(['user_id', 'product_id'])['add_to_cart_order'].mean()\n", "data = data.reset_index()\n", "\n", "#merging previous data with users\n", "data = data.merge(prd, on = 'product_id')\n", "data = data.merge(users, on = 'user_id')\n", "\n", "#user/product combination features about the particular order\n", "data['up_order_rate'] = data['up_orders'] / data['user_orders']\n", "data['up_orders_since_last_order'] = data['user_orders'] - data['up_last_order']\n", "data = data.merge(train_orders[['user_id', 'product_id', 'reordered']], \n", "                  how = 'left', on = ['user_id', 'product_id'])\n", "data = data.merge(products, on = 'product_id')\n", "\n", "del orders_products     #, orders, train_orders\n", "gc.collect()"], "cell_type": "code", "execution_count": 6}, {"metadata": {}, "outputs": [], "source": ["print(' Training and test data for later use in F1 optimization and training  ...')\n", "\n", "#save the actual reordered products of the train set in a list format and then delete the original frames\n", "train_orders = train_orders[train_orders['reordered']==1].drop('reordered',axis=1)\n", "orders.set_index('order_id', drop=False, inplace=True)\n", "train1=orders[['order_id','eval_set']].loc[orders['eval_set']==1]\n", "train1['actual'] = train_orders.groupby('order_id').aggregate({'product_id':lambda x: list(x)})\n", "train1['actual']=train1['actual'].fillna('')\n", "n_actual = train1['actual'].apply(lambda x: len(x)).mean()   # this is the average cart size\n", "\n", "test1=orders[['order_id','eval_set']].loc[orders['eval_set']==2]\n", "test1['actual']=' '\n", "traintest1=pd.concat([train1,test1])\n", "traintest1.set_index('order_id', drop=False, inplace=True)\n", "\n", "del orders, train_orders, train1, test1\n", "gc.collect()"], "cell_type": "code", "execution_count": 7}, {"metadata": {}, "outputs": [], "source": ["print('setting dtypes for data ...')\n", "\n", "#reduce the size by setting data types\n", "data = data.astype(dtype= {'user_id' : np.uint32, 'product_id'  : np.uint16,\n", "            'up_orders'  : np.uint8, 'up_first_order' : np.uint8, 'up_last_order' : np.uint8,\n", "            'up_average_cart_position' : np.uint8, 'prod_orders' : np.uint16, \n", "            'prod_reorder_probability' : np.float16,   \n", "            'prod_reorder_ratio' : np.float16, 'user_orders' : np.uint8,\n", "            'user_period' : np.uint8, 'user_mean_days_since_prior' : np.uint8,\n", "            'user_total_products' : np.uint8, 'user_reorder_ratio' : np.float16, \n", "            'user_distinct_products' : np.uint8, 'user_average_basket' : np.uint8,\n", "            'order_id'  : np.uint32, 'eval_set' : np.uint8, \n", "            'days_since_prior_order' : np.uint8, 'up_order_rate' : np.float16, \n", "            'up_orders_since_last_order':np.uint8,\n", "            'aisle_id': np.uint8, 'department_id': np.uint8})\n", "\n", "data['reordered'].fillna(0, inplace=True)  # replace NaN with zeros (not reordered) \n", "data['reordered']=data['reordered'].astype(np.uint8)\n", "\n", "gc.collect()"], "cell_type": "code", "execution_count": 8}, {"metadata": {}, "outputs": [], "source": ["print('Preparing Train and Test sets ...')\n", "\n", "# filter by eval_set (train=1, test=2) and dropp the id's columns (not part of training features) \n", "# but keep prod_id and user_id in test\n", "\n", "train = data[data['eval_set'] == 1].drop(['eval_set', 'user_id', 'product_id', 'order_id'], axis = 1)\n", "test =  data[data['eval_set'] == 2].drop(['eval_set', 'user_id', 'reordered'], axis = 1)\n", "\n", "check =  data.drop(['eval_set', 'user_id', 'reordered'], axis = 1)\n", "\n", "del data\n", "gc.collect()"], "cell_type": "code", "execution_count": 9}, {"metadata": {}, "outputs": [], "source": ["print('preparing X,y for LightGBM ...')\n", "\n", "# for preliminary runs sample a fraction of the data by (un)commenting the next two lines\n", "#print('sampling train data ...')\n", "#train = train.sample(frac=0.25)\n", "\n", "# Splitting the training set to train and validation set\n", "X_train, X_eval, y_train, y_eval = train_test_split(\n", "    train[train.columns.difference(['reordered'])], train['reordered'], test_size=0.1, random_state=2)\n", "\n", "del train\n", "gc.collect()"], "cell_type": "code", "execution_count": 10}, {"metadata": {}, "outputs": [], "source": ["print('formatting and training LightGBM ...')\n", "\n", "lgb_train = lgb.Dataset(X_train, label=y_train)\n", "lgb_eval = lgb.Dataset(X_eval, y_eval, reference = lgb_train)\n", "\n", "# there is some room to change the parameters and improve - I have not done it systematically\n", "\n", "params = {'task': 'train', 'boosting_type': 'gbdt',   'objective': 'binary', 'metric': {'binary_logloss', 'auc'},\n", "    'num_iterations' : 1000, 'max_bin' : 100, 'num_leaves': 512, 'feature_fraction': 0.8,  'bagging_fraction': 0.95,\n", "    'bagging_freq': 5, 'min_data_in_leaf' : 200, 'learning_rate' : 0.05}\n", "\n", "#lgb_model = lgb.train(params, lgb_train, num_boost_round = 300, valid_sets = lgb_eval, early_stopping_rounds=10)\n", "# set lower num_boost_round to avoid tmout on Kaggle\n", "\n", "lgb_model = lgb.train(params, lgb_train, num_boost_round = 50, valid_sets = lgb_eval, early_stopping_rounds=10)\n", "\n", "del lgb_train, X_train, y_train\n", "gc.collect()\n"], "cell_type": "code", "execution_count": 11}, {"metadata": {"collapsed": true}, "outputs": [], "source": ["# Define an auxiliary function to combine the product data into orders\n", "\n", "def combi(z,df):\n", "    \n", "    prd_bag = dict()\n", "    z_bag = dict()\n", "    for row in df.itertuples():\n", "        if row.reordered > z:   \n", "            try:\n", "                prd_bag[row.order_id] += ' ' + str(row.product_id)\n", "                z_bag[row.order_id]+= ' ' + str(int(100*row.reordered))\n", "            except:\n", "                prd_bag[row.order_id] = str(row.product_id)\n", "                z_bag[row.order_id]= str(int(100*row.reordered))\n", "\n", "    for order in df.order_id:\n", "        if order not in prd_bag:\n", "            prd_bag[order] = ' '\n", "            z_bag[order] = ' '\n", "\n", "    return prd_bag,z_bag \n", "\n", "# F1 function uses the actual products as a list in the train set and the list of predicted products\n", "\n", "def f1_score_single(x):                 #from LiLi but modified to get 1 for both empty\n", "\n", "    y_true = x.actual\n", "    y_pred = x.list_prod\n", "    if y_true == '' and y_pred ==[] : return 1.\n", "    y_true = set(y_true)\n", "    y_pred = set(y_pred)\n", "    cross_size = len(y_true & y_pred)\n", "    if cross_size == 0: return 0.\n", "    p = 1. * cross_size / len(y_pred)\n", "    r = 1. * cross_size / len(y_true)\n", "    return 2 * p * r / (p + r)"], "cell_type": "code", "execution_count": 12}, {"metadata": {"collapsed": true}, "outputs": [], "source": ["# check feature importance\n", "#lgb.plot_importance(lgb_model, figsize=(7,9))\n", "#plt.show()"], "cell_type": "code", "execution_count": 13}, {"metadata": {}, "outputs": [], "source": ["print(' Applying model to all data - both train and test ')\n", "\n", "\n", "check['reordered'] = lgb_model.predict(check[check.columns.difference(\n", "    ['order_id', 'product_id'])], num_iteration = lgb_model.best_iteration)\n", "\n", "gc.collect()"], "cell_type": "code", "execution_count": 14}, {"metadata": {}, "source": ["The next step is to chose a threshold to select the reordered products. We know that a single global threshold of 0.21 maximizes F1 (given that the max is about 0.4 - see also the chart at the begining or end). The method here is to use several (eg 3 such as 0.17,0.21,0.25) thresholds and recalculate F1 on the training set. In addition to the products for each order we will collect the probabilities and store them in list and calculate the mean, max and min as our new features. Then we set up a new classification problem with the target [0,1, or 2] corresponding to which threshold maximizes F1. Alltogether we will use 9 features (we could easily generalize to more thresholds and features) .  This is relatively easy problem and we use Gradient Boosting Classifier with default parameters. "], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["print(' summarizing products and probabilities ...')\n", "\n", "# get the prediction for a range of thresholds\n", "\n", "tt=traintest1.copy()\n", "i=0\n", "\n", "for z in [0.17, 0.21, 0.25]:\n", "    \n", "    prd_bag,z_bag = combi(z,check)\n", "    ptemp = pd.DataFrame.from_dict(prd_bag, orient='index')\n", "    ptemp.reset_index(inplace=True)\n", "    ztemp = pd.DataFrame.from_dict(z_bag, orient='index')\n", "    ztemp.reset_index(inplace=True)\n", "    ptemp.columns = ['order_id', 'products']\n", "    ztemp.columns = ['order_id', 'zs']\n", "    ptemp['list_prod'] = ptemp['products'].apply(lambda x: list(map(int, x.split())))\n", "    ztemp['list_z'] = ztemp['zs'].apply(lambda x: list(map(int, x.split())))\n", "    n_cart = ptemp['products'].apply(lambda x: len(x.split())).mean()\n", "    tt = tt.merge(ptemp,on='order_id',how='inner')\n", "    tt = tt.merge(ztemp,on='order_id',how='inner')\n", "    tt.drop(['products','zs'],axis=1,inplace=True)\n", "    tt['zavg'] = tt['list_z'].apply(lambda x: 0.01*np.mean(x) if x!=[] else 0.).astype(np.float16)\n", "    tt['zmax'] = tt['list_z'].apply(lambda x: 0.01*np.max(x) if x!=[] else 0.).astype(np.float16)\n", "    tt['zmin'] = tt['list_z'].apply(lambda x: 0.01*np.min(x) if x!=[] else 0.).astype(np.float16)\n", "    tt['f1']=tt.apply(f1_score_single,axis=1).astype(np.float16)\n", "    F1 = tt['f1'].loc[tt['eval_set']==1].mean()\n", "    tt = tt.rename(columns={'list_prod': 'prod'+str(i), 'f1': 'f1'+str(i), 'list_z': 'z'+str(i),\n", "                'zavg': 'zavg'+str(i), 'zmax': 'zmax'+str(i),  'zmin': 'zmin'+str(i)})\n", "    print(' z,F1,n_actual,n_cart :  ', z,F1,n_actual,n_cart)\n", "    i=i+1\n", "\n", "tt['fm'] = tt[['f10', 'f11', 'f12']].idxmax(axis=1)\n", "tt['f1'] = tt[['f10', 'f11', 'f12']].max(axis=1)\n", "tt['fm'] = tt.fm.replace({'f10': 0,'f11': 1, 'f12':2}).astype(np.uint8)\n", "print(' f1 maximized ', tt['f1'].loc[tt['eval_set']==1].mean())\n", "    \n", "del prd_bag, z_bag, ptemp, ztemp\n", "gc.collect()"], "cell_type": "code", "execution_count": 15}, {"metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import GradientBoostingClassifier\n", "from sklearn import metrics\n", "\n", "print('Fitting the second classifier for F1 ...')\n", "\n", "X=tt[[ 'zavg0', 'zmax0','zmin0', 'zavg1', 'zmax1', 'zmin1', 'zavg2', 'zmax2', 'zmin2']].loc[tt['eval_set']==1]\n", "y=tt['fm'].loc[tt['eval_set']==1]\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n", "\n", "clf = GradientBoostingClassifier().fit(X_train, y_train)\n", "print('GB Accuracy on training set: {:.2f}' .format(clf.score(X_train, y_train)))\n", "print('Accuracy on test set: {:.2f}' .format(clf.score(X_test, y_test)))\n", "#pd.DataFrame(clf.feature_importances_, index=X_train.columns, columns=[\"Importance\"]).plot(kind='bar')\n", "#plt.show()\n", "\n", "final=tt[['order_id','prod0','prod1','prod2']].loc[tt['eval_set']==2]\n", "df_test=tt[[ 'zavg0', 'zmax0','zmin0', 'zavg1', 'zmax1', 'zmin1', 'zavg2', 'zmax2', 'zmin2']].loc[tt['eval_set']==2]\n", "final['fit']= clf.predict(df_test)\n", "final['best'] = final.apply(lambda row: row['prod0'] if row['fit']==0 else \n", "                                 ( row['prod1'] if row['fit']==1 else  row['prod2'] )  , axis=1)\n", "\n", "final['products']=final['best'].apply(lambda x: ' '.join(str(i) for i in x) if x!=[] else 'None')\n", "final[['order_id','products']].to_csv('final_submission1.csv', index=False)  \n", "\n", "gc.collect()"], "cell_type": "code", "execution_count": 16}, {"metadata": {}, "outputs": [], "source": ["#I saved one of the previous runs so that it is not timed out on Kaggle\n", "X=np.arange(0.12,0.31,0.01)\n", "Y2 = np.empty(19)\n", "Y2.fill(6.31)\n", "Y1=[ 0.3701,0.3757,0.38,0.3839,0.3867,0.3886,0.3897,0.3905,0.3906,0.3903,\n", "    0.3892,0.3877,0.3857,0.3834,0.3808,0.3779,0.3746,0.371,0.3669]\n", "Y3=[ 15.45,14.29,13.26,12.34,11.51,10.76,10.09,9.47,8.91,8.39,7.92,7.49,\n", "    7.08,6.7,6.35,6.03,5.72,5.43,5.16]\n", "#replace X,Y1,Y2,Y3 with arrays from z,F1,n_actual,n_cart to update (running the above cell for the corresponding rane of z's)\n", "\n", "plt.clf()\n", "fig = plt.figure()\n", "ax = fig.add_subplot(111)\n", "lns1 = ax.plot(X, Y2, '-', label = 'Actual')\n", "lns2 = ax.plot(X, Y3, '-', label = 'Predicted')\n", "ax2 = ax.twinx()\n", "lns3 = ax2.plot(X, Y1, '-r', label = 'F1')\n", "lns = lns1+lns2+lns3\n", "labs = [l.get_label() for l in lns]\n", "ax.legend(lns, labs, loc=0)\n", "ax.set_xlabel('Threshold')\n", "ax.set_ylabel('Mean Cart Size')\n", "ax2.set_ylabel('F1')\n", "plt.suptitle('F1 vs Mean Cart Size', size=12)\n", "plt.savefig('F1_vs_mean_cart_size.jpg')\n", "plt.show()"], "cell_type": "code", "execution_count": 17}, {"metadata": {"collapsed": true}, "outputs": [], "source": [], "cell_type": "code", "execution_count": null}], "nbformat_minor": 1}