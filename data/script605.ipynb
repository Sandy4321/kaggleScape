{"nbformat": 4, "cells": [{"cell_type": "markdown", "source": ["This notebook demonstrates a custom loss function for neural nets, that provides a differentiable approximation to AUC. AUC, in turn, has a linear relationship with Gini, hence this is very useful when we want to train a network to maximise AUC.\n", "\n", "We set up 2 identical NNs and run them for a few epochs, to show how this approach improves convergence on AUC compared to binary crossentropy.\n", "\n", "I've used this to get a network that has a local CV AUC around 0.642, which corresponds to Gini of 0.284. The performance on the LB test set is considerably worse (around 0.276)\n", "\n", "This is hacked together from various bits of my local code, and hasn't been thoroughly tested, so let please me know of any bugs etc.\n", "\n", "I would have coded as a script, but I need to use the Theano backend as the AUC function uses Theano specific code. If anyone knows how to make Kaggle Kernels use the Theano backend for script, let me know.\n", "\n", "First of all, imports and constants"], "metadata": {}}, {"outputs": [], "metadata": {"_uuid": "08f2a9a1f9347b4a3694e00d066919eb8775cd87", "_cell_guid": "4d2257b7-b397-4709-8483-34f32e2ba805"}, "cell_type": "code", "source": ["import numpy as np\n", "import pandas as pd\n", "\n", "%env KERAS_BACKEND=theano\n", "\n", "from keras.models import Sequential\n", "from keras.layers import Dropout\n", "from keras.layers.normalization import BatchNormalization\n", "from keras import regularizers\n", "from keras.layers import Dense\n", "from keras.optimizers import Adam\n", "from keras.utils import custom_object_scope\n", "from keras import callbacks\n", "\n", "from sklearn.metrics import roc_auc_score\n", "from sklearn import preprocessing\n", "\n", "import theano\n", "\n", "# train and test data path\n", "#DATA_TRAIN_PATH = '../input/train.csv'\n", "#DATA_TEST_PATH = '../input/test.csv'\n", "\n", "DATA_TRAIN_PATH = 'c:\\\\projects\\\\psdriver\\\\data\\\\train.csv'\n", "DATA_TEST_PATH = 'c:\\\\projects\\\\psdriver\\\\data\\\\test.csv'\n", "\n", "\n", "featuresToDrop = [\n", "    'ps_calc_10',\n", "    'ps_calc_01',\n", "    'ps_calc_02',\n", "    'ps_calc_03',\n", "    'ps_calc_13',\n", "    'ps_calc_08',\n", "    'ps_calc_07',\n", "    'ps_calc_12',\n", "    'ps_calc_04',\n", "    'ps_calc_17_bin',\n", "    'ps_car_10_cat',\n", "    'ps_car_11_cat',\n", "    'ps_calc_14',\n", "    'ps_calc_11',\n", "    'ps_calc_06',\n", "    'ps_calc_16_bin',\n", "    'ps_calc_19_bin',\n", "    'ps_calc_20_bin',\n", "    'ps_calc_15_bin',\n", "    'ps_ind_11_bin',\n", "    'ps_ind_10_bin'\n", "]\n", "\n"], "execution_count": 1}, {"cell_type": "markdown", "source": ["Now, the secret sauce"], "metadata": {}}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["\n", "# An analogue to AUC which takes the differences between each pair of true/false predictions\n", "# and takes the average sigmoid of the differences to get a differentiable loss function.\n", "# Based on code and ideas from https://github.com/Lasagne/Lasagne/issues/767\n", "def soft_AUC_theano(y_true, y_pred):\n", "    # Extract 1s\n", "    pos_pred_vr = y_pred[y_true.nonzero()]\n", "    # Extract zeroes\n", "    neg_pred_vr = y_pred[theano.tensor.eq(y_true, 0).nonzero()]\n", "    # Broadcast the subtraction to give a matrix of differences  between pairs of observations.\n", "    pred_diffs_vr = pos_pred_vr.dimshuffle(0, 'x') - neg_pred_vr.dimshuffle('x', 0)\n", "    # Get signmoid of each pair.\n", "    stats = theano.tensor.nnet.sigmoid(pred_diffs_vr * 2)\n", "    # Take average and reverse sign\n", "    return 1-theano.tensor.mean(stats) # as we want to minimise, and get this to zero\n", "\n", "\n", "# This callback records the SKLearn calculated AUC each round, for use by early stopping\n", "# It also has slots where you can save down metadata or the model at useful points -\n", "# for Kaggle kernel purposes I've commented these out\n", "class AUC_SKlearn_callback(callbacks.Callback):\n", "    def __init__(self, X_train, y_train, useCv = True):\n", "        super(AUC_SKlearn_callback, self).__init__()\n", "        self.bestAucCv = 0\n", "        self.bestAucTrain = 0\n", "        self.cvLosses = []\n", "        self.bestCvLoss = 1,\n", "        self.X_train = X_train\n", "        self.y_train = y_train\n", "        self.useCv = useCv\n", "\n", "    def on_train_begin(self, logs={}):\n", "        return\n", "\n", "    def on_train_end(self, logs={}):\n", "        return\n", "\n", "    def on_epoch_begin(self, epoch, logs={}):\n", "        return\n", "\n", "    def on_epoch_end(self, epoch, logs={}):\n", "        train_pred = self.model.predict(np.array(self.X_train))\n", "        aucTrain = roc_auc_score(self.y_train, train_pred)\n", "        print(\"SKLearn Train AUC score: \" + str(aucTrain))\n", "\n", "        if (self.bestAucTrain < aucTrain):\n", "            self.bestAucTrain = aucTrain\n", "            print (\"Best SKlearn AUC training score so far\")\n", "            #**TODO: Add your own logging/saving/record keeping code here\n", "\n", "        if (self.useCv) :\n", "            cv_pred = self.model.predict(self.validation_data[0])\n", "            aucCv = roc_auc_score(self.validation_data[1], cv_pred)\n", "            print (\"SKLearn CV AUC score: \" +  str(aucCv))\n", "\n", "            if (self.bestAucCv < aucCv) :\n", "                # Great! New best *actual* CV AUC found (as opposed to the proxy AUC surface we are descending)\n", "                print(\"Best SKLearn genuine AUC so far so saving model\")\n", "                self.bestAucCv = aucCv\n", "\n", "                # **TODO: Add your own logging/model saving/record keeping code here.\n", "                self.model.save(\"best_auc_model.h5\", overwrite=True)\n", "\n", "            vl = logs.get('val_loss')\n", "            if (self.bestCvLoss < vl) :\n", "                print(\"Best val loss on SoftAUC so far\")\n", "                #**TODO -  Add your own logging/saving/record keeping code here.\n", "        return\n", "\n", "    def on_batch_begin(self, batch, logs={}):\n", "        return\n", "\n", "    def on_batch_end(self, batch, logs={}):\n", "        # logs include loss, and optionally acc( if accuracy monitoring is enabled).\n", "        return\n", "\n", "\n", "# Create the model.\n", "def create_model_AUC(input_dim, first_layer_size, second_layer_size, third_layer_size, lr, l2reg, dropout):\n", "    return create_model(input_dim, first_layer_size, second_layer_size, third_layer_size, lr, l2reg, dropout, \"AUC\")\n", "\n", "def create_model_bce(input_dim, first_layer_size, second_layer_size, third_layer_size, lr, l2reg, dropout):\n", "    return create_model(input_dim, first_layer_size, second_layer_size, third_layer_size, lr, l2reg, dropout, \"crossentropy\")\n", "\n", "\n", "def create_model(input_dim, first_layer_size, second_layer_size, third_layer_size, lr, l2reg, dropout, mode=\"AUC\") :\n", "    print(\"Creating model with input dim \", input_dim)\n", "    # likely to need tuning!\n", "    reg = regularizers.l2(l2reg)\n", "\n", "    model = Sequential()\n", "\n", "    model.add(Dense(units=first_layer_size, kernel_initializer='lecun_normal', kernel_regularizer=reg, activation='relu', input_dim=input_dim))\n", "    model.add(BatchNormalization())\n", "    model.add(Dropout(dropout))\n", "\n", "    model.add(Dense(units=second_layer_size, kernel_initializer='lecun_normal', activation='relu', kernel_regularizer=reg))\n", "    model.add(BatchNormalization(axis=1))\n", "    model.add(Dropout(dropout))\n", "\n", "    model.add(Dense(units=third_layer_size, kernel_initializer='lecun_normal', activation='relu', kernel_regularizer=reg))\n", "    model.add(BatchNormalization())\n", "    model.add(Dropout(dropout))\n", "\n", "    model.add(Dense(1, kernel_initializer='lecun_normal', activation='sigmoid'))\n", "\n", "    # classifier.compile(loss='mean_absolute_error', optimizer='rmsprop', metrics=['mae', 'accuracy'])\n", "    opt = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n", "    if (mode == \"AUC\"):\n", "        model.compile(loss=soft_AUC_theano, metrics=[soft_AUC_theano], optimizer=opt)  # not sure whether to use metrics here?\n", "    else:\n", "        model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=opt)  # not sure whether to use metrics here?\n", "    return model\n", "\n", "\n", "def train_model( X_train, y_train, model, valSplit=0.15, epochs = 5, batch_size = 4096):\n", "\n", "    callbacksList = [AUC_SKlearn_callback(X_train, y_train, useCv = (valSplit > 0))]\n", "    if (valSplit > 0) :\n", "        early_stopping = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=5,\n", "                                                       verbose=0, mode='min')\n", "        callbacksList.append( early_stopping )\n", "    return model.fit(x=np.array(X_train), y=np.array(y_train),\n", "                        callbacks=callbacksList, validation_split=valSplit,\n", "                        verbose=2, batch_size=batch_size, epochs=epochs)\n", "\n", "\n", "\n", "def scale_features(df_for_range, df_to_scale, columnsToScale) :\n", "    # Scale columnsToScale in df_to_scale\n", "    columnsOut = list(map( (lambda x: x + \"_scaled\"), columnsToScale))\n", "    for c, co in zip(columnsToScale, columnsOut) :\n", "        scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n", "        print(\"scaling \", c ,\" to \",co)\n", "        vals = df_for_range[c].values.reshape(-1, 1)\n", "        scaler.fit(vals )\n", "        df_to_scale[co]=scaler.transform(df_to_scale[c].values.reshape(-1,1))\n", "\n", "    df_to_scale.drop (columnsToScale, axis=1, inplace = True)\n", "\n", "    return df_to_scale\n", "\n", "\n", "def one_hot (df, cols):\n", "    # One hot cols requested, drop original cols, return df\n", "    df = pd.concat([df, pd.get_dummies(df[cols], columns=cols)], axis=1)\n", "    df.drop(cols, axis=1, inplace = True)\n", "    return df\n", "\n", "def get_data() :\n", "    X_train = pd.read_csv(DATA_TRAIN_PATH, index_col = \"id\")\n", "    X_test = pd.read_csv(DATA_TEST_PATH, index_col = \"id\")\n", "\n", "    y_train = pd.DataFrame(index = X_train.index)\n", "    y_train['target'] = X_train.loc[:,'target']\n", "    X_train.drop ('target', axis=1, inplace = True)\n", "    X_train.drop (featuresToDrop, axis=1, inplace = True)\n", "    X_test.drop (featuresToDrop,axis=1, inplace = True)\n", "\n", "    # car_11 is really a cat col\n", "    X_train.rename(columns={'ps_car_11': 'ps_car_11a_cat'}, inplace=True)\n", "    X_test.rename(columns={'ps_car_11': 'ps_car_11a_cat'}, inplace=True)\n", "\n", "    cat_cols = [elem for elem in list(X_train.columns) if \"cat\" in elem]\n", "    bin_cols = [elem for elem in list(X_train.columns) if \"bin\" in elem]\n", "    other_cols = [elem for elem in list(X_train.columns) if elem not in bin_cols and elem not in cat_cols]\n", "\n", "    # Scale numeric features in region of -1,1 using training set as the scaling range\n", "    X_test = scale_features(X_train, X_test, columnsToScale=other_cols)\n", "    X_train = scale_features(X_train, X_train, columnsToScale=other_cols)\n", "\n", "    X_train = one_hot(X_train, cat_cols)\n", "    X_test = one_hot(X_test, cat_cols)\n", "\n", "\n", "    return X_train, X_test, y_train\n", "\n", "\n", "def makeOutputFile(pred_fun, test, subsFile) :\n", "    df_out = pd.DataFrame(index=test.index)\n", "    y_pred = pred_fun( test )\n", "    df_out['target'] = y_pred\n", "    df_out.to_csv(subsFile, index_label=\"id\")\n", "\n", "def main() :\n", "    X_train, X_test, y_train = get_data()\n", "    model = create_model( input_dim=X_train.shape[1],\n", "                          first_layer_size=300,\n", "                          second_layer_size=200,\n", "                          third_layer_size=200,\n", "                          lr=0.0001,\n", "                          l2reg = 0.1,\n", "                          dropout = 0.2,\n", "                          mode=\"AUC\")\n", "\n", "    train_model(X_train, y_train, model)\n", "\n", "    with custom_object_scope({'soft_AUC_theano': soft_AUC_theano}):\n", "        pred_fun = lambda x: model.predict(np.array(x))\n", "        makeOutputFile(pred_fun, X_test, \"auc.csv\")\n", "\n", "    model = create_model_bce( input_dim=X_train.shape[1],\n", "                          first_layer_size=300,\n", "                          second_layer_size=200,\n", "                          third_layer_size=200,\n", "                          lr=0.0001,\n", "                          l2reg = 0.1,\n", "                          dropout = 0.2)\n", "\n", "    train_model(X_train, y_train, model)\n", "\n", "    pred_fun = lambda x: model.predict(np.array(x))\n", "    makeOutputFile(pred_fun, X_test, \"no_auc.csv\")\n", "\n", "main()"], "execution_count": null}], "nbformat_minor": 1, "metadata": {"language_info": {"pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "file_extension": ".py", "nbconvert_exporter": "python", "name": "python", "version": "3.6.3"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}}